[
  {
    "id": 1,
    "question": "What is Docker and why is containerization important?",
    "answer": "Docker is a platform for packaging applications and dependencies into isolated containers. Containers ensure consistency across environments (dev, staging, prod), enable microservices architecture, and provide lightweight isolation compared to VMs",
    "example": "# Dockerfile - defines how to build a container image\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nEXPOSE 3000\nCMD [\"node\", \"server.js\"]\n\n# Build and run\ndocker build -t myapp:1.0 .\ndocker run -p 3000:3000 myapp:1.0\n\n# Key benefits:\n# - \"Works on my machine\" → Works everywhere\n# - Fast startup (seconds vs minutes for VMs)\n# - Resource efficient (shared kernel)\n# - Version controlled infrastructure"
  },
  {
    "id": 2,
    "question": "What is the difference between a Docker image and a container?",
    "answer": "An image is a read-only template containing application code, runtime, libraries, and dependencies. A container is a running instance of an image - an isolated process with its own filesystem, network, and process space",
    "example": "# Image: blueprint (like a class)\n# Container: running instance (like an object)\n\n# List images\ndocker images\n# REPOSITORY    TAG       SIZE\n# node          18-alpine 175MB\n# myapp         1.0       195MB\n\n# List running containers\ndocker ps\n# CONTAINER ID   IMAGE       STATUS\n# a1b2c3d4e5f6   myapp:1.0   Up 5 minutes\n\n# One image can spawn multiple containers\ndocker run -d --name app1 myapp:1.0\ndocker run -d --name app2 myapp:1.0\ndocker run -d --name app3 myapp:1.0\n\n# Containers are ephemeral - data lost when removed\n# Use volumes for persistent data"
  },
  {
    "id": 3,
    "question": "What are the most common Dockerfile instructions?",
    "answer": "FROM sets base image, WORKDIR sets working directory, COPY/ADD copies files, RUN executes commands during build, ENV sets environment variables, EXPOSE documents ports, CMD/ENTRYPOINT defines the startup command",
    "example": "FROM python:3.11-slim          # Base image\n\nWORKDIR /app                    # Set working directory\n\nENV PYTHONDONTWRITEBYTECODE=1   # Environment variable\nENV PYTHONUNBUFFERED=1\n\nCOPY requirements.txt .         # Copy files\nRUN pip install -r requirements.txt  # Run during build\n\nCOPY . .                        # Copy application code\n\nEXPOSE 8000                     # Document port (doesn't publish)\n\n# CMD vs ENTRYPOINT:\n# CMD - default command, can be overridden\nCMD [\"python\", \"app.py\"]\n\n# ENTRYPOINT - always runs, CMD becomes arguments\n# ENTRYPOINT [\"python\"]\n# CMD [\"app.py\"]  # Can override: docker run myapp other.py"
  },
  {
    "id": 4,
    "question": "How do Docker volumes work?",
    "answer": "Volumes persist data outside containers, surviving container removal. Named volumes are managed by Docker, bind mounts link to host paths. Use volumes for databases, uploads, and any data that must persist",
    "example": "# Named volume (Docker managed)\ndocker volume create mydata\ndocker run -v mydata:/app/data myapp\n\n# Bind mount (host directory)\ndocker run -v /host/path:/container/path myapp\ndocker run -v $(pwd):/app myapp  # Current directory\n\n# In docker-compose.yml\nservices:\n  db:\n    image: postgres:15\n    volumes:\n      - postgres_data:/var/lib/postgresql/data  # Named\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql  # Bind\n\nvolumes:\n  postgres_data:  # Declare named volume\n\n# Volume commands\ndocker volume ls\ndocker volume inspect mydata\ndocker volume rm mydata"
  },
  {
    "id": 5,
    "question": "What is Docker Compose and when do you use it?",
    "answer": "Docker Compose defines and runs multi-container applications using a YAML file. It manages container lifecycle, networking, and volumes together. Use it for local development, testing, and simple deployments",
    "example": "# docker-compose.yml\nversion: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=postgres://db:5432/myapp\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:7-alpine\n\nvolumes:\n  postgres_data:\n\n# Commands\ndocker-compose up -d      # Start all services\ndocker-compose down       # Stop and remove\ndocker-compose logs -f    # Follow logs\ndocker-compose ps         # List services"
  },
  {
    "id": 6,
    "question": "What is Kubernetes and what problems does it solve?",
    "answer": "Kubernetes (K8s) is a container orchestration platform that automates deployment, scaling, and management of containerized applications. It handles service discovery, load balancing, self-healing, rolling updates, and secret management",
    "example": "# Problems Kubernetes solves:\n\n# 1. Scaling - automatically scale based on load\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nspec:\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        targetAverageUtilization: 70\n\n# 2. Self-healing - restart failed containers\n# 3. Service discovery - DNS-based service names\n# 4. Load balancing - distribute traffic across pods\n# 5. Rolling updates - zero-downtime deployments\n# 6. Secret management - secure credential storage\n# 7. Configuration - externalized config via ConfigMaps\n# 8. Storage orchestration - dynamic volume provisioning"
  },
  {
    "id": 7,
    "question": "What are the main Kubernetes components?",
    "answer": "Control plane: API Server (entry point), etcd (state store), Scheduler (pod placement), Controller Manager (desired state). Worker nodes: kubelet (node agent), kube-proxy (networking), container runtime (Docker/containerd)",
    "example": "# Control Plane Components:\n# - kube-apiserver: REST API, authentication, validation\n# - etcd: distributed key-value store for cluster state\n# - kube-scheduler: assigns pods to nodes\n# - kube-controller-manager: runs controllers (Deployment, ReplicaSet, etc.)\n\n# Worker Node Components:\n# - kubelet: ensures containers are running in pods\n# - kube-proxy: network rules for pod communication\n# - container runtime: runs containers (containerd, CRI-O)\n\n# Verify components\nkubectl get componentstatuses\nkubectl get nodes\nkubectl cluster-info\n\n# Typical cluster architecture:\n# - 3+ control plane nodes (HA)\n# - N worker nodes (scalable)\n# - etcd can be external or stacked"
  },
  {
    "id": 8,
    "question": "What is a Pod in Kubernetes?",
    "answer": "A Pod is the smallest deployable unit in Kubernetes - one or more containers that share network namespace, storage, and lifecycle. Containers in a pod communicate via localhost and share volumes",
    "example": "# Simple pod with one container\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\nspec:\n  containers:\n    - name: app\n      image: myapp:1.0\n      ports:\n        - containerPort: 8080\n      resources:\n        requests:\n          memory: \"128Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"256Mi\"\n          cpu: \"500m\"\n\n# Multi-container pod (sidecar pattern)\nspec:\n  containers:\n    - name: app\n      image: myapp:1.0\n    - name: log-shipper        # Sidecar\n      image: fluentd\n      volumeMounts:\n        - name: logs\n          mountPath: /var/log\n  volumes:\n    - name: logs\n      emptyDir: {}"
  },
  {
    "id": 9,
    "question": "What is a Kubernetes Deployment?",
    "answer": "A Deployment manages ReplicaSets and provides declarative updates for Pods. It handles rolling updates, rollbacks, and scaling. Always use Deployments instead of creating Pods directly",
    "example": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1        # Extra pods during update\n      maxUnavailable: 0  # Zero downtime\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n        - name: app\n          image: myapp:1.0\n          ports:\n            - containerPort: 8080\n\n# Commands\nkubectl apply -f deployment.yaml\nkubectl rollout status deployment/myapp\nkubectl rollout history deployment/myapp\nkubectl rollout undo deployment/myapp  # Rollback\nkubectl scale deployment/myapp --replicas=5"
  },
  {
    "id": 10,
    "question": "What is a Kubernetes Service?",
    "answer": "A Service provides stable networking for Pods - a consistent IP and DNS name that load balances across pod replicas. Types: ClusterIP (internal), NodePort (external via node port), LoadBalancer (cloud LB)",
    "example": "# ClusterIP - internal only (default)\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  type: ClusterIP\n  selector:\n    app: myapp       # Targets pods with this label\n  ports:\n    - port: 80       # Service port\n      targetPort: 8080  # Container port\n\n# Access: http://myapp.namespace.svc.cluster.local\n\n# NodePort - external via node IP:port\nspec:\n  type: NodePort\n  ports:\n    - port: 80\n      targetPort: 8080\n      nodePort: 30080  # 30000-32767\n\n# LoadBalancer - cloud provider LB\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 8080\n# Gets external IP from cloud provider"
  },
  {
    "id": 11,
    "question": "What is Terraform and what is Infrastructure as Code?",
    "answer": "Terraform is an IaC tool that provisions and manages cloud infrastructure using declarative configuration files. IaC treats infrastructure like software - version controlled, reviewed, tested, and reproducible",
    "example": "# main.tf - declarative infrastructure\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t3.micro\"\n\n  tags = {\n    Name = \"web-server\"\n  }\n}\n\n# Workflow\nterraform init      # Download providers\nterraform plan      # Preview changes\nterraform apply     # Apply changes\nterraform destroy   # Tear down\n\n# Benefits: reproducible, auditable, collaborative"
  },
  {
    "id": 12,
    "question": "What is Terraform state and why is it important?",
    "answer": "Terraform state tracks the mapping between configuration and real infrastructure. It's essential for detecting drift, planning changes, and managing dependencies. Store state remotely for team collaboration",
    "example": "# State stores:\n# - Resource IDs and attributes\n# - Dependency graph\n# - Metadata for updates\n\n# Local state (default) - terraform.tfstate\nterraform {\n  backend \"local\" {\n    path = \"terraform.tfstate\"\n  }\n}\n\n# Remote state (recommended for teams)\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"prod/terraform.tfstate\"\n    region         = \"us-west-2\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"  # State locking\n  }\n}\n\n# State commands\nterraform state list           # List resources\nterraform state show aws_instance.web  # Show resource\nterraform state mv             # Rename resource\nterraform state rm             # Remove from state\nterraform import               # Import existing resource"
  },
  {
    "id": 13,
    "question": "What is the difference between `docker run` flags -d, -it, -p, and -v?",
    "answer": "`-d` runs detached (background), `-it` enables interactive terminal, `-p` publishes ports (host:container), `-v` mounts volumes. These flags control container behavior and connectivity",
    "example": "# -d: Detached mode (background)\ndocker run -d nginx\n# Returns container ID, runs in background\n\n# -it: Interactive + TTY (foreground shell)\ndocker run -it ubuntu bash\n# Opens interactive shell in container\n\n# -p: Port mapping (host:container)\ndocker run -p 8080:80 nginx\n# Access nginx at localhost:8080\ndocker run -p 127.0.0.1:8080:80 nginx  # Bind to specific interface\n\n# -v: Volume mount\ndocker run -v myvolume:/data nginx        # Named volume\ndocker run -v $(pwd):/app nginx           # Bind mount\ndocker run -v $(pwd):/app:ro nginx        # Read-only\n\n# Combined example\ndocker run -d \\\n  --name myapp \\\n  -p 3000:3000 \\\n  -v $(pwd):/app \\\n  -e NODE_ENV=development \\\n  node:18"
  },
  {
    "id": 14,
    "question": "How do you optimize Docker image size?",
    "answer": "Use small base images (Alpine), multi-stage builds, minimize layers, use .dockerignore, avoid installing unnecessary packages, and clean up in the same RUN instruction",
    "example": "# Bad: Large image (1GB+)\nFROM node:18\nCOPY . .\nRUN npm install\nRUN npm run build\n\n# Good: Multi-stage build (100MB)\n# Stage 1: Build\nFROM node:18-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\n# Stage 2: Production (only runtime)\nFROM node:18-alpine\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nCMD [\"node\", \"dist/server.js\"]\n\n# .dockerignore\nnode_modules\n.git\n*.md\nDockerfile\n.env\n\n# Combine RUN commands to reduce layers\nRUN apt-get update && \\\n    apt-get install -y curl && \\\n    rm -rf /var/lib/apt/lists/*"
  },
  {
    "id": 15,
    "question": "What is a Kubernetes Ingress?",
    "answer": "Ingress exposes HTTP/HTTPS routes from outside the cluster to Services. It provides SSL termination, name-based virtual hosting, and path-based routing. Requires an Ingress Controller (nginx, traefik, etc.)",
    "example": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: myapp-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  tls:\n    - hosts:\n        - myapp.example.com\n      secretName: myapp-tls\n  rules:\n    - host: myapp.example.com\n      http:\n        paths:\n          - path: /api\n            pathType: Prefix\n            backend:\n              service:\n                name: api-service\n                port:\n                  number: 80\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: frontend-service\n                port:\n                  number: 80\n\n# Install nginx ingress controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.0/deploy/static/provider/cloud/deploy.yaml"
  },
  {
    "id": 16,
    "question": "What are Kubernetes ConfigMaps and Secrets?",
    "answer": "ConfigMaps store non-sensitive configuration data, Secrets store sensitive data (base64 encoded). Both decouple configuration from container images and can be mounted as files or environment variables",
    "example": "# ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  DATABASE_HOST: postgres.default.svc\n  LOG_LEVEL: info\n  config.json: |\n    {\"feature_flags\": {\"new_ui\": true}}\n\n# Secret\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\nstringData:  # Plain text (encoded automatically)\n  DATABASE_PASSWORD: supersecret\n  API_KEY: abc123\n\n# Using in Pod\nspec:\n  containers:\n    - name: app\n      env:\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: DATABASE_HOST\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: DATABASE_PASSWORD\n      volumeMounts:\n        - name: config\n          mountPath: /etc/config\n  volumes:\n    - name: config\n      configMap:\n        name: app-config"
  },
  {
    "id": 17,
    "question": "What are Terraform variables and outputs?",
    "answer": "Variables parameterize configurations for reusability. Input variables accept values, local variables compute intermediate values, output variables expose data. Use variables.tf and terraform.tfvars files",
    "example": "# variables.tf - declare inputs\nvariable \"environment\" {\n  description = \"Environment name\"\n  type        = string\n  default     = \"dev\"\n}\n\nvariable \"instance_count\" {\n  type    = number\n  default = 1\n}\n\nvariable \"allowed_ips\" {\n  type    = list(string)\n  default = []\n}\n\n# locals - computed values\nlocals {\n  name_prefix = \"${var.environment}-myapp\"\n  common_tags = {\n    Environment = var.environment\n    ManagedBy   = \"terraform\"\n  }\n}\n\n# main.tf - use variables\nresource \"aws_instance\" \"web\" {\n  count         = var.instance_count\n  instance_type = var.environment == \"prod\" ? \"t3.large\" : \"t3.micro\"\n  tags          = local.common_tags\n}\n\n# outputs.tf - expose values\noutput \"instance_ips\" {\n  value = aws_instance.web[*].public_ip\n}\n\n# terraform.tfvars\nenvironment    = \"prod\"\ninstance_count = 3"
  },
  {
    "id": 18,
    "question": "What is a Kubernetes namespace?",
    "answer": "Namespaces provide logical isolation within a cluster - separate environments, teams, or applications. Resources are scoped to namespaces. Default namespaces: default, kube-system, kube-public",
    "example": "# Create namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    env: production\n\n# Deploy to namespace\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: production  # Specify namespace\nspec:\n  # ...\n\n# kubectl commands with namespace\nkubectl get pods -n production\nkubectl get all -n production\nkubectl config set-context --current --namespace=production\n\n# Cross-namespace service access\n# service-name.namespace.svc.cluster.local\nhttp://myapp.production.svc.cluster.local\n\n# Resource quotas per namespace\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: 20Gi\n    limits.cpu: \"20\"\n    limits.memory: 40Gi"
  },
  {
    "id": 19,
    "question": "What is CI/CD and what are popular tools?",
    "answer": "CI (Continuous Integration) automatically builds and tests code on every commit. CD (Continuous Delivery/Deployment) automates releasing to staging/production. Popular tools: GitHub Actions, GitLab CI, Jenkins, CircleCI, ArgoCD",
    "example": "# GitHub Actions workflow (.github/workflows/ci.yml)\nname: CI/CD Pipeline\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n      - run: npm ci\n      - run: npm test\n      - run: npm run build\n\n  deploy:\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to production\n        run: |\n          # Build and push Docker image\n          # Update Kubernetes deployment\n          kubectl set image deployment/myapp app=myapp:${{ github.sha }}"
  },
  {
    "id": 20,
    "question": "How do you handle environment-specific configuration in Docker?",
    "answer": "Use environment variables (-e flag or env_file), different compose files for each environment, build args for build-time config, and mount config files as volumes",
    "example": "# docker-compose.yml (base)\nservices:\n  app:\n    build: .\n    env_file:\n      - .env  # Default env file\n\n# docker-compose.prod.yml (override)\nservices:\n  app:\n    image: myapp:${VERSION:-latest}  # Use built image\n    env_file:\n      - .env.prod\n    deploy:\n      replicas: 3\n\n# Run with overrides\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up\n\n# Environment variables\ndocker run -e NODE_ENV=production \\\n           -e DATABASE_URL=postgres://... \\\n           myapp\n\n# Build arguments (build-time only)\nARG NODE_VERSION=18\nFROM node:${NODE_VERSION}\n\ndocker build --build-arg NODE_VERSION=20 -t myapp ."
  },
  {
    "id": 21,
    "question": "What are Kubernetes probes (liveness, readiness, startup)?",
    "answer": "Liveness probes detect deadlocks and restart containers. Readiness probes determine if a pod can receive traffic. Startup probes handle slow-starting containers. All support HTTP, TCP, and exec checks",
    "example": "apiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n        - name: app\n          image: myapp:1.0\n          ports:\n            - containerPort: 8080\n          \n          # Startup probe - for slow-starting apps\n          startupProbe:\n            httpGet:\n              path: /healthz\n              port: 8080\n            failureThreshold: 30\n            periodSeconds: 10\n          \n          # Liveness - restart if unhealthy\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: 8080\n            initialDelaySeconds: 0\n            periodSeconds: 10\n            failureThreshold: 3\n          \n          # Readiness - remove from service if not ready\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 5\n\n# Exec probe example\nlivenessProbe:\n  exec:\n    command: [\"cat\", \"/tmp/healthy\"]"
  },
  {
    "id": 22,
    "question": "What are Terraform modules?",
    "answer": "Modules are reusable, self-contained packages of Terraform configuration. They encapsulate resources, accept inputs, and produce outputs. Use modules for DRY infrastructure code and consistent patterns",
    "example": "# Module structure\nmodules/\n  vpc/\n    main.tf\n    variables.tf\n    outputs.tf\n\n# modules/vpc/main.tf\nresource \"aws_vpc\" \"main\" {\n  cidr_block = var.cidr_block\n  tags = {\n    Name = var.name\n  }\n}\n\nresource \"aws_subnet\" \"public\" {\n  count      = length(var.public_subnets)\n  vpc_id     = aws_vpc.main.id\n  cidr_block = var.public_subnets[count.index]\n}\n\n# modules/vpc/outputs.tf\noutput \"vpc_id\" {\n  value = aws_vpc.main.id\n}\n\n# Using the module\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n  # Or from registry: source = \"terraform-aws-modules/vpc/aws\"\n  \n  name           = \"production\"\n  cidr_block     = \"10.0.0.0/16\"\n  public_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n}\n\n# Access outputs\nresource \"aws_instance\" \"web\" {\n  subnet_id = module.vpc.public_subnet_ids[0]\n}"
  },
  {
    "id": 23,
    "question": "What is the difference between Terraform `count` and `for_each`?",
    "answer": "`count` creates numbered instances (index-based), `for_each` creates named instances (key-based). Use `for_each` when possible - it's more stable when items are added/removed in the middle",
    "example": "# count - index-based, fragile ordering\nvariable \"subnet_cidrs\" {\n  default = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n}\n\nresource \"aws_subnet\" \"counted\" {\n  count      = length(var.subnet_cidrs)\n  cidr_block = var.subnet_cidrs[count.index]\n  # Resources: aws_subnet.counted[0], aws_subnet.counted[1]\n  # Problem: removing first item shifts all indices\n}\n\n# for_each - key-based, stable\nvariable \"subnets\" {\n  default = {\n    public-a  = \"10.0.1.0/24\"\n    public-b  = \"10.0.2.0/24\"\n    private-a = \"10.0.3.0/24\"\n  }\n}\n\nresource \"aws_subnet\" \"each\" {\n  for_each   = var.subnets\n  cidr_block = each.value\n  tags = {\n    Name = each.key\n  }\n  # Resources: aws_subnet.each[\"public-a\"], etc.\n  # Removing \"public-b\" doesn't affect others\n}\n\n# for_each with list (convert to set)\nresource \"aws_iam_user\" \"users\" {\n  for_each = toset([\"alice\", \"bob\", \"carol\"])\n  name     = each.key\n}"
  },
  {
    "id": 24,
    "question": "How do you implement blue-green deployments in Kubernetes?",
    "answer": "Blue-green runs two identical environments (blue=current, green=new). Switch traffic by updating Service selector. Enables instant rollback. Alternatives: rolling update (default), canary (gradual traffic shift)",
    "example": "# Blue deployment (current)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n      version: blue\n  template:\n    metadata:\n      labels:\n        app: myapp\n        version: blue\n    spec:\n      containers:\n        - name: app\n          image: myapp:1.0\n\n# Green deployment (new)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-green\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n      version: green\n  template:\n    metadata:\n      labels:\n        app: myapp\n        version: green\n    spec:\n      containers:\n        - name: app\n          image: myapp:2.0\n\n# Service - switch by changing selector\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  selector:\n    app: myapp\n    version: blue  # Change to 'green' to switch\n  ports:\n    - port: 80"
  },
  {
    "id": 25,
    "question": "What is Helm and why is it used?",
    "answer": "Helm is the package manager for Kubernetes. Charts are packages of pre-configured K8s resources. Helm handles templating, versioning, dependencies, and releases. Essential for managing complex applications",
    "example": "# Install Helm chart\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-postgres bitnami/postgresql \\\n  --set auth.postgresPassword=secret \\\n  --set primary.persistence.size=10Gi\n\n# Chart structure\nmychart/\n  Chart.yaml        # Metadata\n  values.yaml       # Default values\n  templates/        # K8s manifests with Go templating\n    deployment.yaml\n    service.yaml\n    _helpers.tpl    # Template helpers\n\n# templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  template:\n    spec:\n      containers:\n        - name: {{ .Chart.Name }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n\n# Commands\nhelm install myrelease ./mychart -f custom-values.yaml\nhelm upgrade myrelease ./mychart\nhelm rollback myrelease 1\nhelm list\nhelm uninstall myrelease"
  },
  {
    "id": 26,
    "question": "What is GitOps and how does ArgoCD implement it?",
    "answer": "GitOps uses Git as the single source of truth for infrastructure and applications. Changes go through Git (PRs, reviews), and operators sync cluster state to match. ArgoCD watches Git repos and automatically syncs Kubernetes clusters",
    "example": "# GitOps principles:\n# 1. Declarative - desired state in Git\n# 2. Versioned - Git history = audit trail\n# 3. Automated - operators reconcile state\n# 4. Continuous - constant sync loop\n\n# ArgoCD Application\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/k8s-manifests\n    targetRevision: HEAD\n    path: apps/myapp/overlays/production\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true      # Delete resources removed from Git\n      selfHeal: true   # Revert manual changes\n    syncOptions:\n      - CreateNamespace=true\n\n# Workflow:\n# 1. Developer pushes code → CI builds image\n# 2. CI updates image tag in k8s-manifests repo\n# 3. ArgoCD detects change, syncs to cluster\n# 4. Rollback = git revert"
  },
  {
    "id": 27,
    "question": "How do you manage Terraform state in a team environment?",
    "answer": "Use remote backend (S3, GCS, Terraform Cloud) with state locking. Implement workspaces for environments. Use CI/CD for automated plan/apply. Never commit state files to Git",
    "example": "# Backend configuration with locking\nterraform {\n  backend \"s3\" {\n    bucket         = \"company-terraform-state\"\n    key            = \"projects/myapp/terraform.tfstate\"\n    region         = \"us-west-2\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"  # Prevents concurrent applies\n  }\n}\n\n# DynamoDB table for locking\nresource \"aws_dynamodb_table\" \"terraform_locks\" {\n  name         = \"terraform-locks\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"LockID\"\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n}\n\n# Workspaces for environments\nterraform workspace new production\nterraform workspace select production\n\n# Use workspace in config\nlocals {\n  env = terraform.workspace\n}\n\nresource \"aws_instance\" \"web\" {\n  instance_type = local.env == \"production\" ? \"t3.large\" : \"t3.micro\"\n  tags = {\n    Environment = local.env\n  }\n}\n\n# CI/CD workflow\n# PR: terraform plan (comment on PR)\n# Merge: terraform apply -auto-approve"
  },
  {
    "id": 28,
    "question": "What are Kubernetes resource requests and limits?",
    "answer": "Requests are guaranteed resources for scheduling. Limits are maximum resources before throttling (CPU) or OOM kill (memory). Set both for predictable behavior and cluster efficiency",
    "example": "apiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n        - name: app\n          image: myapp:1.0\n          resources:\n            requests:\n              memory: \"256Mi\"  # Guaranteed\n              cpu: \"250m\"      # 0.25 CPU cores\n            limits:\n              memory: \"512Mi\"  # Max before OOM kill\n              cpu: \"500m\"      # Max before throttling\n\n# CPU: millicores (1000m = 1 core)\n# Memory: Mi (mebibytes), Gi (gibibytes)\n\n# QoS Classes (determined automatically):\n# - Guaranteed: requests == limits (highest priority)\n# - Burstable: requests < limits\n# - BestEffort: no requests/limits (evicted first)\n\n# LimitRange - default limits for namespace\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-limits\nspec:\n  limits:\n    - default:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      defaultRequest:\n        memory: \"256Mi\"\n        cpu: \"250m\"\n      type: Container"
  },
  {
    "id": 29,
    "question": "How do you handle secrets in Terraform?",
    "answer": "Never hardcode secrets. Use environment variables, secret management tools (Vault, AWS Secrets Manager), or encrypted files. Mark sensitive outputs. Consider using data sources to fetch secrets at runtime",
    "example": "# BAD - hardcoded secret\nresource \"aws_db_instance\" \"db\" {\n  password = \"supersecret123\"  # Never do this!\n}\n\n# Option 1: Environment variable\nvariable \"db_password\" {\n  type      = string\n  sensitive = true  # Hides in output\n}\n# Set: export TF_VAR_db_password=\"secret\"\n\n# Option 2: AWS Secrets Manager\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"myapp/db-password\"\n}\n\nresource \"aws_db_instance\" \"db\" {\n  password = data.aws_secretsmanager_secret_version.db_password.secret_string\n}\n\n# Option 3: Vault\ndata \"vault_generic_secret\" \"db\" {\n  path = \"secret/myapp/database\"\n}\n\nresource \"aws_db_instance\" \"db\" {\n  password = data.vault_generic_secret.db.data[\"password\"]\n}\n\n# Sensitive output\noutput \"db_password\" {\n  value     = var.db_password\n  sensitive = true\n}\n\n# .gitignore\n*.tfvars\n*.tfstate\n*.tfstate.backup"
  },
  {
    "id": 30,
    "question": "What is a StatefulSet in Kubernetes?",
    "answer": "StatefulSet manages stateful applications requiring stable network identity, persistent storage, and ordered deployment/scaling. Each pod gets a stable hostname (pod-0, pod-1) and dedicated PersistentVolumeClaim",
    "example": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\nspec:\n  serviceName: postgres  # Headless service for DNS\n  replicas: 3\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:15\n          ports:\n            - containerPort: 5432\n          volumeMounts:\n            - name: data\n              mountPath: /var/lib/postgresql/data\n  volumeClaimTemplates:  # Each pod gets its own PVC\n    - metadata:\n        name: data\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        resources:\n          requests:\n            storage: 10Gi\n\n# Headless Service (clusterIP: None)\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\nspec:\n  clusterIP: None\n  selector:\n    app: postgres\n  ports:\n    - port: 5432\n\n# DNS: postgres-0.postgres.namespace.svc.cluster.local"
  },
  {
    "id": 31,
    "question": "How do you implement canary deployments?",
    "answer": "Canary releases roll out changes to a small subset of users before full deployment. Implement with weighted traffic splitting using Ingress annotations, service mesh (Istio), or deployment strategies (Argo Rollouts)",
    "example": "# Simple canary with two deployments\n# Stable: 90% traffic\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-stable\nspec:\n  replicas: 9\n  selector:\n    matchLabels:\n      app: myapp\n      track: stable\n\n# Canary: 10% traffic\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n      track: canary\n\n# Single service selects both\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  selector:\n    app: myapp  # Matches both stable and canary\n\n# Argo Rollouts - advanced canary\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nspec:\n  strategy:\n    canary:\n      steps:\n        - setWeight: 10\n        - pause: {duration: 1h}\n        - setWeight: 30\n        - pause: {duration: 1h}\n        - setWeight: 50\n        - pause: {duration: 1h}"
  },
  {
    "id": 32,
    "question": "What are Terraform data sources?",
    "answer": "Data sources fetch information from providers or external sources without creating resources. Use them to reference existing infrastructure, query APIs, or compute values dynamically",
    "example": "# Query existing VPC\ndata \"aws_vpc\" \"existing\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"production-vpc\"]\n  }\n}\n\n# Use in resource\nresource \"aws_subnet\" \"new\" {\n  vpc_id     = data.aws_vpc.existing.id\n  cidr_block = \"10.0.100.0/24\"\n}\n\n# Get latest AMI\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"]  # Canonical\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-*-22.04-amd64-server-*\"]\n  }\n}\n\nresource \"aws_instance\" \"web\" {\n  ami = data.aws_ami.ubuntu.id\n}\n\n# Query availability zones\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n\n# Read local file\ndata \"local_file\" \"config\" {\n  filename = \"${path.module}/config.json\"\n}\n\n# External data source (run script)\ndata \"external\" \"version\" {\n  program = [\"bash\", \"${path.module}/get-version.sh\"]\n}"
  },
  {
    "id": 33,
    "question": "How do you debug containers in Kubernetes?",
    "answer": "Use kubectl logs, exec, describe, and events. Debug pods with ephemeral containers or debug images. Check resource usage with top. For networking, use temporary debug pods",
    "example": "# View logs\nkubectl logs pod-name\nkubectl logs pod-name -c container-name  # Multi-container\nkubectl logs pod-name --previous          # Previous instance\nkubectl logs -f pod-name                  # Follow\nkubectl logs -l app=myapp                 # By label\n\n# Execute commands in container\nkubectl exec -it pod-name -- /bin/sh\nkubectl exec pod-name -- cat /etc/config\n\n# Describe resource (events, conditions)\nkubectl describe pod pod-name\nkubectl get events --sort-by=.lastTimestamp\n\n# Resource usage\nkubectl top pods\nkubectl top nodes\n\n# Ephemeral debug container (K8s 1.25+)\nkubectl debug pod-name -it --image=busybox\n\n# Debug node\nkubectl debug node/node-name -it --image=ubuntu\n\n# Network debugging pod\nkubectl run debug --rm -it --image=nicolaka/netshoot -- /bin/bash\n# Then: curl, dig, ping, tcpdump, etc.\n\n# Port forward for local testing\nkubectl port-forward pod-name 8080:80\nkubectl port-forward svc/service-name 8080:80"
  },
  {
    "id": 34,
    "question": "What is the Terraform lifecycle block?",
    "answer": "Lifecycle customizes resource behavior: `create_before_destroy`, `prevent_destroy`, `ignore_changes`, and `replace_triggered_by`. Use for safe updates, protecting critical resources, and handling external changes",
    "example": "resource \"aws_instance\" \"web\" {\n  ami           = var.ami_id\n  instance_type = \"t3.micro\"\n  \n  lifecycle {\n    # Create new before destroying old (zero downtime)\n    create_before_destroy = true\n    \n    # Prevent accidental deletion\n    prevent_destroy = true\n    \n    # Ignore external changes to tags\n    ignore_changes = [\n      tags[\"LastModified\"],\n      user_data,  # Ignore user_data changes\n    ]\n  }\n}\n\n# Replace when another resource changes\nresource \"aws_instance\" \"web\" {\n  lifecycle {\n    replace_triggered_by = [\n      aws_security_group.web.id  # Replace if SG changes\n    ]\n  }\n}\n\n# Preconditions and postconditions (TF 1.2+)\nresource \"aws_instance\" \"web\" {\n  lifecycle {\n    precondition {\n      condition     = var.environment != \"production\" || var.instance_type != \"t3.micro\"\n      error_message = \"Production must use larger instance type.\"\n    }\n    postcondition {\n      condition     = self.public_ip != \"\"\n      error_message = \"Instance must have public IP.\"\n    }\n  }\n}"
  },
  {
    "id": 35,
    "question": "How do you implement network policies in Kubernetes?",
    "answer": "NetworkPolicies control pod-to-pod traffic using labels. By default all traffic is allowed; policies are additive. Requires a CNI that supports policies (Calico, Cilium, Weave Net)",
    "example": "# Default deny all ingress\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: production\nspec:\n  podSelector: {}  # All pods in namespace\n  policyTypes:\n    - Ingress\n\n# Allow specific traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: api-policy\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: frontend  # From frontend namespace\n        - podSelector:\n            matchLabels:\n              app: web        # From web pods\n      ports:\n        - protocol: TCP\n          port: 8080\n  egress:\n    - to:\n        - podSelector:\n            matchLabels:\n              app: database\n      ports:\n        - protocol: TCP\n          port: 5432\n    - to:  # Allow DNS\n        - namespaceSelector: {}\n          podSelector:\n            matchLabels:\n              k8s-app: kube-dns\n      ports:\n        - protocol: UDP\n          port: 53"
  },
  {
    "id": 36,
    "question": "What is Docker layer caching and how do you optimize it?",
    "answer": "Docker caches each instruction's result as a layer. Cache is invalidated when an instruction or its inputs change. Order instructions from least to most frequently changing to maximize cache hits",
    "example": "# BAD - cache invalidated on every code change\nFROM node:18-alpine\nCOPY . .                    # Invalidates cache on any file change\nRUN npm install             # Always runs\nRUN npm run build\n\n# GOOD - optimized layer caching\nFROM node:18-alpine\nWORKDIR /app\n\n# Layer 1: Dependencies (changes rarely)\nCOPY package.json package-lock.json ./\nRUN npm ci                  # Cached unless package.json changes\n\n# Layer 2: Build tools/config (changes occasionally)\nCOPY tsconfig.json ./\nCOPY webpack.config.js ./\n\n# Layer 3: Source code (changes frequently)\nCOPY src/ ./src/\nRUN npm run build\n\n# CI caching with BuildKit\nFROM node:18-alpine\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci\n\n# Docker build with cache\ndocker build --cache-from myapp:latest -t myapp:new .\n\n# BuildKit cache mount for package managers\nRUN --mount=type=cache,target=/var/cache/apt \\\n    apt-get update && apt-get install -y curl"
  },
  {
    "id": 37,
    "question": "What are DaemonSets in Kubernetes?",
    "answer": "DaemonSet ensures a pod runs on all (or selected) nodes. Used for node-level services: log collectors, monitoring agents, network plugins, storage drivers. Pods are automatically added/removed as nodes join/leave",
    "example": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      tolerations:  # Run on all nodes including masters\n        - key: node-role.kubernetes.io/control-plane\n          effect: NoSchedule\n      containers:\n        - name: fluentd\n          image: fluentd:v1.16\n          resources:\n            limits:\n              memory: 200Mi\n            requests:\n              cpu: 100m\n              memory: 200Mi\n          volumeMounts:\n            - name: varlog\n              mountPath: /var/log\n            - name: containers\n              mountPath: /var/lib/docker/containers\n              readOnly: true\n      volumes:\n        - name: varlog\n          hostPath:\n            path: /var/log\n        - name: containers\n          hostPath:\n            path: /var/lib/docker/containers\n\n# Run only on specific nodes\nspec:\n  template:\n    spec:\n      nodeSelector:\n        node-type: gpu  # Only GPU nodes"
  },
  {
    "id": 38,
    "question": "How do you use Terraform workspaces vs directory structure for environments?",
    "answer": "Workspaces share code with separate state files - good for similar environments. Directory structure separates entire configurations - better for significantly different environments. Many teams use both",
    "example": "# Workspace approach (same code, different state)\nterraform workspace new dev\nterraform workspace new staging\nterraform workspace new prod\n\n# In configuration\nlocals {\n  env_config = {\n    dev     = { instance_type = \"t3.micro\", count = 1 }\n    staging = { instance_type = \"t3.small\", count = 2 }\n    prod    = { instance_type = \"t3.large\", count = 5 }\n  }\n  config = local.env_config[terraform.workspace]\n}\n\nresource \"aws_instance\" \"web\" {\n  count         = local.config.count\n  instance_type = local.config.instance_type\n}\n\n# Directory approach (separate configurations)\ninfra/\n  modules/\n    vpc/\n    app/\n  environments/\n    dev/\n      main.tf       # module \"vpc\" { source = \"../../modules/vpc\" }\n      terraform.tfvars\n    staging/\n      main.tf\n      terraform.tfvars\n    prod/\n      main.tf\n      terraform.tfvars\n\n# Hybrid: directories + workspaces\n# environments/prod/ with workspaces for regions\nterraform workspace new us-west-2\nterraform workspace new eu-west-1"
  },
  {
    "id": 39,
    "question": "What is a Kubernetes Operator?",
    "answer": "Operators extend Kubernetes with custom controllers that manage complex applications. They encode operational knowledge (install, upgrade, backup, scaling) into software. Built using Custom Resource Definitions (CRDs)",
    "example": "# Custom Resource Definition\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: postgresclusters.postgres.example.com\nspec:\n  group: postgres.example.com\n  names:\n    kind: PostgresCluster\n    plural: postgresclusters\n    singular: postgrescluster\n    shortNames: [pg]\n  scope: Namespaced\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                replicas:\n                  type: integer\n                version:\n                  type: string\n\n# Custom Resource (instance)\napiVersion: postgres.example.com/v1\nkind: PostgresCluster\nmetadata:\n  name: my-postgres\nspec:\n  replicas: 3\n  version: \"15\"\n\n# Operator watches PostgresCluster resources and:\n# - Creates StatefulSet, Services, ConfigMaps\n# - Handles upgrades, backups, failover\n# - Manages replication configuration\n\n# Popular operators: Prometheus, cert-manager, Strimzi (Kafka)"
  },
  {
    "id": 40,
    "question": "How do you implement Pod Disruption Budgets?",
    "answer": "PodDisruptionBudget (PDB) limits voluntary disruptions (node drain, upgrades) to ensure minimum availability. Set minAvailable or maxUnavailable. Essential for high-availability applications during cluster maintenance",
    "example": "# Ensure at least 2 pods always running\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: myapp-pdb\nspec:\n  minAvailable: 2  # Or use percentage: \"50%\"\n  selector:\n    matchLabels:\n      app: myapp\n\n# Alternative: max 1 pod unavailable\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: myapp-pdb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: myapp\n\n# Check PDB status\nkubectl get pdb\n# NAME        MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE\n# myapp-pdb   2               N/A               1                     1h\n\n# PDB affects:\n# - kubectl drain (node maintenance)\n# - Cluster autoscaler\n# - Node upgrades\n# Does NOT affect:\n# - Involuntary disruptions (node crash)\n# - Pod deletion\n# - Scaling down deployment"
  },
  {
    "id": 41,
    "question": "What is the Terraform import workflow for existing infrastructure?",
    "answer": "Import brings existing resources under Terraform management. Write configuration first, then import. Use `terraform import` or import blocks (TF 1.5+). Always run plan after import to verify",
    "example": "# Step 1: Write configuration matching existing resource\nresource \"aws_instance\" \"existing\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t3.micro\"\n  tags = {\n    Name = \"existing-server\"\n  }\n}\n\n# Step 2a: Import using CLI\nterraform import aws_instance.existing i-1234567890abcdef0\n\n# Step 2b: Or use import block (TF 1.5+)\nimport {\n  to = aws_instance.existing\n  id = \"i-1234567890abcdef0\"\n}\n\n# Step 3: Run plan to check for drift\nterraform plan\n# Shows differences between config and actual state\n\n# Step 4: Adjust configuration to match reality\n# Or accept planned changes if intentional\n\n# Generate config automatically (TF 1.5+)\nterraform plan -generate-config-out=generated.tf\n\n# Bulk import with for_each\nimport {\n  for_each = toset([\"i-111\", \"i-222\", \"i-333\"])\n  to       = aws_instance.servers[each.key]\n  id       = each.key\n}"
  },
  {
    "id": 42,
    "question": "How do you implement RBAC in Kubernetes?",
    "answer": "RBAC controls access using Roles (namespace-scoped) or ClusterRoles (cluster-wide), bound to users/groups/service accounts via RoleBindings or ClusterRoleBindings. Follow principle of least privilege",
    "example": "# Role - namespace scoped permissions\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: default\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\"]\n    verbs: [\"get\", \"list\"]\n\n# RoleBinding - grants Role to user/SA\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n  - kind: User\n    name: jane\n    apiGroup: rbac.authorization.k8s.io\n  - kind: ServiceAccount\n    name: monitoring\n    namespace: default\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n\n# ClusterRole - cluster-wide\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"list\"]\n\n# Check permissions\nkubectl auth can-i get pods --as jane\nkubectl auth can-i '*' '*'  # Check admin access"
  },
  {
    "id": 43,
    "question": "What is Prometheus and how do you use it for monitoring?",
    "answer": "Prometheus is a time-series database and monitoring system. It scrapes metrics from targets via HTTP, stores them, and provides PromQL for querying. Pairs with Grafana for visualization and Alertmanager for alerts",
    "example": "# Prometheus scrape config (prometheus.yml)\nscrape_configs:\n  - job_name: 'kubernetes-pods'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n\n# Pod annotations for scraping\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"8080\"\n    prometheus.io/path: \"/metrics\"\n\n# PromQL queries\n# CPU usage by pod\nsum(rate(container_cpu_usage_seconds_total[5m])) by (pod)\n\n# Memory usage percentage\n(container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100\n\n# HTTP request rate\nsum(rate(http_requests_total[5m])) by (status_code)\n\n# Alert rule\ngroups:\n  - name: example\n    rules:\n      - alert: HighErrorRate\n        expr: sum(rate(http_requests_total{status=~\"5..\"}[5m])) > 10\n        for: 5m\n        labels:\n          severity: critical"
  },
  {
    "id": 44,
    "question": "How do you manage multi-cloud infrastructure with Terraform?",
    "answer": "Configure multiple providers in one configuration or separate state files per cloud. Use modules for abstraction. Consider Terragrunt for DRY configurations across environments and clouds",
    "example": "# Multiple providers in one config\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\nprovider \"google\" {\n  project = \"my-project\"\n  region  = \"us-central1\"\n}\n\n# Provider aliases for multiple regions\nprovider \"aws\" {\n  alias  = \"eu\"\n  region = \"eu-west-1\"\n}\n\nresource \"aws_instance\" \"us_server\" {\n  provider = aws  # Default\n}\n\nresource \"aws_instance\" \"eu_server\" {\n  provider = aws.eu\n}\n\n# Abstraction module for compute\nmodule \"compute\" {\n  source = \"./modules/compute\"\n  \n  cloud    = var.cloud  # \"aws\" or \"gcp\"\n  instance_type = var.cloud == \"aws\" ? \"t3.micro\" : \"e2-micro\"\n}\n\n# Terragrunt for DRY config\n# terragrunt.hcl in each environment\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n\ninputs = {\n  environment = \"production\"\n  region      = \"us-west-2\"\n}"
  },
  {
    "id": 45,
    "question": "What is a Service Mesh and how does Istio work?",
    "answer": "Service mesh manages service-to-service communication with sidecar proxies. Istio provides traffic management, security (mTLS), and observability. The data plane (Envoy sidecars) handles traffic; control plane configures them",
    "example": "# Istio VirtualService - traffic routing\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: myapp\nspec:\n  hosts:\n    - myapp\n  http:\n    - match:\n        - headers:\n            x-canary:\n              exact: \"true\"\n      route:\n        - destination:\n            host: myapp\n            subset: canary\n    - route:\n        - destination:\n            host: myapp\n            subset: stable\n          weight: 90\n        - destination:\n            host: myapp\n            subset: canary\n          weight: 10\n\n# DestinationRule - traffic policies\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: myapp\nspec:\n  host: myapp\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 100\n    outlierDetection:  # Circuit breaker\n      consecutive5xxErrors: 5\n      interval: 30s\n      baseEjectionTime: 30s\n  subsets:\n    - name: stable\n      labels:\n        version: v1\n    - name: canary\n      labels:\n        version: v2\n\n# mTLS enabled by default in strict mode\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: istio-system\nspec:\n  mtls:\n    mode: STRICT"
  },
  {
    "id": 46,
    "question": "How do you handle zero-downtime database migrations?",
    "answer": "Use expand-contract pattern: 1) Add new columns/tables (expand), 2) Deploy app supporting both schemas, 3) Migrate data, 4) Remove old columns (contract). Never rename or delete columns in use",
    "example": "# Phase 1: Expand - Add new column (backward compatible)\n-- Migration 001_add_email_verified.sql\nALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT false;\n\n# Phase 2: Deploy app version that writes to BOTH columns\n# App v2: writes to old AND new columns, reads from new with fallback\n\n# Phase 3: Backfill data\n-- Migration 002_backfill_email_verified.sql\nUPDATE users SET email_verified = (verified_at IS NOT NULL)\nWHERE email_verified IS NULL;\n\n# Phase 4: Deploy app using only new column\n# App v3: reads/writes only new column\n\n# Phase 5: Contract - Remove old column\n-- Migration 003_drop_verified_at.sql\nALTER TABLE users DROP COLUMN verified_at;\n\n# Kubernetes: Rolling deployment ensures both versions run during transition\napiVersion: apps/v1\nkind: Deployment\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n\n# Init container for migrations\nspec:\n  initContainers:\n    - name: migrate\n      image: myapp:v2\n      command: [\"./migrate\", \"up\"]\n  containers:\n    - name: app\n      image: myapp:v2"
  },
  {
    "id": 47,
    "question": "What is the Terraform moved block for refactoring?",
    "answer": "`moved` blocks declare resource renames without destroying/recreating. Terraform updates state to reflect the new address. Essential for refactoring module structure and renaming resources safely",
    "example": "# Rename a resource\n# Before: resource \"aws_instance\" \"web\" {...}\n# After:  resource \"aws_instance\" \"app\" {...}\n\nmoved {\n  from = aws_instance.web\n  to   = aws_instance.app\n}\n\n# Move into a module\n# Before: resource \"aws_instance\" \"web\" {...}\n# After:  module \"compute\" { ... } containing the resource\n\nmoved {\n  from = aws_instance.web\n  to   = module.compute.aws_instance.web\n}\n\n# Move indexed resource to for_each\n# Before: aws_instance.web[0], aws_instance.web[1]\n# After:  aws_instance.web[\"primary\"], aws_instance.web[\"secondary\"]\n\nmoved {\n  from = aws_instance.web[0]\n  to   = aws_instance.web[\"primary\"]\n}\nmoved {\n  from = aws_instance.web[1]\n  to   = aws_instance.web[\"secondary\"]\n}\n\n# Workflow:\n# 1. Add moved block\n# 2. Rename resource in config\n# 3. terraform plan (shows no changes - just state update)\n# 4. terraform apply\n# 5. Optionally remove moved block later"
  },
  {
    "id": 48,
    "question": "How do you implement horizontal pod autoscaling with custom metrics?",
    "answer": "HPA scales based on CPU/memory (built-in) or custom/external metrics (requires metrics adapter). Custom metrics come from Prometheus via prometheus-adapter or cloud provider metrics",
    "example": "# Standard HPA with CPU/memory\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: myapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n\n# Custom metrics (requires prometheus-adapter)\n    - type: Pods\n      pods:\n        metric:\n          name: http_requests_per_second\n        target:\n          type: AverageValue\n          averageValue: 100\n\n# External metrics (e.g., SQS queue depth)\n    - type: External\n      external:\n        metric:\n          name: sqs_messages_visible\n          selector:\n            matchLabels:\n              queue: myqueue\n        target:\n          type: Value\n          value: 10\n\n# Scale behavior (prevent flapping)\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n        - type: Percent\n          value: 10\n          periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n        - type: Percent\n          value: 100\n          periodSeconds: 15"
  },
  {
    "id": 49,
    "question": "What are Terraform provider configurations for assume role and workload identity?",
    "answer": "Configure providers to assume IAM roles instead of static credentials. Use OIDC for CI/CD (GitHub Actions, GitLab). Enables secure, credential-free authentication following least privilege",
    "example": "# AWS: Assume role\nprovider \"aws\" {\n  region = \"us-west-2\"\n  \n  assume_role {\n    role_arn     = \"arn:aws:iam::123456789012:role/TerraformRole\"\n    session_name = \"terraform-session\"\n    external_id  = \"my-external-id\"  # Optional\n  }\n}\n\n# AWS: OIDC for GitHub Actions\n# In GitHub Actions workflow:\n# - uses: aws-actions/configure-aws-credentials@v4\n#   with:\n#     role-to-assume: arn:aws:iam::123456789012:role/GitHubActions\n#     aws-region: us-west-2\n\n# GCP: Workload Identity Federation\nprovider \"google\" {\n  project = \"my-project\"\n}\n\n# Uses GOOGLE_APPLICATION_CREDENTIALS or workload identity\n# For GKE: link K8s SA to GCP SA\n# kubectl annotate serviceaccount myapp \\\n#   iam.gke.io/gcp-service-account=myapp@project.iam.gserviceaccount.com\n\n# Azure: Service Principal with OIDC\nprovider \"azurerm\" {\n  features {}\n  use_oidc = true\n}\n\n# Multiple accounts with aliases\nprovider \"aws\" {\n  alias  = \"production\"\n  assume_role {\n    role_arn = \"arn:aws:iam::PROD_ACCOUNT:role/TerraformRole\"\n  }\n}\n\nresource \"aws_instance\" \"prod\" {\n  provider = aws.production\n}"
  },
  {
    "id": 50,
    "question": "How do you implement disaster recovery for Kubernetes clusters?",
    "answer": "Backup etcd and persistent volumes with Velero. Use GitOps for declarative recovery. Multi-cluster with federation or service mesh. Regular DR drills. Consider managed K8s backup solutions",
    "example": "# Velero - backup and restore\n# Install Velero with cloud provider plugin\nvelero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.7.0 \\\n  --bucket velero-backups \\\n  --backup-location-config region=us-west-2 \\\n  --snapshot-location-config region=us-west-2\n\n# Schedule daily backups\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: daily-backup\n  namespace: velero\nspec:\n  schedule: \"0 2 * * *\"  # 2 AM daily\n  template:\n    includedNamespaces:\n      - production\n      - staging\n    ttl: 720h  # 30 days retention\n    snapshotVolumes: true\n\n# Manual backup\nvelero backup create pre-upgrade --include-namespaces production\n\n# Restore to new cluster\nvelero restore create --from-backup daily-backup-20240101\n\n# GitOps DR strategy:\n# 1. All manifests in Git\n# 2. Bootstrap new cluster: install ArgoCD\n# 3. Point ArgoCD to Git repo\n# 4. Cluster rebuilds from declarative state\n# 5. Restore PV data from Velero\n\n# etcd backup (if self-managed)\nETCDCTL_API=3 etcdctl snapshot save backup.db \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key"
  }
]
