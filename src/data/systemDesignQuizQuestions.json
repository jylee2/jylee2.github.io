[
  {
    "id": 1,
    "question": "What is the Single Responsibility Principle (SRP)?",
    "answer": "A class or module should have only one reason to change - meaning it should have only one job or responsibility. This improves maintainability, testability, and reduces coupling",
    "example": "Bad: UserService handles authentication, email sending, and database operations\n\nGood: Separate into focused classes:\n- AuthenticationService: handles login/logout\n- EmailService: handles sending emails\n- UserRepository: handles database operations\n\nReal-world example:\nTwitter's notification system is separate from tweet storage.\nChanging how notifications work shouldn't affect how tweets are stored."
  },
  {
    "id": 2,
    "question": "What is the Open/Closed Principle (OCP)?",
    "answer": "Software entities should be open for extension but closed for modification. Add new functionality by adding new code, not changing existing code. Achieved through abstraction and polymorphism",
    "example": "Scenario: Payment processing system\n\nBad: Adding PayPal requires modifying PaymentProcessor class\nif (type == 'stripe') { ... }\nelse if (type == 'paypal') { ... }  // Must modify existing code\n\nGood: Use abstractions\n- PaymentGateway interface with process() method\n- StripeGateway implements PaymentGateway\n- PayPalGateway implements PaymentGateway (new file, no changes to existing)\n\nAdding Apple Pay = create new ApplePayGateway class.\nExisting code remains untouched."
  },
  {
    "id": 3,
    "question": "What is the Liskov Substitution Principle (LSP)?",
    "answer": "Subtypes must be substitutable for their base types without altering program correctness. If S is a subtype of T, objects of type T can be replaced with objects of type S without breaking the application",
    "example": "Classic violation: Square extends Rectangle\n\nRectangle has setWidth() and setHeight() independently.\nSquare overrides them to keep width == height.\n\nProblem:\nrectangle.setWidth(5)\nrectangle.setHeight(10)\nassert rectangle.area() == 50  // Fails for Square!\n\nBetter design:\n- Shape interface with area() method\n- Rectangle and Square are separate implementations\n- Don't force inheritance where behavior differs\n\nReal-world: A \"ReadOnlyUser\" shouldn't extend \"User\" if User has save() method."
  },
  {
    "id": 4,
    "question": "What is the Interface Segregation Principle (ISP)?",
    "answer": "Clients should not be forced to depend on interfaces they don't use. Prefer many small, specific interfaces over one large, general-purpose interface",
    "example": "Bad: One large interface\nInterface Worker {\n  work()\n  eat()\n  sleep()\n}\n\nRobotWorker must implement eat() and sleep() - makes no sense!\n\nGood: Segregated interfaces\n- Workable { work() }\n- Eatable { eat() }\n- Sleepable { sleep() }\n\nHumanWorker implements Workable, Eatable, Sleepable\nRobotWorker implements Workable only\n\nReal-world: Twitter API\n- ReadAPI: getTweets(), getUser()\n- WriteAPI: postTweet(), deleteTweet()\n- AdminAPI: banUser(), getAnalytics()\n\nMobile app only needs Read + Write, not Admin."
  },
  {
    "id": 5,
    "question": "What is the Dependency Inversion Principle (DIP)?",
    "answer": "High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details; details should depend on abstractions",
    "example": "Bad: Direct dependency on implementation\nclass OrderService {\n  private MySQLDatabase db = new MySQLDatabase();\n  // Tightly coupled to MySQL\n}\n\nGood: Depend on abstraction\nclass OrderService {\n  private Database db;  // Interface\n  constructor(Database db) { this.db = db; }  // Injected\n}\n\nNow OrderService works with MySQL, PostgreSQL, MongoDB...\n\nReal-world: E-commerce checkout\n- CheckoutService depends on PaymentGateway interface\n- Can swap Stripe for Adyen without changing CheckoutService\n- Enables easy testing with mock payment gateway"
  },
  {
    "id": 6,
    "question": "What is Domain-Driven Design (DDD)?",
    "answer": "A software design approach focusing on modeling the core business domain. Uses ubiquitous language shared by developers and domain experts. Organizes code around business concepts, not technical layers",
    "example": "E-commerce domain concepts:\n\nEntities (identity matters):\n- Order (has unique ID, lifecycle)\n- Customer (has unique ID, persists over time)\n\nValue Objects (identity doesn't matter):\n- Money (amount + currency, interchangeable)\n- Address (street, city, zip)\n\nAggregates (consistency boundary):\n- Order aggregate contains OrderItems\n- Can't modify OrderItems without going through Order\n\nDomain Events:\n- OrderPlaced, PaymentReceived, OrderShipped\n\nBounded Contexts:\n- Sales context: Customer means \"buyer\"\n- Support context: Customer means \"ticket requester\"\n- Same word, different models"
  },
  {
    "id": 7,
    "question": "What is Test-Driven Development (TDD)?",
    "answer": "Write tests before implementation code. Red-Green-Refactor cycle: write a failing test (red), write minimal code to pass (green), then refactor. Ensures testable design and high coverage",
    "example": "Building a shopping cart:\n\n1. RED - Write failing test:\ntest('empty cart has zero total') {\n  cart = new Cart()\n  assert cart.total() == 0\n}\n// Fails: Cart doesn't exist\n\n2. GREEN - Minimal implementation:\nclass Cart {\n  total() { return 0 }\n}\n// Test passes\n\n3. RED - Next test:\ntest('cart with one item') {\n  cart = new Cart()\n  cart.add(Item(price: 10))\n  assert cart.total() == 10\n}\n// Fails\n\n4. GREEN - Implement:\nclass Cart {\n  items = []\n  add(item) { items.push(item) }\n  total() { return items.sum(i => i.price) }\n}\n\n5. REFACTOR - Improve design without changing behavior"
  },
  {
    "id": 8,
    "question": "What is Clean Architecture?",
    "answer": "Architecture with dependencies pointing inward toward business logic. Inner layers (entities, use cases) don't know about outer layers (frameworks, databases). Enables technology-agnostic business rules",
    "example": "Layers (inner to outer):\n\n1. Entities (innermost)\n   - Pure business objects: User, Order, Product\n   - No framework dependencies\n\n2. Use Cases / Application\n   - Business rules: PlaceOrder, ProcessPayment\n   - Orchestrates entities\n\n3. Interface Adapters\n   - Controllers, Presenters, Gateways\n   - Converts data between layers\n\n4. Frameworks & Drivers (outermost)\n   - Database, Web framework, UI\n   - Implementation details\n\nDependency Rule:\n- Inner layers know nothing about outer layers\n- Database can change from MySQL to MongoDB\n- Web framework can change from Express to Fastify\n- Business logic remains unchanged"
  },
  {
    "id": 9,
    "question": "What is the difference between horizontal and vertical scaling?",
    "answer": "Vertical scaling (scale up): add more power to existing machine (CPU, RAM). Horizontal scaling (scale out): add more machines. Horizontal is preferred for high availability and unlimited growth",
    "example": "Twitter-like application:\n\nVertical Scaling:\n- Upgrade server: 8GB RAM → 128GB RAM\n- Limits: hardware maximums, single point of failure\n- Downtime during upgrades\n\nHorizontal Scaling:\n- Add more servers: 1 → 10 → 100 servers\n- Load balancer distributes traffic\n- No single point of failure\n- Scale infinitely (in theory)\n\nChallenges with horizontal:\n- Session management (use Redis)\n- Database becomes bottleneck\n- Need for data partitioning\n\nReal-world:\n- Instagram: horizontally scaled web servers\n- Database: vertically scaled initially, then sharded horizontally"
  },
  {
    "id": 10,
    "question": "What is database sharding?",
    "answer": "Horizontal partitioning of data across multiple database servers. Each shard holds a subset of data. Enables scaling beyond single-server limits but adds complexity for cross-shard queries",
    "example": "Twitter DMs sharding strategy:\n\nShard by user_id:\n- Shard 0: users 1-1M\n- Shard 1: users 1M-2M\n- Shard 2: users 2M-3M\n\nBenefits:\n- Each shard handles fewer users\n- Queries for one user hit one shard\n- Can add shards as users grow\n\nChallenges:\n- Conversation between users on different shards?\n- Solution: store in both shards or use conversation_id\n\nShard key selection is critical:\n- user_id: good for user-centric queries\n- timestamp: good for time-series, but hot shards\n- geographic region: good for latency\n\nResharding (adding shards) is painful - plan ahead!"
  },
  {
    "id": 11,
    "question": "What is CQRS (Command Query Responsibility Segregation)?",
    "answer": "Separate read (query) and write (command) operations into different models. Write model optimized for consistency, read model optimized for queries. Enables independent scaling and optimization",
    "example": "Twitter timeline design:\n\nWithout CQRS:\n- Read timeline: JOIN tweets, followers, users...\n- Slow, complex query on every load\n\nWith CQRS:\n\nWrite Model (commands):\n- PostTweet command → store in Tweets table\n- Optimized for writes, normalized\n\nRead Model (queries):\n- Pre-computed timeline per user\n- Denormalized, optimized for fast reads\n- When tweet posted → fan-out to followers' timelines\n\nSync between models:\n- Event-driven: TweetPosted event updates read model\n- Eventually consistent (acceptable for timelines)\n\nBenefits:\n- Scale reads and writes independently\n- Optimize each for its purpose\n- 1000x more reads than writes? Scale read replicas."
  },
  {
    "id": 12,
    "question": "What is Event Sourcing?",
    "answer": "Store state as a sequence of events rather than current state. Events are immutable facts. Current state is derived by replaying events. Provides complete audit trail and enables temporal queries",
    "example": "Bank account with Event Sourcing:\n\nTraditional: Store current balance = $500\n\nEvent Sourcing: Store events\n1. AccountOpened { id: 123 }\n2. MoneyDeposited { amount: 1000 }\n3. MoneyWithdrawn { amount: 200 }\n4. MoneyWithdrawn { amount: 300 }\n\nCurrent balance = replay events = $500\n\nBenefits:\n- Full audit trail (compliance, debugging)\n- \"What was balance on March 15?\" - replay to that point\n- Rebuild read models from events\n- Fix bugs by replaying with corrected logic\n\nReal-world uses:\n- Banking transactions\n- Shopping cart changes\n- Document collaboration (like Google Docs)\n- Git (commits are events)"
  },
  {
    "id": 13,
    "question": "How would you design a rate limiter?",
    "answer": "Control request rate per user/IP to prevent abuse. Algorithms: Token Bucket (bursty), Leaky Bucket (smooth), Fixed Window, Sliding Window. Consider distributed rate limiting for multiple servers",
    "example": "Twitter API rate limiting:\n\nRequirements:\n- 100 requests per 15 minutes per user\n- Return 429 Too Many Requests when exceeded\n\nToken Bucket Algorithm:\n- Bucket holds max 100 tokens\n- Refills at 100 tokens / 15 min\n- Each request consumes 1 token\n- Allow bursts up to bucket size\n\nDistributed implementation:\nRedis for shared state:\n  MULTI\n  INCR user:123:requests\n  EXPIRE user:123:requests 900\n  EXEC\n\nSliding Window (more accurate):\n- Track timestamp of each request\n- Count requests in last 15 minutes\n- More memory but smoother limiting\n\nResponse headers:\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 45\nX-RateLimit-Reset: 1234567890"
  },
  {
    "id": 14,
    "question": "What is the CAP theorem?",
    "answer": "Distributed systems can only guarantee two of three properties: Consistency (all nodes see same data), Availability (every request gets response), Partition tolerance (system works despite network failures). Must choose CP or AP",
    "example": "Real-world trade-offs:\n\nCP Systems (Consistency + Partition Tolerance):\n- Bank transactions: balance must be accurate\n- Inventory: can't oversell\n- During partition: refuse requests rather than risk inconsistency\n- Examples: MongoDB (default), HBase, Redis Cluster\n\nAP Systems (Availability + Partition Tolerance):\n- Social media likes: ok if count is slightly off\n- DNS: better to serve stale data than nothing\n- During partition: serve requests, sync later\n- Examples: Cassandra, DynamoDB, CouchDB\n\nTwitter example:\n- Tweet storage: AP (eventual consistency ok)\n- User authentication: CP (must be correct)\n- Like counts: AP (approximate is fine)\n- Direct messages: CP (can't lose messages)"
  },
  {
    "id": 15,
    "question": "What is database replication and what are the strategies?",
    "answer": "Copying data across multiple database servers for availability, fault tolerance, and read scaling. Strategies: master-slave (single writer), master-master (multiple writers), synchronous vs asynchronous",
    "example": "E-commerce database replication:\n\nMaster-Slave (Primary-Replica):\n- 1 master: handles all writes\n- N slaves: handle reads (scale horizontally)\n- Async replication: slight lag acceptable for product pages\n- Failover: promote slave to master if master dies\n\nMaster-Master:\n- Both accept writes\n- Conflict resolution needed\n- Use case: multi-region active-active\n\nSync vs Async:\n- Synchronous: write confirmed after all replicas ACK\n  - Stronger consistency, higher latency\n  - Critical data: payments, inventory\n- Asynchronous: write confirmed after master ACK\n  - Lower latency, risk of data loss\n  - Less critical: product views, recommendations\n\nRead-your-writes consistency:\n- After user posts, read from master briefly\n- Then safe to read from replicas"
  },
  {
    "id": 16,
    "question": "What is a Message Queue and when would you use it?",
    "answer": "Asynchronous communication between services via messages stored in a queue. Decouples producers from consumers. Use for: background jobs, traffic spikes, service decoupling, event distribution",
    "example": "Instagram post upload:\n\nWithout queue (synchronous):\n1. Upload image\n2. Wait for resize (multiple sizes)\n3. Wait for face detection\n4. Wait for storage\n5. Wait for CDN propagation\n6. Finally return to user (30 seconds!)\n\nWith queue (asynchronous):\n1. Upload image to temp storage\n2. Return success to user immediately\n3. Queue message: \"process image abc123\"\n\nBackground workers consume queue:\n- Worker 1: resize images\n- Worker 2: face detection\n- Worker 3: upload to CDN\n- Worker 4: update database\n- Notify user when complete\n\nBenefits:\n- Fast user response\n- Retry failed jobs\n- Scale workers independently\n- Handle traffic spikes (queue buffers)\n\nTools: RabbitMQ, Kafka, SQS, Redis Streams"
  },
  {
    "id": 17,
    "question": "What is a CDN and how does it improve performance?",
    "answer": "Content Delivery Network caches content at edge locations worldwide. Reduces latency by serving from nearby servers. Used for static assets, videos, and sometimes dynamic content",
    "example": "Netflix streaming architecture:\n\nWithout CDN:\n- All users fetch from origin in Virginia\n- User in Tokyo: 200ms latency per request\n- Origin server overloaded\n\nWith CDN:\n- Content cached at 100+ edge locations\n- User in Tokyo: served from Tokyo edge (10ms)\n- Origin only handles cache misses\n\nWhat to cache:\n- Static: images, CSS, JS, videos (long TTL)\n- Semi-static: product pages (short TTL)\n- Dynamic: personalized content (don't cache or edge compute)\n\nCache invalidation strategies:\n- TTL expiration\n- Purge API (instant invalidation)\n- Versioned URLs: style.v2.css\n\nCDN providers: CloudFlare, Akamai, CloudFront, Fastly\n\nTwitter uses CDN for:\n- Profile images\n- Media attachments\n- Static web assets"
  },
  {
    "id": 18,
    "question": "What is the Repository pattern?",
    "answer": "Abstracts data access behind a collection-like interface. Hides database implementation details from business logic. Enables switching databases and simplifies testing with in-memory implementations",
    "example": "E-commerce order management:\n\nRepository Interface:\nOrderRepository {\n  findById(id): Order\n  findByCustomer(customerId): Order[]\n  save(order): void\n  delete(id): void\n}\n\nImplementations:\n- PostgresOrderRepository: production database\n- InMemoryOrderRepository: unit tests\n- CachedOrderRepository: wraps real repo with cache\n\nUsage in business logic:\nclass OrderService {\n  constructor(orderRepo: OrderRepository) {}\n  \n  cancelOrder(id) {\n    order = orderRepo.findById(id)\n    order.cancel()\n    orderRepo.save(order)\n  }\n}\n\nBenefits:\n- Business logic doesn't know about SQL/MongoDB\n- Easy to test with fake repository\n- Can swap database without changing business code\n- Single place for query optimization"
  },
  {
    "id": 19,
    "question": "How would you design a URL shortener like bit.ly?",
    "answer": "Generate short codes for long URLs, redirect on access. Key decisions: encoding scheme, collision handling, analytics tracking, expiration. Scale with caching and database sharding",
    "example": "Requirements:\n- Shorten: long URL → short code\n- Redirect: short code → original URL\n- 100M URLs/month, 10B redirects/month\n\nDesign:\n\n1. Short code generation:\n- Base62 encoding (a-z, A-Z, 0-9)\n- 7 characters = 62^7 = 3.5 trillion combinations\n- Counter-based: ID 1 → \"0000001\" encoded\n- Or random + collision check\n\n2. Storage:\n- Key-value store: shortCode → {longUrl, created, expires}\n- Shard by short code prefix\n\n3. Read path (hot):\n- Check cache (Redis) first\n- 99% cache hit rate for popular links\n- Fallback to database\n- 301 redirect (cacheable) vs 302 (trackable)\n\n4. Write path:\n- Generate code, check uniqueness\n- Store in DB, add to cache\n\n5. Analytics:\n- Async logging to Kafka\n- Batch process for click stats"
  },
  {
    "id": 20,
    "question": "What is the difference between REST and GraphQL?",
    "answer": "REST uses fixed endpoints returning predetermined data. GraphQL uses single endpoint where client specifies exact data needed. GraphQL reduces over/under-fetching but adds complexity",
    "example": "Fetching user profile with posts:\n\nREST:\nGET /users/123           → {id, name, email}\nGET /users/123/posts     → [{id, title, body}, ...]\nGET /users/123/followers → [{id, name}, ...]\n\n3 requests, might return unused fields\n\nGraphQL:\nPOST /graphql\nquery {\n  user(id: 123) {\n    name\n    posts(limit: 5) {\n      title\n    }\n    followersCount\n  }\n}\n\n1 request, exact fields needed\n\nWhen to use REST:\n- Simple CRUD APIs\n- Caching important (HTTP caching easy)\n- Team familiar with REST\n\nWhen to use GraphQL:\n- Mobile apps (bandwidth sensitive)\n- Multiple clients with different data needs\n- Rapidly evolving frontend requirements\n\nTwitter/Facebook use GraphQL for mobile apps."
  },
  {
    "id": 21,
    "question": "What is the Saga pattern for distributed transactions?",
    "answer": "Manage distributed transactions as a sequence of local transactions with compensating actions for rollback. Each service completes its transaction and triggers the next. If one fails, execute compensations in reverse",
    "example": "E-commerce order flow:\n\nTraditional transaction (doesn't work distributed):\nBEGIN\n  deduct inventory\n  charge payment\n  create shipping\nCOMMIT\n\nSaga pattern:\n\n1. Order Service: Create order (pending)\n2. Inventory Service: Reserve items\n   - Compensation: Release reservation\n3. Payment Service: Charge card\n   - Compensation: Refund payment\n4. Shipping Service: Schedule delivery\n   - Compensation: Cancel shipment\n5. Order Service: Mark complete\n\nIf Payment fails:\n- Execute compensations in reverse\n- Release inventory reservation\n- Cancel order\n\nOrchestration vs Choreography:\n- Orchestrator: central coordinator manages saga\n- Choreography: services react to events, no coordinator\n\nReal-world: Uber ride booking, Airbnb reservations"
  },
  {
    "id": 22,
    "question": "What is the Circuit Breaker pattern?",
    "answer": "Prevent cascading failures by failing fast when a service is unhealthy. States: Closed (normal), Open (failing fast), Half-Open (testing recovery). Protects system resources and enables graceful degradation",
    "example": "Payment service calling fraud detection:\n\nWithout circuit breaker:\n- Fraud service down\n- Payment requests timeout (30s each)\n- Thread pool exhausted\n- Payment service becomes unresponsive\n- Cascading failure to checkout, cart...\n\nWith circuit breaker:\n\nClosed state (normal):\n- Requests pass through\n- Track failure rate\n\nOpen state (after 5 failures in 1 minute):\n- Fail immediately, don't call fraud service\n- Return fallback: \"approve with limit\" or queue for later\n- Users not blocked\n\nHalf-open state (after 30 seconds):\n- Allow one test request\n- If succeeds → Closed\n- If fails → Open again\n\nConfiguration:\n- Failure threshold: 5 failures\n- Timeout: 30 seconds\n- Half-open requests: 1\n\nTools: Hystrix, Resilience4j, Polly"
  },
  {
    "id": 23,
    "question": "How would you design a notification system?",
    "answer": "Multi-channel delivery (push, email, SMS) with user preferences, rate limiting, and delivery tracking. Use message queues for async processing and handle retries for failed deliveries",
    "example": "Instagram notification system:\n\nRequirements:\n- Push, email, SMS channels\n- User preferences (per channel, per type)\n- Don't spam users\n- Track delivery/read status\n\nArchitecture:\n\n1. Event occurs (new like, comment, follow)\n   → Publish to notification queue\n\n2. Notification Service consumes:\n   - Check user preferences\n   - Apply rate limiting (max 10/hour)\n   - Aggregate similar notifications (\"5 people liked...\")\n\n3. Route to channel services:\n   - Push Service → APNs/FCM\n   - Email Service → SendGrid\n   - SMS Service → Twilio\n\n4. Delivery tracking:\n   - Store: sent, delivered, read timestamps\n   - Retry failed deliveries with backoff\n\nPriority handling:\n- Security alerts: immediate, all channels\n- Likes: batched, push only\n- Marketing: respect quiet hours"
  },
  {
    "id": 24,
    "question": "What is the Strangler Fig pattern for migration?",
    "answer": "Incrementally replace legacy system by routing features to new system one by one. Facade intercepts requests and routes to old or new system. Eventually legacy is completely replaced (strangled)",
    "example": "Migrating monolith to microservices:\n\nLegacy monolith handles:\n- User management\n- Products\n- Orders\n- Payments\n\nPhase 1: Add facade/API gateway\n- All traffic goes through gateway\n- Gateway routes everything to monolith\n\nPhase 2: Extract User service\n- Build new User microservice\n- Gateway routes /users/* to new service\n- Everything else still goes to monolith\n\nPhase 3: Extract Products service\n- Gateway: /products/* → Products service\n- /users/* → Users service\n- /orders/*, /payments/* → monolith\n\nPhase 4, 5...: Continue extracting\n\nFinal: Monolith is empty, decommission it\n\nBenefits:\n- Zero big-bang risk\n- Rollback: just change routing\n- Team learns incrementally\n- Production validation at each step"
  },
  {
    "id": 25,
    "question": "How would you design Twitter's like/unlike feature?",
    "answer": "Idempotent operations, eventual consistency for counts, fan-out for notifications. Store likes in normalized table, cache counts. Handle race conditions and prevent duplicate likes",
    "example": "Requirements:\n- Like/unlike tweets\n- Show like count on tweets\n- Show if current user liked\n- Handle millions of likes per minute\n\nData model:\nLikes table: (user_id, tweet_id, created_at)\nPrimary key: (user_id, tweet_id) - prevents duplicates\n\nTweets table: like_count (denormalized)\n\nLike operation:\n1. INSERT INTO likes (user_id, tweet_id)\n   ON CONFLICT DO NOTHING (idempotent)\n2. If inserted (not duplicate):\n   - Increment tweet.like_count (async)\n   - Queue notification to author\n\nRead path:\n- like_count: cached in Redis\n- \"Did I like?\": check cache or DB\n\nScale considerations:\n- Hot tweets (viral): like_count updated frequently\n- Solution: buffer increments, batch update\n- Or: approximate counts (Redis INCR, periodic sync)\n\nConsistency:\n- Exact count not critical\n- User's own like status must be consistent (read-your-writes)"
  },
  {
    "id": 26,
    "question": "What is the difference between optimistic and pessimistic locking?",
    "answer": "Pessimistic: lock resource before access, block others until done. Optimistic: no locks, detect conflicts at write time using version numbers. Optimistic scales better for read-heavy workloads",
    "example": "Inventory management during flash sale:\n\nPessimistic locking:\nBEGIN\nSELECT stock FROM products WHERE id=1 FOR UPDATE\n-- Row is locked, others wait\nUPDATE products SET stock = stock - 1\nCOMMIT\n-- Lock released\n\nProblem: 10,000 concurrent users = massive contention\n\nOptimistic locking:\n1. Read: stock=100, version=5\n2. User modifies locally\n3. Write: UPDATE products \n   SET stock=99, version=6\n   WHERE id=1 AND version=5\n4. If affected rows = 0, version changed → retry\n\nWhen to use:\n- Pessimistic: high contention, short transactions\n  Example: bank transfer between accounts\n  \n- Optimistic: low contention, long transactions\n  Example: editing a document (conflict rare)\n  \nE-commerce: often hybrid\n- Optimistic for cart updates\n- Pessimistic for final checkout"
  },
  {
    "id": 27,
    "question": "What is the Bulkhead pattern?",
    "answer": "Isolate components into pools so failure in one doesn't exhaust resources for others. Like ship bulkheads that contain flooding. Apply to thread pools, connections, and services",
    "example": "Microservices with shared thread pool:\n\nProblem without bulkhead:\n- Payment service slow (3rd party issue)\n- All threads waiting on payment\n- No threads left for product, search, cart\n- Entire system unresponsive\n\nWith bulkhead pattern:\n\nSeparate thread pools:\n- Payment pool: 20 threads max\n- Product pool: 50 threads max\n- Search pool: 30 threads max\n\nPayment service slow:\n- Payment pool exhausted (20 threads blocked)\n- Product and search pools unaffected\n- Users can still browse and add to cart\n\nImplementation levels:\n- Thread pool bulkhead (Hystrix)\n- Connection pool per service\n- Separate containers/pods per service\n- Separate clusters for critical vs non-critical\n\nReal-world: Netflix isolates recommendations\nfrom core playback functionality."
  },
  {
    "id": 28,
    "question": "How do you handle idempotency in distributed systems?",
    "answer": "Ensure operations produce same result regardless of how many times executed. Critical for retries and at-least-once delivery. Use idempotency keys, unique constraints, and conditional updates",
    "example": "Payment processing (must not double-charge):\n\nProblem:\n1. Client sends payment request\n2. Server processes, charges card\n3. Response lost (network issue)\n4. Client retries\n5. Customer charged twice!\n\nSolution - Idempotency key:\n\n1. Client generates unique key: \"pay_abc123\"\n2. Request: POST /payments {idempotency_key: \"pay_abc123\", amount: 100}\n\nServer logic:\n- Check if idempotency_key exists in DB\n- If exists: return stored result (no processing)\n- If not: process payment, store result with key\n\nDatabase:\nCREATE TABLE payments (\n  idempotency_key VARCHAR PRIMARY KEY,\n  result JSONB,\n  created_at TIMESTAMP\n)\n\nOther patterns:\n- Conditional updates: UPDATE WHERE version = X\n- Natural idempotency: DELETE (deleting twice = same result)\n- Unique constraints: prevent duplicate inserts"
  },
  {
    "id": 29,
    "question": "What are the trade-offs between SQL and NoSQL databases?",
    "answer": "SQL: ACID transactions, structured schemas, complex queries, vertical scaling. NoSQL: flexible schemas, horizontal scaling, eventual consistency, optimized for specific access patterns. Choose based on requirements",
    "example": "Choosing database for different use cases:\n\nUser accounts → SQL (PostgreSQL)\n- Structured data (name, email, password)\n- Transactions for balance updates\n- Complex queries for admin\n- Strong consistency required\n\nUser sessions → Key-Value NoSQL (Redis)\n- Simple lookup by session_id\n- Auto-expiration (TTL)\n- High read/write throughput\n- No relationships needed\n\nProduct catalog → Document NoSQL (MongoDB)\n- Varying attributes per product type\n- Embedded reviews for fast reads\n- Flexible schema evolution\n- Horizontal scaling\n\nSocial graph → Graph NoSQL (Neo4j)\n- \"Friends of friends\" queries\n- Relationship traversal\n- Not possible efficiently in SQL\n\nTime-series metrics → Time-series DB (InfluxDB)\n- Optimized for time-range queries\n- Automatic downsampling\n- High write throughput\n\nMany apps use multiple databases (polyglot persistence)."
  },
  {
    "id": 30,
    "question": "What is the API Gateway pattern?",
    "answer": "Single entry point for all client requests. Handles cross-cutting concerns: authentication, rate limiting, routing, load balancing, request transformation. Simplifies clients by aggregating microservices",
    "example": "E-commerce mobile app:\n\nWithout gateway:\n- App calls 10 different services directly\n- Each service handles auth separately\n- Mobile handles failures, retries, aggregation\n- Chattiness kills mobile battery/bandwidth\n\nWith API Gateway:\n\nClient → Gateway → Microservices\n\nGateway responsibilities:\n1. Authentication\n   - Validate JWT once\n   - Pass user context to services\n\n2. Request routing\n   - /users/* → User Service\n   - /products/* → Product Service\n\n3. Request aggregation\n   - GET /app/home\n   - Gateway calls: users, products, recommendations\n   - Returns single combined response\n\n4. Rate limiting\n   - 100 requests/minute per user\n\n5. Protocol translation\n   - REST for mobile\n   - gRPC for internal services\n\nTools: Kong, AWS API Gateway, Nginx, Envoy\n\nBackends for Frontends (BFF):\n- Separate gateways for web, mobile, IoT"
  },
  {
    "id": 31,
    "question": "What is Consistent Hashing and why is it important?",
    "answer": "A technique for distributing data across nodes where adding or removing nodes only requires remapping a small fraction of keys. Uses a hash ring where both keys and nodes are hashed to positions. Essential for distributed caches and databases",
    "example": "Distributed cache with 4 servers:\n\nNaive hashing: hash(key) % num_servers\n- Add server: ~75% of keys need remapping\n- Remove server: ~75% of keys need remapping\n- Cache miss storm!\n\nConsistent hashing:\n1. Hash servers to positions on ring (0 to 2^32)\n   - Server A: position 1000\n   - Server B: position 3000\n   - Server C: position 6000\n   - Server D: position 9000\n\n2. Hash keys to ring, walk clockwise to find server\n   - Key \"user:123\" hashes to 2500 → Server B\n   - Key \"user:456\" hashes to 7000 → Server D\n\n3. Add Server E at position 5000:\n   - Only keys between 3000-5000 move to E\n   - ~20% of keys remapped, not 75%!\n\nVirtual nodes:\n- Each server has multiple positions on ring\n- Better distribution, handles heterogeneous servers\n\nUsed by: Cassandra, DynamoDB, Memcached"
  },
  {
    "id": 32,
    "question": "What are different caching strategies?",
    "answer": "Cache-aside: app manages cache and DB separately. Read-through: cache loads from DB on miss. Write-through: writes go to cache and DB. Write-behind: writes to cache, async to DB. Refresh-ahead: proactively refresh before expiry",
    "example": "E-commerce product pages:\n\nCache-Aside (most common):\nread:\n  if cache.has(key):\n    return cache.get(key)\n  data = db.query(key)\n  cache.set(key, data, ttl=300)\n  return data\n\nwrite:\n  db.update(key, data)\n  cache.delete(key)  # Invalidate\n\nRead-Through:\n- Cache handles DB loading automatically\n- Simpler app code\n- Cache library does the work\n\nWrite-Through:\n- Write to cache first\n- Cache synchronously writes to DB\n- Strong consistency, higher write latency\n\nWrite-Behind (Write-Back):\n- Write to cache, return immediately\n- Cache async writes to DB\n- Fast writes, risk of data loss\n- Good for: analytics, view counts\n\nRefresh-Ahead:\n- Proactively refresh popular keys before TTL\n- No cache miss latency for hot data\n\nCombinations common:\n- Read: cache-aside\n- Write: write-through for critical, write-behind for counts"
  },
  {
    "id": 33,
    "question": "What are different load balancing algorithms?",
    "answer": "Round Robin: rotate through servers. Weighted Round Robin: more requests to higher-capacity servers. Least Connections: send to server with fewest active connections. IP Hash: same client always to same server. Random: simple random selection",
    "example": "Web application with 3 servers:\n\nRound Robin:\n- Request 1 → Server A\n- Request 2 → Server B\n- Request 3 → Server C\n- Request 4 → Server A...\nSimple but ignores server load\n\nWeighted Round Robin:\n- Server A (weight 3): 3 requests\n- Server B (weight 2): 2 requests\n- Server C (weight 1): 1 request\nGood for heterogeneous servers\n\nLeast Connections:\n- Server A: 10 connections\n- Server B: 5 connections\n- Server C: 8 connections\n- New request → Server B\nBest for long-lived connections (WebSocket)\n\nIP Hash:\nhash(client_ip) % num_servers\n- Same user always hits same server\n- Good for session affinity (no shared session store)\n- Problem: server removal affects many users\n\nLeast Response Time:\n- Track server response times\n- Send to fastest server\n- Good for latency-sensitive apps\n\nLayer 4 vs Layer 7:\n- L4: TCP level (faster, less flexible)\n- L7: HTTP level (can route by URL, headers)"
  },
  {
    "id": 34,
    "question": "What is Service Discovery?",
    "answer": "Mechanism for services to find and communicate with each other without hardcoded addresses. Essential in dynamic environments where instances scale up/down. Two patterns: client-side discovery and server-side discovery",
    "example": "Microservices on Kubernetes:\n\nProblem without service discovery:\n- Order Service needs Payment Service\n- Payment Service at 10.0.0.5:8080 (hardcoded)\n- Payment Service moves, scales, or restarts\n- Order Service fails!\n\nClient-side discovery:\n1. Service Registry stores service locations\n   - Payment Service: [10.0.0.5:8080, 10.0.0.6:8080]\n2. Order Service queries registry\n3. Order Service picks instance (client-side LB)\n4. Direct connection to Payment Service\n\nTools: Consul, Eureka, etcd\n\nServer-side discovery:\n1. Order Service calls \"payment-service\" DNS name\n2. Load balancer/proxy resolves to instance\n3. No registry client in Order Service\n\nTools: Kubernetes Services, AWS ALB\n\nHealth checks:\n- Services send heartbeats to registry\n- Unhealthy instances removed\n- New instances register on startup\n\nKubernetes example:\n- Service: \"payment-service.default.svc.cluster.local\"\n- kube-proxy handles discovery and load balancing"
  },
  {
    "id": 35,
    "question": "What is a Bloom Filter?",
    "answer": "A space-efficient probabilistic data structure for set membership testing. May return false positives but never false negatives. Uses multiple hash functions mapping to a bit array. Perfect for \"definitely not in set\" checks",
    "example": "Use case: Checking if username is taken\n\nWithout Bloom filter:\n- Every signup: query database\n- Expensive for 100M users\n\nWith Bloom filter:\n1. Initialize bit array of size m (e.g., 10M bits)\n2. For each existing username:\n   - Apply k hash functions (e.g., k=3)\n   - Set bits at resulting positions to 1\n\nChecking \"john_doe\":\n- Hash to positions: 42, 1337, 99999\n- If ANY bit is 0: definitely not taken\n- If ALL bits are 1: probably taken (check DB)\n\nFalse positive rate:\n- Depends on m (array size) and n (items)\n- 1% false positive rate is common\n- Never false negatives!\n\nReal-world uses:\n- Chrome: malicious URL check\n- Medium: article recommendations (avoid showing same)\n- Cassandra: check if row exists before disk read\n- Spell checkers: is word in dictionary?\n\nTrade-off:\n- Cannot delete items (use Counting Bloom Filter)\n- Cannot enumerate contents"
  },
  {
    "id": 36,
    "question": "What is Leader Election in distributed systems?",
    "answer": "Process of designating a single node as coordinator for specific tasks. Ensures only one node performs certain operations. Uses consensus algorithms or external coordination services. Must handle leader failure gracefully",
    "example": "Scenario: Scheduled job processing\n\nProblem:\n- 3 app servers, each with scheduler\n- 9 AM: send daily digest email\n- Without leader election: 3 emails sent!\n\nSolution: Leader election\n- One node becomes leader\n- Only leader runs scheduled jobs\n- Followers on standby\n\nImplementation approaches:\n\n1. Database-based:\nSELECT * FROM leader_lock\n  WHERE resource = 'scheduler'\n  FOR UPDATE NOWAIT\n- First to lock wins\n- Release on crash via timeout\n\n2. ZooKeeper / etcd:\n- Create ephemeral sequential node\n- Lowest sequence number is leader\n- Node deleted on disconnect → new election\n\n3. Redis SETNX:\nSETNX leader_key \"node-1\" EX 30\n- First to set wins\n- Leader renews every 10s\n- Expire at 30s handles crashes\n\nLeader responsibilities:\n- Send heartbeats to maintain leadership\n- Graceful handoff before shutdown\n- Followers monitor leader health\n\nUsed in: Kafka (partition leader), ElasticSearch (master)"
  },
  {
    "id": 37,
    "question": "What is the Gossip Protocol?",
    "answer": "A peer-to-peer communication protocol where nodes periodically exchange state information with random peers. Information spreads exponentially like rumors. Provides eventual consistency and failure detection without central coordinator",
    "example": "Cassandra cluster membership:\n\nHow it works:\n1. Each node maintains membership list with heartbeat counters\n2. Every second, pick random peer\n3. Exchange membership info\n4. Merge: keep higher heartbeat counts\n\nExample:\nNode A knows: {A:100, B:99, C:98}\nNode B knows: {A:98, B:100, C:99}\n\nAfter gossip:\nBoth know: {A:100, B:100, C:99}\n\nFailure detection:\n- Node C stops sending heartbeats\n- Heartbeat counter stops incrementing\n- Other nodes notice C's counter stale\n- Mark C as suspected, then dead\n\nBenefits:\n- No single point of failure\n- Scalable: O(log n) rounds to spread info\n- Handles network partitions gracefully\n\nDrawbacks:\n- Eventual consistency (not immediate)\n- Bandwidth overhead from periodic messages\n\nUsed by:\n- Cassandra: membership and failure detection\n- Consul: service health propagation\n- Amazon S3: tracking server states"
  },
  {
    "id": 38,
    "question": "What are database indexing strategies?",
    "answer": "B-Tree: balanced tree for range queries and equality. Hash index: O(1) for equality only. Composite: multiple columns for common query patterns. Covering: includes all query columns to avoid table lookup. Partial: index subset of rows",
    "example": "E-commerce products table:\n\nB-Tree Index (default):\nCREATE INDEX idx_price ON products(price)\n- Good for: price > 100 AND price < 500\n- Good for: ORDER BY price\n- Sorted structure enables range scans\n\nHash Index:\nCREATE INDEX idx_sku ON products USING HASH(sku)\n- Good for: sku = 'ABC123' (O(1) lookup)\n- Bad for: ranges, sorting\n- Less common, specific use cases\n\nComposite Index:\nCREATE INDEX idx_cat_price ON products(category, price)\n- Excellent: category='electronics' AND price < 100\n- Good: category='electronics' (leftmost prefix)\n- Bad: price < 100 alone (can't use index)\n- Order matters!\n\nCovering Index:\nCREATE INDEX idx_cover ON products(category, price) INCLUDE (name)\n- Query needs only: category, price, name\n- All data in index, no table lookup\n- Faster but more storage\n\nPartial Index:\nCREATE INDEX idx_active ON products(name) WHERE status='active'\n- Only index active products (10% of data)\n- Smaller index, faster updates\n\nAnalyze queries with EXPLAIN before adding indexes."
  },
  {
    "id": 39,
    "question": "What is the difference between WebSockets, Server-Sent Events, and Long Polling?",
    "answer": "Long Polling: client polls, server holds until data available. SSE: server pushes over single HTTP connection, client receives. WebSockets: full duplex bidirectional communication over single TCP connection. Choose based on communication pattern",
    "example": "Real-time features comparison:\n\nLong Polling:\n- Client: GET /messages (waits)\n- Server: holds connection until new message\n- Server: responds, connection closes\n- Client: immediately polls again\n\nUse case: Simple notifications, legacy support\nDownside: Connection overhead, not truly real-time\n\nServer-Sent Events (SSE):\n- Client: EventSource('/stream')\n- Server: keeps connection, sends events\n- One-way: server → client only\n- Auto-reconnect built-in\n\nUse case: Live feeds, stock tickers, notifications\nDownside: No client-to-server messages\n\nWebSockets:\n- Full duplex on single TCP connection\n- Binary or text frames\n- Client and server can send anytime\n- ws:// or wss:// protocol\n\nUse case: Chat, gaming, collaborative editing\nDownside: More complex, needs sticky sessions\n\nDecision guide:\n- Notifications only? → SSE\n- Chat/bidirectional? → WebSockets\n- Legacy browser support? → Long Polling\n\nTwitter uses:\n- WebSockets for real-time notifications\n- Polling for less critical updates"
  },
  {
    "id": 40,
    "question": "What is Two-Phase Commit (2PC)?",
    "answer": "A distributed transaction protocol ensuring all nodes commit or all abort. Phase 1: Coordinator asks all participants to prepare. Phase 2: If all vote yes, coordinator sends commit; otherwise abort. Guarantees atomicity but blocks on coordinator failure",
    "example": "Bank transfer: $100 from Account A (Bank 1) to Account B (Bank 2)\n\nPhase 1 - Prepare:\n1. Coordinator: \"Prepare to debit $100 from A\"\n   Bank 1: locks A, writes to log, votes YES\n2. Coordinator: \"Prepare to credit $100 to B\"\n   Bank 2: locks B, writes to log, votes YES\n\nPhase 2 - Commit:\nIf all YES:\n  Coordinator: \"Commit\" to both\n  Bank 1: commits, unlocks A\n  Bank 2: commits, unlocks B\n\nIf any NO (or timeout):\n  Coordinator: \"Abort\" to all\n  Everyone rolls back\n\nProblems:\n1. Blocking: if coordinator crashes after prepare\n   - Participants stuck holding locks\n   - Can't decide commit or abort alone\n\n2. Performance: locks held during entire protocol\n   - High latency across data centers\n   - Not suitable for high-throughput\n\nAlternatives:\n- 3PC: adds pre-commit phase (non-blocking)\n- Saga: eventual consistency with compensations\n- TCC (Try-Confirm-Cancel): reservation pattern\n\nUsed in: Traditional databases, XA transactions"
  },
  {
    "id": 41,
    "question": "What are Blue-Green and Canary deployments?",
    "answer": "Blue-Green: maintain two identical environments, switch traffic instantly between them. Canary: gradually route small percentage of traffic to new version, increase if healthy. Both enable zero-downtime deploys and fast rollback",
    "example": "Deploying new version of payment service:\n\nBlue-Green Deployment:\n1. Blue (current): serving 100% traffic\n2. Deploy to Green (new): v2.0\n3. Run smoke tests on Green\n4. Switch load balancer: 100% → Green\n5. Keep Blue ready for instant rollback\n\nPros: Instant rollback, simple\nCons: Double infrastructure cost\n\nCanary Deployment:\n1. Current: 100% on v1.0\n2. Deploy canary: v2.0 to small subset\n3. Route 5% traffic to canary\n4. Monitor errors, latency, business metrics\n5. If healthy: 5% → 25% → 50% → 100%\n6. If unhealthy: rollback canary\n\nCanary selection strategies:\n- Random 5% of users\n- Internal employees first\n- Specific regions\n- Beta/premium users\n\nFeature flags complement both:\n- Deploy code but keep feature off\n- Enable for canary users via flag\n- Decouple deploy from release\n\nKubernetes supports both:\n- Blue-Green: switch Service selector\n- Canary: weighted traffic split"
  },
  {
    "id": 42,
    "question": "What is Observability and what are its three pillars?",
    "answer": "The ability to understand system internal state from external outputs. Three pillars: Logs (discrete events), Metrics (aggregated measurements), Traces (request flow across services). Essential for debugging distributed systems",
    "example": "Debugging slow checkout in microservices:\n\nLogs (What happened):\n- Discrete events with context\n- \"Order 123 created by user 456\"\n- \"Payment failed: timeout after 30s\"\n- Structured JSON for searchability\n- Tools: ELK Stack, Splunk, Datadog\n\nMetrics (How much/how fast):\n- Aggregated numbers over time\n- Request rate: 1000 req/s\n- Error rate: 2%\n- P99 latency: 500ms\n- CPU, memory, connections\n- Tools: Prometheus + Grafana, CloudWatch\n\nTraces (Where did time go):\n- Follow request across services\n- Checkout → Order → Inventory → Payment\n- See which service is slow\n- Span: single operation\n- Trace: collection of spans\n- Tools: Jaeger, Zipkin, AWS X-Ray\n\nCorrelation ID:\n- Generate unique ID at entry point\n- Pass through all services\n- Search logs by correlation ID\n- Link traces to logs\n\nAlert on metrics, search logs, trace specific requests."
  },
  {
    "id": 43,
    "question": "What is Back Pressure in distributed systems?",
    "answer": "A flow control mechanism where downstream systems signal upstream to slow down when overwhelmed. Prevents cascade failures from overload. Implemented via queue limits, rate limiting, or load shedding",
    "example": "Video processing pipeline:\n\nUpload → Transcode → Store → Notify\n\nProblem without back pressure:\n- Marketing campaign: 10x uploads\n- Transcode queue grows unbounded\n- Memory exhausted, service crashes\n- All pending jobs lost\n\nWith back pressure:\n\n1. Bounded queues:\n- Transcode queue max: 1000 jobs\n- When full: reject new uploads\n- Client sees 429 Too Many Requests\n- Retry with exponential backoff\n\n2. Rate limiting at entry:\n- Max 100 uploads/second\n- Excess queued or rejected\n\n3. Load shedding:\n- When overloaded, reject low-priority work\n- Process premium users first\n- Better to serve some than crash serving none\n\n4. Circuit breaker:\n- When downstream slow, fail fast\n- Don't accumulate requests\n\nReactive Streams:\n- Consumer tells producer how many items to send\n- Producer only sends what consumer can handle\n- Built into: Project Reactor, RxJava, Akka Streams\n\nResult: System degrades gracefully instead of crashing."
  },
  {
    "id": 44,
    "question": "What is a Dead Letter Queue?",
    "answer": "A secondary queue for messages that cannot be processed successfully after multiple attempts. Isolates problematic messages to prevent blocking the main queue. Enables manual inspection and reprocessing",
    "example": "Order processing system:\n\nMain Queue: process_order\n- Message: {order_id: 123, items: [...]}\n- Worker tries to process\n\nFailure scenarios:\n- Invalid data: items array empty\n- External failure: payment service down\n- Bug: null pointer exception\n\nWithout DLQ:\n- Failed message retried forever\n- Or lost after max retries\n- Blocks other messages (head-of-line)\n- No visibility into failures\n\nWith DLQ:\n1. Attempt processing (retry 3 times)\n2. After 3 failures → move to DLQ\n3. Main queue continues processing\n4. DLQ accumulates poison messages\n\nDLQ handling:\n- Alert when DLQ not empty\n- Dashboard shows failed messages\n- Inspect message content\n- Fix bug, reprocess from DLQ\n- Or delete if invalid data\n\nAWS SQS configuration:\n{\n  \"maxReceiveCount\": 3,\n  \"deadLetterTargetArn\": \"arn:aws:sqs:dlq\"\n}\n\nDifferentiate errors:\n- Transient (retry): network timeout\n- Permanent (DLQ immediately): invalid data"
  },
  {
    "id": 45,
    "question": "How would you design a chat system like Slack?",
    "answer": "Real-time messaging using WebSockets for delivery. Messages stored in database, fanout to channel members. Presence tracking for online status. Support for channels, DMs, threads, file sharing, and search",
    "example": "Core components:\n\n1. Connection Service:\n- WebSocket connections per user\n- Horizontal scaling with sticky sessions\n- Heartbeat for connection health\n- Redis pub/sub for cross-server messaging\n\n2. Message Flow:\n- User sends: WS → Connection Service\n- Validate, store in Messages DB\n- Fanout to channel members:\n  - Small channel: direct delivery\n  - Large channel: queue + workers\n- Deliver via recipient's WebSocket\n\n3. Data Model:\n- Channels: {id, name, workspace_id, members[]}\n- Messages: {id, channel_id, user_id, content, ts}\n- Partition messages by channel_id\n\n4. Presence:\n- Track last_seen timestamp\n- Heartbeat every 30s updates Redis\n- \"Online\" if last_seen < 1 minute\n- Broadcast presence changes to relevant users\n\n5. Read receipts:\n- Store: {user_id, channel_id, last_read_ts}\n- Unread count: messages after last_read_ts\n\n6. Search:\n- Elasticsearch for full-text search\n- Index messages asynchronously\n\nScale considerations:\n- Hot channels: queue messages, batch process\n- Message ordering: sequence numbers per channel"
  },
  {
    "id": 46,
    "question": "What is the Sidecar pattern?",
    "answer": "Deploy auxiliary functionality as a separate process alongside the main application. Sidecar handles cross-cutting concerns like logging, monitoring, security, or networking. Enables language-agnostic features without modifying app code",
    "example": "Microservices with service mesh:\n\nWithout sidecar:\n- Each service implements:\n  - TLS termination\n  - Retry logic\n  - Circuit breakers\n  - Metrics collection\n  - Log shipping\n- Duplicate code in every language/service\n\nWith sidecar (Envoy proxy):\n\nPod:\n┌─────────────────────────┐\n│ ┌─────────┐ ┌─────────┐ │\n│ │  App    │ │ Sidecar │ │\n│ │ :8080   │ │ :15001  │ │\n│ └────┬────┘ └────┬────┘ │\n│      └─────┬─────┘      │\n└────────────┼────────────┘\n             ↓\n         Network\n\nSidecar handles:\n- Inbound: TLS, authentication, rate limiting\n- Outbound: retries, circuit breakers, load balancing\n- Observability: metrics, traces, logs\n\nApp only does business logic!\n\nService Mesh (Istio, Linkerd):\n- Deploys sidecars to all services\n- Centralized traffic management\n- Mutual TLS between all services\n- No code changes required\n\nOther sidecar uses:\n- Log collectors (Fluentd)\n- Secret management (Vault agent)\n- Configuration sync"
  },
  {
    "id": 47,
    "question": "What is Data Partitioning and what are common strategies?",
    "answer": "Dividing data across multiple nodes to distribute load and storage. Strategies: Range-based (by key ranges), Hash-based (by key hash), Directory-based (lookup table), Geographic (by region). Key choice affects query patterns",
    "example": "Social media posts table (1 billion rows):\n\nRange Partitioning:\n- Partition by created_date\n- Jan 2024: Partition 1\n- Feb 2024: Partition 2\n- Good for: time-range queries\n- Bad: recent partition always hot\n\nHash Partitioning:\n- partition = hash(user_id) % num_partitions\n- Uniform distribution\n- Good for: point queries by user_id\n- Bad: range queries scan all partitions\n\nComposite Partitioning:\n- First: hash by user_id (even distribution)\n- Then: range by date (within user's data)\n- Good for: \"user X's posts in March\"\n\nGeographic Partitioning:\n- US users: US data center\n- EU users: EU data center\n- Low latency for reads\n- GDPR compliance (data residency)\n\nDirectory-Based:\n- Lookup service maps key → partition\n- Flexible, can move data easily\n- Extra hop for every query\n\nHot partition solutions:\n- Split hot partitions\n- Add random suffix to hot keys\n- Cache hot data\n\nRebalancing challenges:\n- Adding partition: need to redistribute\n- Consistent hashing helps minimize movement"
  },
  {
    "id": 48,
    "question": "What is the Retry pattern with Exponential Backoff?",
    "answer": "Automatically retry failed operations with increasing delays between attempts. Prevents overwhelming recovering services. Add jitter (randomness) to avoid thundering herd. Define max retries and circuit breaker integration",
    "example": "Calling payment API:\n\nBasic retry (bad):\nfor i in range(5):\n  try: return api.charge()\n  except: continue\n\nProblem: 1000 clients retry immediately\n- Server recovering from overload\n- Instant retry = more overload\n- Thundering herd!\n\nExponential backoff:\nmax_retries = 5\nbase_delay = 1 second\n\nfor attempt in range(max_retries):\n  try:\n    return api.charge()\n  except RetryableError:\n    delay = base_delay * (2 ** attempt)\n    # attempt 0: 1s, 1: 2s, 2: 4s, 3: 8s, 4: 16s\n    sleep(delay)\n\nWith jitter (best):\ndelay = base_delay * (2 ** attempt)\ndelay = delay * random(0.5, 1.5)  # Add randomness\n\n- Clients retry at different times\n- Spreads load on recovering server\n\nWhat to retry:\n- Network timeouts: YES\n- 500 Server Error: YES\n- 429 Rate Limited: YES (respect Retry-After header)\n- 400 Bad Request: NO (won't succeed)\n- 401 Unauthorized: NO (need new credentials)\n\nCircuit breaker integration:\n- After N failures, stop retrying entirely\n- Fail fast instead of accumulating delays"
  },
  {
    "id": 49,
    "question": "How would you design a search autocomplete system?",
    "answer": "Pre-compute popular query prefixes with results. Use Trie data structure for prefix matching. Rank by popularity, personalization, and recency. Cache heavily as same prefixes repeat. Balance freshness vs computation cost",
    "example": "Google-like search suggestions:\n\nRequirements:\n- As user types, show top 10 suggestions\n- Sub-100ms latency\n- Handle 10K queries/second\n\nData structure - Trie:\n     (root)\n      /  \\\n     t    f\n    /      \\\n   tw      fa\n  /  \\       \\\n twi  two    fac\n  |          |\n twit       face\n  |          |\ntwitter  facebook\n\nEach node stores:\n- Top K suggestions for this prefix\n- Pre-computed, not calculated on query\n\nData pipeline:\n1. Collect search logs\n2. Aggregate query counts (hourly)\n3. Build/update Trie with top suggestions\n4. Replicate to query servers\n\nRanking factors:\n- Search frequency\n- Recency (trending topics)\n- User's past searches (personalization)\n- Geographic relevance\n\nOptimizations:\n- Cache popular prefixes in CDN\n- Start suggesting after 2-3 characters\n- Debounce requests (wait 100ms after keystroke)\n- Sampling for extremely popular prefixes\n\nFreshness:\n- Breaking news needs quick updates\n- Background job updates trending terms\n- Most suggestions stable, cache aggressively"
  },
  {
    "id": 50,
    "question": "What is the difference between Microservices and Monolith architecture?",
    "answer": "Monolith: single deployable unit containing all functionality. Microservices: multiple small services, each owning a specific domain, independently deployable. Microservices add complexity but enable team autonomy and independent scaling",
    "example": "E-commerce application:\n\nMonolith:\n┌─────────────────────────┐\n│    E-commerce App       │\n│  ┌─────┐ ┌─────┐ ┌────┐│\n│  │User │ │Order│ │Prod││\n│  └─────┘ └─────┘ └────┘│\n│    Single Database      │\n└─────────────────────────┘\n\nPros:\n- Simple development/deployment\n- Easy debugging (single process)\n- No network latency between modules\n- ACID transactions trivial\n\nCons:\n- Teams step on each other\n- Scale everything together\n- Long deploy times\n- One bug can crash all\n\nMicroservices:\n┌──────┐ ┌──────┐ ┌──────┐\n│ User │ │Order │ │ Prod │\n│ Svc  │ │ Svc  │ │ Svc  │\n│  DB  │ │  DB  │ │  DB  │\n└──────┘ └──────┘ └──────┘\n\nPros:\n- Independent deployment\n- Scale services individually\n- Team autonomy\n- Technology flexibility\n\nCons:\n- Network complexity\n- Distributed transactions hard\n- Operational overhead\n- Need DevOps maturity\n\nStart with monolith, split when:\n- Team grows beyond ~10 developers\n- Clear domain boundaries emerge\n- Different scaling needs\n- Deployment conflicts frequent"
  }
]
