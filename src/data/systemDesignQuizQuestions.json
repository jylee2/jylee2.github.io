[
  {
    "id": 1,
    "question": "What is the Single Responsibility Principle (SRP)?",
    "answer": "A class or module should have only one reason to change - meaning it should have only one job or responsibility. This improves maintainability, testability, and reduces coupling",
    "example": "Bad: UserService handles authentication, email sending, and database operations\n\nGood: Separate into focused classes:\n- AuthenticationService: handles login/logout\n- EmailService: handles sending emails\n- UserRepository: handles database operations\n\nReal-world example:\nTwitter's notification system is separate from tweet storage.\nChanging how notifications work shouldn't affect how tweets are stored."
  },
  {
    "id": 2,
    "question": "What is the Open/Closed Principle (OCP)?",
    "answer": "Software entities should be open for extension but closed for modification. Add new functionality by adding new code, not changing existing code. Achieved through abstraction and polymorphism",
    "example": "Scenario: Payment processing system\n\nBad: Adding PayPal requires modifying PaymentProcessor class\nif (type == 'stripe') { ... }\nelse if (type == 'paypal') { ... }  // Must modify existing code\n\nGood: Use abstractions\n- PaymentGateway interface with process() method\n- StripeGateway implements PaymentGateway\n- PayPalGateway implements PaymentGateway (new file, no changes to existing)\n\nAdding Apple Pay = create new ApplePayGateway class.\nExisting code remains untouched."
  },
  {
    "id": 3,
    "question": "What is the Liskov Substitution Principle (LSP)?",
    "answer": "Subtypes must be substitutable for their base types without altering program correctness. If S is a subtype of T, objects of type T can be replaced with objects of type S without breaking the application",
    "example": "Classic violation: Square extends Rectangle\n\nRectangle has setWidth() and setHeight() independently.\nSquare overrides them to keep width == height.\n\nProblem:\nrectangle.setWidth(5)\nrectangle.setHeight(10)\nassert rectangle.area() == 50  // Fails for Square!\n\nBetter design:\n- Shape interface with area() method\n- Rectangle and Square are separate implementations\n- Don't force inheritance where behavior differs\n\nReal-world: A \"ReadOnlyUser\" shouldn't extend \"User\" if User has save() method."
  },
  {
    "id": 4,
    "question": "What is the Interface Segregation Principle (ISP)?",
    "answer": "Clients should not be forced to depend on interfaces they don't use. Prefer many small, specific interfaces over one large, general-purpose interface",
    "example": "Bad: One large interface\nInterface Worker {\n  work()\n  eat()\n  sleep()\n}\n\nRobotWorker must implement eat() and sleep() - makes no sense!\n\nGood: Segregated interfaces\n- Workable { work() }\n- Eatable { eat() }\n- Sleepable { sleep() }\n\nHumanWorker implements Workable, Eatable, Sleepable\nRobotWorker implements Workable only\n\nReal-world: Twitter API\n- ReadAPI: getTweets(), getUser()\n- WriteAPI: postTweet(), deleteTweet()\n- AdminAPI: banUser(), getAnalytics()\n\nMobile app only needs Read + Write, not Admin."
  },
  {
    "id": 5,
    "question": "What is the Dependency Inversion Principle (DIP)?",
    "answer": "High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details; details should depend on abstractions",
    "example": "Bad: Direct dependency on implementation\nclass OrderService {\n  private MySQLDatabase db = new MySQLDatabase();\n  // Tightly coupled to MySQL\n}\n\nGood: Depend on abstraction\nclass OrderService {\n  private Database db;  // Interface\n  constructor(Database db) { this.db = db; }  // Injected\n}\n\nNow OrderService works with MySQL, PostgreSQL, MongoDB...\n\nReal-world: E-commerce checkout\n- CheckoutService depends on PaymentGateway interface\n- Can swap Stripe for Adyen without changing CheckoutService\n- Enables easy testing with mock payment gateway"
  },
  {
    "id": 6,
    "question": "What is Domain-Driven Design (DDD)?",
    "answer": "A software design approach focusing on modeling the core business domain. Uses ubiquitous language shared by developers and domain experts. Organizes code around business concepts, not technical layers",
    "example": "E-commerce domain concepts:\n\nEntities (identity matters):\n- Order (has unique ID, lifecycle)\n- Customer (has unique ID, persists over time)\n\nValue Objects (identity doesn't matter):\n- Money (amount + currency, interchangeable)\n- Address (street, city, zip)\n\nAggregates (consistency boundary):\n- Order aggregate contains OrderItems\n- Can't modify OrderItems without going through Order\n\nDomain Events:\n- OrderPlaced, PaymentReceived, OrderShipped\n\nBounded Contexts:\n- Sales context: Customer means \"buyer\"\n- Support context: Customer means \"ticket requester\"\n- Same word, different models"
  },
  {
    "id": 7,
    "question": "What is Test-Driven Development (TDD)?",
    "answer": "Write tests before implementation code. Red-Green-Refactor cycle: write a failing test (red), write minimal code to pass (green), then refactor. Ensures testable design and high coverage",
    "example": "Building a shopping cart:\n\n1. RED - Write failing test:\ntest('empty cart has zero total') {\n  cart = new Cart()\n  assert cart.total() == 0\n}\n// Fails: Cart doesn't exist\n\n2. GREEN - Minimal implementation:\nclass Cart {\n  total() { return 0 }\n}\n// Test passes\n\n3. RED - Next test:\ntest('cart with one item') {\n  cart = new Cart()\n  cart.add(Item(price: 10))\n  assert cart.total() == 10\n}\n// Fails\n\n4. GREEN - Implement:\nclass Cart {\n  items = []\n  add(item) { items.push(item) }\n  total() { return items.sum(i => i.price) }\n}\n\n5. REFACTOR - Improve design without changing behavior"
  },
  {
    "id": 8,
    "question": "What is Clean Architecture?",
    "answer": "Architecture with dependencies pointing inward toward business logic. Inner layers (entities, use cases) don't know about outer layers (frameworks, databases). Enables technology-agnostic business rules",
    "example": "Layers (inner to outer):\n\n1. Entities (innermost)\n   - Pure business objects: User, Order, Product\n   - No framework dependencies\n\n2. Use Cases / Application\n   - Business rules: PlaceOrder, ProcessPayment\n   - Orchestrates entities\n\n3. Interface Adapters\n   - Controllers, Presenters, Gateways\n   - Converts data between layers\n\n4. Frameworks & Drivers (outermost)\n   - Database, Web framework, UI\n   - Implementation details\n\nDependency Rule:\n- Inner layers know nothing about outer layers\n- Database can change from MySQL to MongoDB\n- Web framework can change from Express to Fastify\n- Business logic remains unchanged"
  },
  {
    "id": 9,
    "question": "What is the difference between horizontal and vertical scaling?",
    "answer": "Vertical scaling (scale up): add more power to existing machine (CPU, RAM). Horizontal scaling (scale out): add more machines. Horizontal is preferred for high availability and unlimited growth",
    "example": "Twitter-like application:\n\nVertical Scaling:\n- Upgrade server: 8GB RAM → 128GB RAM\n- Limits: hardware maximums, single point of failure\n- Downtime during upgrades\n\nHorizontal Scaling:\n- Add more servers: 1 → 10 → 100 servers\n- Load balancer distributes traffic\n- No single point of failure\n- Scale infinitely (in theory)\n\nChallenges with horizontal:\n- Session management (use Redis)\n- Database becomes bottleneck\n- Need for data partitioning\n\nReal-world:\n- Instagram: horizontally scaled web servers\n- Database: vertically scaled initially, then sharded horizontally"
  },
  {
    "id": 10,
    "question": "What is database sharding?",
    "answer": "Horizontal partitioning of data across multiple database servers. Each shard holds a subset of data. Enables scaling beyond single-server limits but adds complexity for cross-shard queries",
    "example": "Twitter DMs sharding strategy:\n\nShard by user_id:\n- Shard 0: users 1-1M\n- Shard 1: users 1M-2M\n- Shard 2: users 2M-3M\n\nBenefits:\n- Each shard handles fewer users\n- Queries for one user hit one shard\n- Can add shards as users grow\n\nChallenges:\n- Conversation between users on different shards?\n- Solution: store in both shards or use conversation_id\n\nShard key selection is critical:\n- user_id: good for user-centric queries\n- timestamp: good for time-series, but hot shards\n- geographic region: good for latency\n\nResharding (adding shards) is painful - plan ahead!"
  },
  {
    "id": 11,
    "question": "What is CQRS (Command Query Responsibility Segregation)?",
    "answer": "Separate read (query) and write (command) operations into different models. Write model optimized for consistency, read model optimized for queries. Enables independent scaling and optimization",
    "example": "Twitter timeline design:\n\nWithout CQRS:\n- Read timeline: JOIN tweets, followers, users...\n- Slow, complex query on every load\n\nWith CQRS:\n\nWrite Model (commands):\n- PostTweet command → store in Tweets table\n- Optimized for writes, normalized\n\nRead Model (queries):\n- Pre-computed timeline per user\n- Denormalized, optimized for fast reads\n- When tweet posted → fan-out to followers' timelines\n\nSync between models:\n- Event-driven: TweetPosted event updates read model\n- Eventually consistent (acceptable for timelines)\n\nBenefits:\n- Scale reads and writes independently\n- Optimize each for its purpose\n- 1000x more reads than writes? Scale read replicas."
  },
  {
    "id": 12,
    "question": "What is Event Sourcing?",
    "answer": "Store state as a sequence of events rather than current state. Events are immutable facts. Current state is derived by replaying events. Provides complete audit trail and enables temporal queries",
    "example": "Bank account with Event Sourcing:\n\nTraditional: Store current balance = $500\n\nEvent Sourcing: Store events\n1. AccountOpened { id: 123 }\n2. MoneyDeposited { amount: 1000 }\n3. MoneyWithdrawn { amount: 200 }\n4. MoneyWithdrawn { amount: 300 }\n\nCurrent balance = replay events = $500\n\nBenefits:\n- Full audit trail (compliance, debugging)\n- \"What was balance on March 15?\" - replay to that point\n- Rebuild read models from events\n- Fix bugs by replaying with corrected logic\n\nReal-world uses:\n- Banking transactions\n- Shopping cart changes\n- Document collaboration (like Google Docs)\n- Git (commits are events)"
  },
  {
    "id": 13,
    "question": "How would you design a rate limiter?",
    "answer": "Control request rate per user/IP to prevent abuse. Algorithms: Token Bucket (bursty), Leaky Bucket (smooth), Fixed Window, Sliding Window. Consider distributed rate limiting for multiple servers",
    "example": "Twitter API rate limiting:\n\nRequirements:\n- 100 requests per 15 minutes per user\n- Return 429 Too Many Requests when exceeded\n\nToken Bucket Algorithm:\n- Bucket holds max 100 tokens\n- Refills at 100 tokens / 15 min\n- Each request consumes 1 token\n- Allow bursts up to bucket size\n\nDistributed implementation:\nRedis for shared state:\n  MULTI\n  INCR user:123:requests\n  EXPIRE user:123:requests 900\n  EXEC\n\nSliding Window (more accurate):\n- Track timestamp of each request\n- Count requests in last 15 minutes\n- More memory but smoother limiting\n\nResponse headers:\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 45\nX-RateLimit-Reset: 1234567890"
  },
  {
    "id": 14,
    "question": "What is the CAP theorem?",
    "answer": "Distributed systems can only guarantee two of three properties: Consistency (all nodes see same data), Availability (every request gets response), Partition tolerance (system works despite network failures). Must choose CP or AP",
    "example": "Real-world trade-offs:\n\nCP Systems (Consistency + Partition Tolerance):\n- Bank transactions: balance must be accurate\n- Inventory: can't oversell\n- During partition: refuse requests rather than risk inconsistency\n- Examples: MongoDB (default), HBase, Redis Cluster\n\nAP Systems (Availability + Partition Tolerance):\n- Social media likes: ok if count is slightly off\n- DNS: better to serve stale data than nothing\n- During partition: serve requests, sync later\n- Examples: Cassandra, DynamoDB, CouchDB\n\nTwitter example:\n- Tweet storage: AP (eventual consistency ok)\n- User authentication: CP (must be correct)\n- Like counts: AP (approximate is fine)\n- Direct messages: CP (can't lose messages)"
  },
  {
    "id": 15,
    "question": "What is database replication and what are the strategies?",
    "answer": "Copying data across multiple database servers for availability, fault tolerance, and read scaling. Strategies: master-slave (single writer), master-master (multiple writers), synchronous vs asynchronous",
    "example": "E-commerce database replication:\n\nMaster-Slave (Primary-Replica):\n- 1 master: handles all writes\n- N slaves: handle reads (scale horizontally)\n- Async replication: slight lag acceptable for product pages\n- Failover: promote slave to master if master dies\n\nMaster-Master:\n- Both accept writes\n- Conflict resolution needed\n- Use case: multi-region active-active\n\nSync vs Async:\n- Synchronous: write confirmed after all replicas ACK\n  - Stronger consistency, higher latency\n  - Critical data: payments, inventory\n- Asynchronous: write confirmed after master ACK\n  - Lower latency, risk of data loss\n  - Less critical: product views, recommendations\n\nRead-your-writes consistency:\n- After user posts, read from master briefly\n- Then safe to read from replicas"
  },
  {
    "id": 16,
    "question": "What is a Message Queue and when would you use it?",
    "answer": "Asynchronous communication between services via messages stored in a queue. Decouples producers from consumers. Use for: background jobs, traffic spikes, service decoupling, event distribution",
    "example": "Instagram post upload:\n\nWithout queue (synchronous):\n1. Upload image\n2. Wait for resize (multiple sizes)\n3. Wait for face detection\n4. Wait for storage\n5. Wait for CDN propagation\n6. Finally return to user (30 seconds!)\n\nWith queue (asynchronous):\n1. Upload image to temp storage\n2. Return success to user immediately\n3. Queue message: \"process image abc123\"\n\nBackground workers consume queue:\n- Worker 1: resize images\n- Worker 2: face detection\n- Worker 3: upload to CDN\n- Worker 4: update database\n- Notify user when complete\n\nBenefits:\n- Fast user response\n- Retry failed jobs\n- Scale workers independently\n- Handle traffic spikes (queue buffers)\n\nTools: RabbitMQ, Kafka, SQS, Redis Streams"
  },
  {
    "id": 17,
    "question": "What is a CDN and how does it improve performance?",
    "answer": "Content Delivery Network caches content at edge locations worldwide. Reduces latency by serving from nearby servers. Used for static assets, videos, and sometimes dynamic content",
    "example": "Netflix streaming architecture:\n\nWithout CDN:\n- All users fetch from origin in Virginia\n- User in Tokyo: 200ms latency per request\n- Origin server overloaded\n\nWith CDN:\n- Content cached at 100+ edge locations\n- User in Tokyo: served from Tokyo edge (10ms)\n- Origin only handles cache misses\n\nWhat to cache:\n- Static: images, CSS, JS, videos (long TTL)\n- Semi-static: product pages (short TTL)\n- Dynamic: personalized content (don't cache or edge compute)\n\nCache invalidation strategies:\n- TTL expiration\n- Purge API (instant invalidation)\n- Versioned URLs: style.v2.css\n\nCDN providers: CloudFlare, Akamai, CloudFront, Fastly\n\nTwitter uses CDN for:\n- Profile images\n- Media attachments\n- Static web assets"
  },
  {
    "id": 18,
    "question": "What is the Repository pattern?",
    "answer": "Abstracts data access behind a collection-like interface. Hides database implementation details from business logic. Enables switching databases and simplifies testing with in-memory implementations",
    "example": "E-commerce order management:\n\nRepository Interface:\nOrderRepository {\n  findById(id): Order\n  findByCustomer(customerId): Order[]\n  save(order): void\n  delete(id): void\n}\n\nImplementations:\n- PostgresOrderRepository: production database\n- InMemoryOrderRepository: unit tests\n- CachedOrderRepository: wraps real repo with cache\n\nUsage in business logic:\nclass OrderService {\n  constructor(orderRepo: OrderRepository) {}\n  \n  cancelOrder(id) {\n    order = orderRepo.findById(id)\n    order.cancel()\n    orderRepo.save(order)\n  }\n}\n\nBenefits:\n- Business logic doesn't know about SQL/MongoDB\n- Easy to test with fake repository\n- Can swap database without changing business code\n- Single place for query optimization"
  },
  {
    "id": 19,
    "question": "How would you design a URL shortener like bit.ly?",
    "answer": "Generate short codes for long URLs, redirect on access. Key decisions: encoding scheme, collision handling, analytics tracking, expiration. Scale with caching and database sharding",
    "example": "Requirements:\n- Shorten: long URL → short code\n- Redirect: short code → original URL\n- 100M URLs/month, 10B redirects/month\n\nDesign:\n\n1. Short code generation:\n- Base62 encoding (a-z, A-Z, 0-9)\n- 7 characters = 62^7 = 3.5 trillion combinations\n- Counter-based: ID 1 → \"0000001\" encoded\n- Or random + collision check\n\n2. Storage:\n- Key-value store: shortCode → {longUrl, created, expires}\n- Shard by short code prefix\n\n3. Read path (hot):\n- Check cache (Redis) first\n- 99% cache hit rate for popular links\n- Fallback to database\n- 301 redirect (cacheable) vs 302 (trackable)\n\n4. Write path:\n- Generate code, check uniqueness\n- Store in DB, add to cache\n\n5. Analytics:\n- Async logging to Kafka\n- Batch process for click stats"
  },
  {
    "id": 20,
    "question": "What is the difference between REST and GraphQL?",
    "answer": "REST uses fixed endpoints returning predetermined data. GraphQL uses single endpoint where client specifies exact data needed. GraphQL reduces over/under-fetching but adds complexity",
    "example": "Fetching user profile with posts:\n\nREST:\nGET /users/123           → {id, name, email}\nGET /users/123/posts     → [{id, title, body}, ...]\nGET /users/123/followers → [{id, name}, ...]\n\n3 requests, might return unused fields\n\nGraphQL:\nPOST /graphql\nquery {\n  user(id: 123) {\n    name\n    posts(limit: 5) {\n      title\n    }\n    followersCount\n  }\n}\n\n1 request, exact fields needed\n\nWhen to use REST:\n- Simple CRUD APIs\n- Caching important (HTTP caching easy)\n- Team familiar with REST\n\nWhen to use GraphQL:\n- Mobile apps (bandwidth sensitive)\n- Multiple clients with different data needs\n- Rapidly evolving frontend requirements\n\nTwitter/Facebook use GraphQL for mobile apps."
  },
  {
    "id": 21,
    "question": "What is the Saga pattern for distributed transactions?",
    "answer": "Manage distributed transactions as a sequence of local transactions with compensating actions for rollback. Each service completes its transaction and triggers the next. If one fails, execute compensations in reverse",
    "example": "E-commerce order flow:\n\nTraditional transaction (doesn't work distributed):\nBEGIN\n  deduct inventory\n  charge payment\n  create shipping\nCOMMIT\n\nSaga pattern:\n\n1. Order Service: Create order (pending)\n2. Inventory Service: Reserve items\n   - Compensation: Release reservation\n3. Payment Service: Charge card\n   - Compensation: Refund payment\n4. Shipping Service: Schedule delivery\n   - Compensation: Cancel shipment\n5. Order Service: Mark complete\n\nIf Payment fails:\n- Execute compensations in reverse\n- Release inventory reservation\n- Cancel order\n\nOrchestration vs Choreography:\n- Orchestrator: central coordinator manages saga\n- Choreography: services react to events, no coordinator\n\nReal-world: Uber ride booking, Airbnb reservations"
  },
  {
    "id": 22,
    "question": "What is the Circuit Breaker pattern?",
    "answer": "Prevent cascading failures by failing fast when a service is unhealthy. States: Closed (normal), Open (failing fast), Half-Open (testing recovery). Protects system resources and enables graceful degradation",
    "example": "Payment service calling fraud detection:\n\nWithout circuit breaker:\n- Fraud service down\n- Payment requests timeout (30s each)\n- Thread pool exhausted\n- Payment service becomes unresponsive\n- Cascading failure to checkout, cart...\n\nWith circuit breaker:\n\nClosed state (normal):\n- Requests pass through\n- Track failure rate\n\nOpen state (after 5 failures in 1 minute):\n- Fail immediately, don't call fraud service\n- Return fallback: \"approve with limit\" or queue for later\n- Users not blocked\n\nHalf-open state (after 30 seconds):\n- Allow one test request\n- If succeeds → Closed\n- If fails → Open again\n\nConfiguration:\n- Failure threshold: 5 failures\n- Timeout: 30 seconds\n- Half-open requests: 1\n\nTools: Hystrix, Resilience4j, Polly"
  },
  {
    "id": 23,
    "question": "How would you design a notification system?",
    "answer": "Multi-channel delivery (push, email, SMS) with user preferences, rate limiting, and delivery tracking. Use message queues for async processing and handle retries for failed deliveries",
    "example": "Instagram notification system:\n\nRequirements:\n- Push, email, SMS channels\n- User preferences (per channel, per type)\n- Don't spam users\n- Track delivery/read status\n\nArchitecture:\n\n1. Event occurs (new like, comment, follow)\n   → Publish to notification queue\n\n2. Notification Service consumes:\n   - Check user preferences\n   - Apply rate limiting (max 10/hour)\n   - Aggregate similar notifications (\"5 people liked...\")\n\n3. Route to channel services:\n   - Push Service → APNs/FCM\n   - Email Service → SendGrid\n   - SMS Service → Twilio\n\n4. Delivery tracking:\n   - Store: sent, delivered, read timestamps\n   - Retry failed deliveries with backoff\n\nPriority handling:\n- Security alerts: immediate, all channels\n- Likes: batched, push only\n- Marketing: respect quiet hours"
  },
  {
    "id": 24,
    "question": "What is the Strangler Fig pattern for migration?",
    "answer": "Incrementally replace legacy system by routing features to new system one by one. Facade intercepts requests and routes to old or new system. Eventually legacy is completely replaced (strangled)",
    "example": "Migrating monolith to microservices:\n\nLegacy monolith handles:\n- User management\n- Products\n- Orders\n- Payments\n\nPhase 1: Add facade/API gateway\n- All traffic goes through gateway\n- Gateway routes everything to monolith\n\nPhase 2: Extract User service\n- Build new User microservice\n- Gateway routes /users/* to new service\n- Everything else still goes to monolith\n\nPhase 3: Extract Products service\n- Gateway: /products/* → Products service\n- /users/* → Users service\n- /orders/*, /payments/* → monolith\n\nPhase 4, 5...: Continue extracting\n\nFinal: Monolith is empty, decommission it\n\nBenefits:\n- Zero big-bang risk\n- Rollback: just change routing\n- Team learns incrementally\n- Production validation at each step"
  },
  {
    "id": 25,
    "question": "How would you design Twitter's like/unlike feature?",
    "answer": "Idempotent operations, eventual consistency for counts, fan-out for notifications. Store likes in normalized table, cache counts. Handle race conditions and prevent duplicate likes",
    "example": "Requirements:\n- Like/unlike tweets\n- Show like count on tweets\n- Show if current user liked\n- Handle millions of likes per minute\n\nData model:\nLikes table: (user_id, tweet_id, created_at)\nPrimary key: (user_id, tweet_id) - prevents duplicates\n\nTweets table: like_count (denormalized)\n\nLike operation:\n1. INSERT INTO likes (user_id, tweet_id)\n   ON CONFLICT DO NOTHING (idempotent)\n2. If inserted (not duplicate):\n   - Increment tweet.like_count (async)\n   - Queue notification to author\n\nRead path:\n- like_count: cached in Redis\n- \"Did I like?\": check cache or DB\n\nScale considerations:\n- Hot tweets (viral): like_count updated frequently\n- Solution: buffer increments, batch update\n- Or: approximate counts (Redis INCR, periodic sync)\n\nConsistency:\n- Exact count not critical\n- User's own like status must be consistent (read-your-writes)"
  },
  {
    "id": 26,
    "question": "What is the difference between optimistic and pessimistic locking?",
    "answer": "Pessimistic: lock resource before access, block others until done. Optimistic: no locks, detect conflicts at write time using version numbers. Optimistic scales better for read-heavy workloads",
    "example": "Inventory management during flash sale:\n\nPessimistic locking:\nBEGIN\nSELECT stock FROM products WHERE id=1 FOR UPDATE\n-- Row is locked, others wait\nUPDATE products SET stock = stock - 1\nCOMMIT\n-- Lock released\n\nProblem: 10,000 concurrent users = massive contention\n\nOptimistic locking:\n1. Read: stock=100, version=5\n2. User modifies locally\n3. Write: UPDATE products \n   SET stock=99, version=6\n   WHERE id=1 AND version=5\n4. If affected rows = 0, version changed → retry\n\nWhen to use:\n- Pessimistic: high contention, short transactions\n  Example: bank transfer between accounts\n  \n- Optimistic: low contention, long transactions\n  Example: editing a document (conflict rare)\n  \nE-commerce: often hybrid\n- Optimistic for cart updates\n- Pessimistic for final checkout"
  },
  {
    "id": 27,
    "question": "What is the Bulkhead pattern?",
    "answer": "Isolate components into pools so failure in one doesn't exhaust resources for others. Like ship bulkheads that contain flooding. Apply to thread pools, connections, and services",
    "example": "Microservices with shared thread pool:\n\nProblem without bulkhead:\n- Payment service slow (3rd party issue)\n- All threads waiting on payment\n- No threads left for product, search, cart\n- Entire system unresponsive\n\nWith bulkhead pattern:\n\nSeparate thread pools:\n- Payment pool: 20 threads max\n- Product pool: 50 threads max\n- Search pool: 30 threads max\n\nPayment service slow:\n- Payment pool exhausted (20 threads blocked)\n- Product and search pools unaffected\n- Users can still browse and add to cart\n\nImplementation levels:\n- Thread pool bulkhead (Hystrix)\n- Connection pool per service\n- Separate containers/pods per service\n- Separate clusters for critical vs non-critical\n\nReal-world: Netflix isolates recommendations\nfrom core playback functionality."
  },
  {
    "id": 28,
    "question": "How do you handle idempotency in distributed systems?",
    "answer": "Ensure operations produce same result regardless of how many times executed. Critical for retries and at-least-once delivery. Use idempotency keys, unique constraints, and conditional updates",
    "example": "Payment processing (must not double-charge):\n\nProblem:\n1. Client sends payment request\n2. Server processes, charges card\n3. Response lost (network issue)\n4. Client retries\n5. Customer charged twice!\n\nSolution - Idempotency key:\n\n1. Client generates unique key: \"pay_abc123\"\n2. Request: POST /payments {idempotency_key: \"pay_abc123\", amount: 100}\n\nServer logic:\n- Check if idempotency_key exists in DB\n- If exists: return stored result (no processing)\n- If not: process payment, store result with key\n\nDatabase:\nCREATE TABLE payments (\n  idempotency_key VARCHAR PRIMARY KEY,\n  result JSONB,\n  created_at TIMESTAMP\n)\n\nOther patterns:\n- Conditional updates: UPDATE WHERE version = X\n- Natural idempotency: DELETE (deleting twice = same result)\n- Unique constraints: prevent duplicate inserts"
  },
  {
    "id": 29,
    "question": "What are the trade-offs between SQL and NoSQL databases?",
    "answer": "SQL: ACID transactions, structured schemas, complex queries, vertical scaling. NoSQL: flexible schemas, horizontal scaling, eventual consistency, optimized for specific access patterns. Choose based on requirements",
    "example": "Choosing database for different use cases:\n\nUser accounts → SQL (PostgreSQL)\n- Structured data (name, email, password)\n- Transactions for balance updates\n- Complex queries for admin\n- Strong consistency required\n\nUser sessions → Key-Value NoSQL (Redis)\n- Simple lookup by session_id\n- Auto-expiration (TTL)\n- High read/write throughput\n- No relationships needed\n\nProduct catalog → Document NoSQL (MongoDB)\n- Varying attributes per product type\n- Embedded reviews for fast reads\n- Flexible schema evolution\n- Horizontal scaling\n\nSocial graph → Graph NoSQL (Neo4j)\n- \"Friends of friends\" queries\n- Relationship traversal\n- Not possible efficiently in SQL\n\nTime-series metrics → Time-series DB (InfluxDB)\n- Optimized for time-range queries\n- Automatic downsampling\n- High write throughput\n\nMany apps use multiple databases (polyglot persistence)."
  },
  {
    "id": 30,
    "question": "What is the API Gateway pattern?",
    "answer": "Single entry point for all client requests. Handles cross-cutting concerns: authentication, rate limiting, routing, load balancing, request transformation. Simplifies clients by aggregating microservices",
    "example": "E-commerce mobile app:\n\nWithout gateway:\n- App calls 10 different services directly\n- Each service handles auth separately\n- Mobile handles failures, retries, aggregation\n- Chattiness kills mobile battery/bandwidth\n\nWith API Gateway:\n\nClient → Gateway → Microservices\n\nGateway responsibilities:\n1. Authentication\n   - Validate JWT once\n   - Pass user context to services\n\n2. Request routing\n   - /users/* → User Service\n   - /products/* → Product Service\n\n3. Request aggregation\n   - GET /app/home\n   - Gateway calls: users, products, recommendations\n   - Returns single combined response\n\n4. Rate limiting\n   - 100 requests/minute per user\n\n5. Protocol translation\n   - REST for mobile\n   - gRPC for internal services\n\nTools: Kong, AWS API Gateway, Nginx, Envoy\n\nBackends for Frontends (BFF):\n- Separate gateways for web, mobile, IoT"
  },
  {
    "id": 31,
    "question": "What is Consistent Hashing and why is it important?",
    "answer": "A technique for distributing data across nodes where adding or removing nodes only requires remapping a small fraction of keys. Uses a hash ring where both keys and nodes are hashed to positions. Essential for distributed caches and databases",
    "example": "Distributed cache with 4 servers:\n\nNaive hashing: hash(key) % num_servers\n- Add server: ~75% of keys need remapping\n- Remove server: ~75% of keys need remapping\n- Cache miss storm!\n\nConsistent hashing:\n1. Hash servers to positions on ring (0 to 2^32)\n   - Server A: position 1000\n   - Server B: position 3000\n   - Server C: position 6000\n   - Server D: position 9000\n\n2. Hash keys to ring, walk clockwise to find server\n   - Key \"user:123\" hashes to 2500 → Server B\n   - Key \"user:456\" hashes to 7000 → Server D\n\n3. Add Server E at position 5000:\n   - Only keys between 3000-5000 move to E\n   - ~20% of keys remapped, not 75%!\n\nVirtual nodes:\n- Each server has multiple positions on ring\n- Better distribution, handles heterogeneous servers\n\nUsed by: Cassandra, DynamoDB, Memcached"
  },
  {
    "id": 32,
    "question": "What are different caching strategies?",
    "answer": "Cache-aside: app manages cache and DB separately. Read-through: cache loads from DB on miss. Write-through: writes go to cache and DB. Write-behind: writes to cache, async to DB. Refresh-ahead: proactively refresh before expiry",
    "example": "E-commerce product pages:\n\nCache-Aside (most common):\nread:\n  if cache.has(key):\n    return cache.get(key)\n  data = db.query(key)\n  cache.set(key, data, ttl=300)\n  return data\n\nwrite:\n  db.update(key, data)\n  cache.delete(key)  # Invalidate\n\nRead-Through:\n- Cache handles DB loading automatically\n- Simpler app code\n- Cache library does the work\n\nWrite-Through:\n- Write to cache first\n- Cache synchronously writes to DB\n- Strong consistency, higher write latency\n\nWrite-Behind (Write-Back):\n- Write to cache, return immediately\n- Cache async writes to DB\n- Fast writes, risk of data loss\n- Good for: analytics, view counts\n\nRefresh-Ahead:\n- Proactively refresh popular keys before TTL\n- No cache miss latency for hot data\n\nCombinations common:\n- Read: cache-aside\n- Write: write-through for critical, write-behind for counts"
  },
  {
    "id": 33,
    "question": "What are different load balancing algorithms?",
    "answer": "Round Robin: rotate through servers. Weighted Round Robin: more requests to higher-capacity servers. Least Connections: send to server with fewest active connections. IP Hash: same client always to same server. Random: simple random selection",
    "example": "Web application with 3 servers:\n\nRound Robin:\n- Request 1 → Server A\n- Request 2 → Server B\n- Request 3 → Server C\n- Request 4 → Server A...\nSimple but ignores server load\n\nWeighted Round Robin:\n- Server A (weight 3): 3 requests\n- Server B (weight 2): 2 requests\n- Server C (weight 1): 1 request\nGood for heterogeneous servers\n\nLeast Connections:\n- Server A: 10 connections\n- Server B: 5 connections\n- Server C: 8 connections\n- New request → Server B\nBest for long-lived connections (WebSocket)\n\nIP Hash:\nhash(client_ip) % num_servers\n- Same user always hits same server\n- Good for session affinity (no shared session store)\n- Problem: server removal affects many users\n\nLeast Response Time:\n- Track server response times\n- Send to fastest server\n- Good for latency-sensitive apps\n\nLayer 4 vs Layer 7:\n- L4: TCP level (faster, less flexible)\n- L7: HTTP level (can route by URL, headers)"
  },
  {
    "id": 34,
    "question": "What is Service Discovery?",
    "answer": "Mechanism for services to find and communicate with each other without hardcoded addresses. Essential in dynamic environments where instances scale up/down. Two patterns: client-side discovery and server-side discovery",
    "example": "Microservices on Kubernetes:\n\nProblem without service discovery:\n- Order Service needs Payment Service\n- Payment Service at 10.0.0.5:8080 (hardcoded)\n- Payment Service moves, scales, or restarts\n- Order Service fails!\n\nClient-side discovery:\n1. Service Registry stores service locations\n   - Payment Service: [10.0.0.5:8080, 10.0.0.6:8080]\n2. Order Service queries registry\n3. Order Service picks instance (client-side LB)\n4. Direct connection to Payment Service\n\nTools: Consul, Eureka, etcd\n\nServer-side discovery:\n1. Order Service calls \"payment-service\" DNS name\n2. Load balancer/proxy resolves to instance\n3. No registry client in Order Service\n\nTools: Kubernetes Services, AWS ALB\n\nHealth checks:\n- Services send heartbeats to registry\n- Unhealthy instances removed\n- New instances register on startup\n\nKubernetes example:\n- Service: \"payment-service.default.svc.cluster.local\"\n- kube-proxy handles discovery and load balancing"
  },
  {
    "id": 35,
    "question": "What is a Bloom Filter?",
    "answer": "A space-efficient probabilistic data structure for set membership testing. May return false positives but never false negatives. Uses multiple hash functions mapping to a bit array. Perfect for \"definitely not in set\" checks",
    "example": "Use case: Checking if username is taken\n\nWithout Bloom filter:\n- Every signup: query database\n- Expensive for 100M users\n\nWith Bloom filter:\n1. Initialize bit array of size m (e.g., 10M bits)\n2. For each existing username:\n   - Apply k hash functions (e.g., k=3)\n   - Set bits at resulting positions to 1\n\nChecking \"john_doe\":\n- Hash to positions: 42, 1337, 99999\n- If ANY bit is 0: definitely not taken\n- If ALL bits are 1: probably taken (check DB)\n\nFalse positive rate:\n- Depends on m (array size) and n (items)\n- 1% false positive rate is common\n- Never false negatives!\n\nReal-world uses:\n- Chrome: malicious URL check\n- Medium: article recommendations (avoid showing same)\n- Cassandra: check if row exists before disk read\n- Spell checkers: is word in dictionary?\n\nTrade-off:\n- Cannot delete items (use Counting Bloom Filter)\n- Cannot enumerate contents"
  },
  {
    "id": 36,
    "question": "What is Leader Election in distributed systems?",
    "answer": "Process of designating a single node as coordinator for specific tasks. Ensures only one node performs certain operations. Uses consensus algorithms or external coordination services. Must handle leader failure gracefully",
    "example": "Scenario: Scheduled job processing\n\nProblem:\n- 3 app servers, each with scheduler\n- 9 AM: send daily digest email\n- Without leader election: 3 emails sent!\n\nSolution: Leader election\n- One node becomes leader\n- Only leader runs scheduled jobs\n- Followers on standby\n\nImplementation approaches:\n\n1. Database-based:\nSELECT * FROM leader_lock\n  WHERE resource = 'scheduler'\n  FOR UPDATE NOWAIT\n- First to lock wins\n- Release on crash via timeout\n\n2. ZooKeeper / etcd:\n- Create ephemeral sequential node\n- Lowest sequence number is leader\n- Node deleted on disconnect → new election\n\n3. Redis SETNX:\nSETNX leader_key \"node-1\" EX 30\n- First to set wins\n- Leader renews every 10s\n- Expire at 30s handles crashes\n\nLeader responsibilities:\n- Send heartbeats to maintain leadership\n- Graceful handoff before shutdown\n- Followers monitor leader health\n\nUsed in: Kafka (partition leader), ElasticSearch (master)"
  },
  {
    "id": 37,
    "question": "What is the Gossip Protocol?",
    "answer": "A peer-to-peer communication protocol where nodes periodically exchange state information with random peers. Information spreads exponentially like rumors. Provides eventual consistency and failure detection without central coordinator",
    "example": "Cassandra cluster membership:\n\nHow it works:\n1. Each node maintains membership list with heartbeat counters\n2. Every second, pick random peer\n3. Exchange membership info\n4. Merge: keep higher heartbeat counts\n\nExample:\nNode A knows: {A:100, B:99, C:98}\nNode B knows: {A:98, B:100, C:99}\n\nAfter gossip:\nBoth know: {A:100, B:100, C:99}\n\nFailure detection:\n- Node C stops sending heartbeats\n- Heartbeat counter stops incrementing\n- Other nodes notice C's counter stale\n- Mark C as suspected, then dead\n\nBenefits:\n- No single point of failure\n- Scalable: O(log n) rounds to spread info\n- Handles network partitions gracefully\n\nDrawbacks:\n- Eventual consistency (not immediate)\n- Bandwidth overhead from periodic messages\n\nUsed by:\n- Cassandra: membership and failure detection\n- Consul: service health propagation\n- Amazon S3: tracking server states"
  },
  {
    "id": 38,
    "question": "What are database indexing strategies?",
    "answer": "B-Tree: balanced tree for range queries and equality. Hash index: O(1) for equality only. Composite: multiple columns for common query patterns. Covering: includes all query columns to avoid table lookup. Partial: index subset of rows",
    "example": "E-commerce products table:\n\nB-Tree Index (default):\nCREATE INDEX idx_price ON products(price)\n- Good for: price > 100 AND price < 500\n- Good for: ORDER BY price\n- Sorted structure enables range scans\n\nHash Index:\nCREATE INDEX idx_sku ON products USING HASH(sku)\n- Good for: sku = 'ABC123' (O(1) lookup)\n- Bad for: ranges, sorting\n- Less common, specific use cases\n\nComposite Index:\nCREATE INDEX idx_cat_price ON products(category, price)\n- Excellent: category='electronics' AND price < 100\n- Good: category='electronics' (leftmost prefix)\n- Bad: price < 100 alone (can't use index)\n- Order matters!\n\nCovering Index:\nCREATE INDEX idx_cover ON products(category, price) INCLUDE (name)\n- Query needs only: category, price, name\n- All data in index, no table lookup\n- Faster but more storage\n\nPartial Index:\nCREATE INDEX idx_active ON products(name) WHERE status='active'\n- Only index active products (10% of data)\n- Smaller index, faster updates\n\nAnalyze queries with EXPLAIN before adding indexes."
  },
  {
    "id": 39,
    "question": "What is the difference between WebSockets, Server-Sent Events, and Long Polling?",
    "answer": "Long Polling: client polls, server holds until data available. SSE: server pushes over single HTTP connection, client receives. WebSockets: full duplex bidirectional communication over single TCP connection. Choose based on communication pattern",
    "example": "Real-time features comparison:\n\nLong Polling:\n- Client: GET /messages (waits)\n- Server: holds connection until new message\n- Server: responds, connection closes\n- Client: immediately polls again\n\nUse case: Simple notifications, legacy support\nDownside: Connection overhead, not truly real-time\n\nServer-Sent Events (SSE):\n- Client: EventSource('/stream')\n- Server: keeps connection, sends events\n- One-way: server → client only\n- Auto-reconnect built-in\n\nUse case: Live feeds, stock tickers, notifications\nDownside: No client-to-server messages\n\nWebSockets:\n- Full duplex on single TCP connection\n- Binary or text frames\n- Client and server can send anytime\n- ws:// or wss:// protocol\n\nUse case: Chat, gaming, collaborative editing\nDownside: More complex, needs sticky sessions\n\nDecision guide:\n- Notifications only? → SSE\n- Chat/bidirectional? → WebSockets\n- Legacy browser support? → Long Polling\n\nTwitter uses:\n- WebSockets for real-time notifications\n- Polling for less critical updates"
  },
  {
    "id": 40,
    "question": "What is Two-Phase Commit (2PC)?",
    "answer": "A distributed transaction protocol ensuring all nodes commit or all abort. Phase 1: Coordinator asks all participants to prepare. Phase 2: If all vote yes, coordinator sends commit; otherwise abort. Guarantees atomicity but blocks on coordinator failure",
    "example": "Bank transfer: $100 from Account A (Bank 1) to Account B (Bank 2)\n\nPhase 1 - Prepare:\n1. Coordinator: \"Prepare to debit $100 from A\"\n   Bank 1: locks A, writes to log, votes YES\n2. Coordinator: \"Prepare to credit $100 to B\"\n   Bank 2: locks B, writes to log, votes YES\n\nPhase 2 - Commit:\nIf all YES:\n  Coordinator: \"Commit\" to both\n  Bank 1: commits, unlocks A\n  Bank 2: commits, unlocks B\n\nIf any NO (or timeout):\n  Coordinator: \"Abort\" to all\n  Everyone rolls back\n\nProblems:\n1. Blocking: if coordinator crashes after prepare\n   - Participants stuck holding locks\n   - Can't decide commit or abort alone\n\n2. Performance: locks held during entire protocol\n   - High latency across data centers\n   - Not suitable for high-throughput\n\nAlternatives:\n- 3PC: adds pre-commit phase (non-blocking)\n- Saga: eventual consistency with compensations\n- TCC (Try-Confirm-Cancel): reservation pattern\n\nUsed in: Traditional databases, XA transactions"
  },
  {
    "id": 41,
    "question": "What are Blue-Green and Canary deployments?",
    "answer": "Blue-Green: maintain two identical environments, switch traffic instantly between them. Canary: gradually route small percentage of traffic to new version, increase if healthy. Both enable zero-downtime deploys and fast rollback",
    "example": "Deploying new version of payment service:\n\nBlue-Green Deployment:\n1. Blue (current): serving 100% traffic\n2. Deploy to Green (new): v2.0\n3. Run smoke tests on Green\n4. Switch load balancer: 100% → Green\n5. Keep Blue ready for instant rollback\n\nPros: Instant rollback, simple\nCons: Double infrastructure cost\n\nCanary Deployment:\n1. Current: 100% on v1.0\n2. Deploy canary: v2.0 to small subset\n3. Route 5% traffic to canary\n4. Monitor errors, latency, business metrics\n5. If healthy: 5% → 25% → 50% → 100%\n6. If unhealthy: rollback canary\n\nCanary selection strategies:\n- Random 5% of users\n- Internal employees first\n- Specific regions\n- Beta/premium users\n\nFeature flags complement both:\n- Deploy code but keep feature off\n- Enable for canary users via flag\n- Decouple deploy from release\n\nKubernetes supports both:\n- Blue-Green: switch Service selector\n- Canary: weighted traffic split"
  },
  {
    "id": 42,
    "question": "What is Observability and what are its three pillars?",
    "answer": "The ability to understand system internal state from external outputs. Three pillars: Logs (discrete events), Metrics (aggregated measurements), Traces (request flow across services). Essential for debugging distributed systems",
    "example": "Debugging slow checkout in microservices:\n\nLogs (What happened):\n- Discrete events with context\n- \"Order 123 created by user 456\"\n- \"Payment failed: timeout after 30s\"\n- Structured JSON for searchability\n- Tools: ELK Stack, Splunk, Datadog\n\nMetrics (How much/how fast):\n- Aggregated numbers over time\n- Request rate: 1000 req/s\n- Error rate: 2%\n- P99 latency: 500ms\n- CPU, memory, connections\n- Tools: Prometheus + Grafana, CloudWatch\n\nTraces (Where did time go):\n- Follow request across services\n- Checkout → Order → Inventory → Payment\n- See which service is slow\n- Span: single operation\n- Trace: collection of spans\n- Tools: Jaeger, Zipkin, AWS X-Ray\n\nCorrelation ID:\n- Generate unique ID at entry point\n- Pass through all services\n- Search logs by correlation ID\n- Link traces to logs\n\nAlert on metrics, search logs, trace specific requests."
  },
  {
    "id": 43,
    "question": "What is Back Pressure in distributed systems?",
    "answer": "A flow control mechanism where downstream systems signal upstream to slow down when overwhelmed. Prevents cascade failures from overload. Implemented via queue limits, rate limiting, or load shedding",
    "example": "Video processing pipeline:\n\nUpload → Transcode → Store → Notify\n\nProblem without back pressure:\n- Marketing campaign: 10x uploads\n- Transcode queue grows unbounded\n- Memory exhausted, service crashes\n- All pending jobs lost\n\nWith back pressure:\n\n1. Bounded queues:\n- Transcode queue max: 1000 jobs\n- When full: reject new uploads\n- Client sees 429 Too Many Requests\n- Retry with exponential backoff\n\n2. Rate limiting at entry:\n- Max 100 uploads/second\n- Excess queued or rejected\n\n3. Load shedding:\n- When overloaded, reject low-priority work\n- Process premium users first\n- Better to serve some than crash serving none\n\n4. Circuit breaker:\n- When downstream slow, fail fast\n- Don't accumulate requests\n\nReactive Streams:\n- Consumer tells producer how many items to send\n- Producer only sends what consumer can handle\n- Built into: Project Reactor, RxJava, Akka Streams\n\nResult: System degrades gracefully instead of crashing."
  },
  {
    "id": 44,
    "question": "What is a Dead Letter Queue?",
    "answer": "A secondary queue for messages that cannot be processed successfully after multiple attempts. Isolates problematic messages to prevent blocking the main queue. Enables manual inspection and reprocessing",
    "example": "Order processing system:\n\nMain Queue: process_order\n- Message: {order_id: 123, items: [...]}\n- Worker tries to process\n\nFailure scenarios:\n- Invalid data: items array empty\n- External failure: payment service down\n- Bug: null pointer exception\n\nWithout DLQ:\n- Failed message retried forever\n- Or lost after max retries\n- Blocks other messages (head-of-line)\n- No visibility into failures\n\nWith DLQ:\n1. Attempt processing (retry 3 times)\n2. After 3 failures → move to DLQ\n3. Main queue continues processing\n4. DLQ accumulates poison messages\n\nDLQ handling:\n- Alert when DLQ not empty\n- Dashboard shows failed messages\n- Inspect message content\n- Fix bug, reprocess from DLQ\n- Or delete if invalid data\n\nAWS SQS configuration:\n{\n  \"maxReceiveCount\": 3,\n  \"deadLetterTargetArn\": \"arn:aws:sqs:dlq\"\n}\n\nDifferentiate errors:\n- Transient (retry): network timeout\n- Permanent (DLQ immediately): invalid data"
  },
  {
    "id": 45,
    "question": "How would you design a chat system like Slack?",
    "answer": "Real-time messaging using WebSockets for delivery. Messages stored in database, fanout to channel members. Presence tracking for online status. Support for channels, DMs, threads, file sharing, and search",
    "example": "Core components:\n\n1. Connection Service:\n- WebSocket connections per user\n- Horizontal scaling with sticky sessions\n- Heartbeat for connection health\n- Redis pub/sub for cross-server messaging\n\n2. Message Flow:\n- User sends: WS → Connection Service\n- Validate, store in Messages DB\n- Fanout to channel members:\n  - Small channel: direct delivery\n  - Large channel: queue + workers\n- Deliver via recipient's WebSocket\n\n3. Data Model:\n- Channels: {id, name, workspace_id, members[]}\n- Messages: {id, channel_id, user_id, content, ts}\n- Partition messages by channel_id\n\n4. Presence:\n- Track last_seen timestamp\n- Heartbeat every 30s updates Redis\n- \"Online\" if last_seen < 1 minute\n- Broadcast presence changes to relevant users\n\n5. Read receipts:\n- Store: {user_id, channel_id, last_read_ts}\n- Unread count: messages after last_read_ts\n\n6. Search:\n- Elasticsearch for full-text search\n- Index messages asynchronously\n\nScale considerations:\n- Hot channels: queue messages, batch process\n- Message ordering: sequence numbers per channel"
  },
  {
    "id": 46,
    "question": "What is the Sidecar pattern?",
    "answer": "Deploy auxiliary functionality as a separate process alongside the main application. Sidecar handles cross-cutting concerns like logging, monitoring, security, or networking. Enables language-agnostic features without modifying app code",
    "example": "Microservices with service mesh:\n\nWithout sidecar:\n- Each service implements:\n  - TLS termination\n  - Retry logic\n  - Circuit breakers\n  - Metrics collection\n  - Log shipping\n- Duplicate code in every language/service\n\nWith sidecar (Envoy proxy):\n\nPod:\n┌─────────────────────────┐\n│ ┌─────────┐ ┌─────────┐ │\n│ │  App    │ │ Sidecar │ │\n│ │ :8080   │ │ :15001  │ │\n│ └────┬────┘ └────┬────┘ │\n│      └─────┬─────┘      │\n└────────────┼────────────┘\n             ↓\n         Network\n\nSidecar handles:\n- Inbound: TLS, authentication, rate limiting\n- Outbound: retries, circuit breakers, load balancing\n- Observability: metrics, traces, logs\n\nApp only does business logic!\n\nService Mesh (Istio, Linkerd):\n- Deploys sidecars to all services\n- Centralized traffic management\n- Mutual TLS between all services\n- No code changes required\n\nOther sidecar uses:\n- Log collectors (Fluentd)\n- Secret management (Vault agent)\n- Configuration sync"
  },
  {
    "id": 47,
    "question": "What is Data Partitioning and what are common strategies?",
    "answer": "Dividing data across multiple nodes to distribute load and storage. Strategies: Range-based (by key ranges), Hash-based (by key hash), Directory-based (lookup table), Geographic (by region). Key choice affects query patterns",
    "example": "Social media posts table (1 billion rows):\n\nRange Partitioning:\n- Partition by created_date\n- Jan 2024: Partition 1\n- Feb 2024: Partition 2\n- Good for: time-range queries\n- Bad: recent partition always hot\n\nHash Partitioning:\n- partition = hash(user_id) % num_partitions\n- Uniform distribution\n- Good for: point queries by user_id\n- Bad: range queries scan all partitions\n\nComposite Partitioning:\n- First: hash by user_id (even distribution)\n- Then: range by date (within user's data)\n- Good for: \"user X's posts in March\"\n\nGeographic Partitioning:\n- US users: US data center\n- EU users: EU data center\n- Low latency for reads\n- GDPR compliance (data residency)\n\nDirectory-Based:\n- Lookup service maps key → partition\n- Flexible, can move data easily\n- Extra hop for every query\n\nHot partition solutions:\n- Split hot partitions\n- Add random suffix to hot keys\n- Cache hot data\n\nRebalancing challenges:\n- Adding partition: need to redistribute\n- Consistent hashing helps minimize movement"
  },
  {
    "id": 48,
    "question": "What is the Retry pattern with Exponential Backoff?",
    "answer": "Automatically retry failed operations with increasing delays between attempts. Prevents overwhelming recovering services. Add jitter (randomness) to avoid thundering herd. Define max retries and circuit breaker integration",
    "example": "Calling payment API:\n\nBasic retry (bad):\nfor i in range(5):\n  try: return api.charge()\n  except: continue\n\nProblem: 1000 clients retry immediately\n- Server recovering from overload\n- Instant retry = more overload\n- Thundering herd!\n\nExponential backoff:\nmax_retries = 5\nbase_delay = 1 second\n\nfor attempt in range(max_retries):\n  try:\n    return api.charge()\n  except RetryableError:\n    delay = base_delay * (2 ** attempt)\n    # attempt 0: 1s, 1: 2s, 2: 4s, 3: 8s, 4: 16s\n    sleep(delay)\n\nWith jitter (best):\ndelay = base_delay * (2 ** attempt)\ndelay = delay * random(0.5, 1.5)  # Add randomness\n\n- Clients retry at different times\n- Spreads load on recovering server\n\nWhat to retry:\n- Network timeouts: YES\n- 500 Server Error: YES\n- 429 Rate Limited: YES (respect Retry-After header)\n- 400 Bad Request: NO (won't succeed)\n- 401 Unauthorized: NO (need new credentials)\n\nCircuit breaker integration:\n- After N failures, stop retrying entirely\n- Fail fast instead of accumulating delays"
  },
  {
    "id": 49,
    "question": "How would you design a search autocomplete system?",
    "answer": "Pre-compute popular query prefixes with results. Use Trie data structure for prefix matching. Rank by popularity, personalization, and recency. Cache heavily as same prefixes repeat. Balance freshness vs computation cost",
    "example": "Google-like search suggestions:\n\nRequirements:\n- As user types, show top 10 suggestions\n- Sub-100ms latency\n- Handle 10K queries/second\n\nData structure - Trie:\n     (root)\n      /  \\\n     t    f\n    /      \\\n   tw      fa\n  /  \\       \\\n twi  two    fac\n  |          |\n twit       face\n  |          |\ntwitter  facebook\n\nEach node stores:\n- Top K suggestions for this prefix\n- Pre-computed, not calculated on query\n\nData pipeline:\n1. Collect search logs\n2. Aggregate query counts (hourly)\n3. Build/update Trie with top suggestions\n4. Replicate to query servers\n\nRanking factors:\n- Search frequency\n- Recency (trending topics)\n- User's past searches (personalization)\n- Geographic relevance\n\nOptimizations:\n- Cache popular prefixes in CDN\n- Start suggesting after 2-3 characters\n- Debounce requests (wait 100ms after keystroke)\n- Sampling for extremely popular prefixes\n\nFreshness:\n- Breaking news needs quick updates\n- Background job updates trending terms\n- Most suggestions stable, cache aggressively"
  },
  {
    "id": 50,
    "question": "What is the difference between Microservices and Monolith architecture?",
    "answer": "Monolith: single deployable unit containing all functionality. Microservices: multiple small services, each owning a specific domain, independently deployable. Microservices add complexity but enable team autonomy and independent scaling",
    "example": "E-commerce application:\n\nMonolith:\n┌─────────────────────────┐\n│    E-commerce App       │\n│  ┌─────┐ ┌─────┐ ┌────┐│\n│  │User │ │Order│ │Prod││\n│  └─────┘ └─────┘ └────┘│\n│    Single Database      │\n└─────────────────────────┘\n\nPros:\n- Simple development/deployment\n- Easy debugging (single process)\n- No network latency between modules\n- ACID transactions trivial\n\nCons:\n- Teams step on each other\n- Scale everything together\n- Long deploy times\n- One bug can crash all\n\nMicroservices:\n┌──────┐ ┌──────┐ ┌──────┐\n│ User │ │Order │ │ Prod │\n│ Svc  │ │ Svc  │ │ Svc  │\n│  DB  │ │  DB  │ │  DB  │\n└──────┘ └──────┘ └──────┘\n\nPros:\n- Independent deployment\n- Scale services individually\n- Team autonomy\n- Technology flexibility\n\nCons:\n- Network complexity\n- Distributed transactions hard\n- Operational overhead\n- Need DevOps maturity\n\nStart with monolith, split when:\n- Team grows beyond ~10 developers\n- Clear domain boundaries emerge\n- Different scaling needs\n- Deployment conflicts frequent"
  },
  {
    "id": 51,
    "question": "What is a Write-Ahead Log (WAL)?",
    "answer": "A technique where changes are first written to an append-only log before applying to the main data structure. Ensures durability and enables recovery after crashes. Used by databases, message queues, and distributed systems",
    "example": "PostgreSQL transaction:\n\n1. User: UPDATE balance SET amount = 500\n\n2. Without WAL:\n   - Write directly to data pages\n   - Crash mid-write = corrupted data\n   - No recovery possible\n\n3. With WAL:\n   a. Write change to WAL file (sequential, fast)\n   b. Return success to client\n   c. Later: apply change to data pages (checkpoint)\n   d. After checkpoint: WAL segment can be recycled\n\nCrash recovery:\n- On restart, replay WAL from last checkpoint\n- Incomplete transactions rolled back\n- Committed transactions guaranteed applied\n\nBenefits:\n- Sequential writes (WAL) faster than random (data pages)\n- Durability: fsync WAL, async data writes\n- Replication: ship WAL to replicas\n- Point-in-time recovery: replay WAL to specific moment\n\nUsed by:\n- PostgreSQL, MySQL (redo log)\n- Kafka (commit log)\n- Redis (AOF)\n- LevelDB/RocksDB"
  },
  {
    "id": 52,
    "question": "What is the difference between Kafka and RabbitMQ?",
    "answer": "Kafka is a distributed log optimized for high-throughput streaming with message retention. RabbitMQ is a traditional message broker with flexible routing and acknowledgment. Kafka for event streaming, RabbitMQ for task queues",
    "example": "Architecture differences:\n\nKafka:\n- Messages stored in ordered, partitioned log\n- Consumers track their own offset\n- Messages retained for configured period\n- Pull-based consumption\n- Replay possible from any offset\n\nRabbitMQ:\n- Messages in queues, deleted after consumption\n- Broker tracks what's delivered\n- Complex routing (exchanges, bindings)\n- Push-based with prefetch\n- No replay after acknowledgment\n\nWhen to use Kafka:\n- Event streaming (click stream, logs)\n- High throughput (millions/sec)\n- Multiple consumers need same messages\n- Need to replay historical events\n- Event sourcing architecture\n\nWhen to use RabbitMQ:\n- Task queues (background jobs)\n- Complex routing patterns\n- Priority queues\n- RPC patterns\n- Lower latency requirements\n- Message-level acknowledgment\n\nReal-world:\n- LinkedIn (Kafka origin): activity tracking\n- Instagram: task queue for image processing (Celery/RabbitMQ)\n- Both: often used together in same system"
  },
  {
    "id": 53,
    "question": "What is the N+1 Query Problem?",
    "answer": "A performance anti-pattern where code executes N additional queries to fetch related data for N items, instead of fetching all data in one or two queries. Common in ORMs. Solve with eager loading, joins, or batch fetching",
    "example": "Display 100 posts with author names:\n\nN+1 Problem:\n```\nposts = Post.all()  # Query 1: SELECT * FROM posts\nfor post in posts:\n    print(post.author.name)  # Query 2-101: SELECT * FROM users WHERE id = ?\n```\nTotal: 101 queries!\n\nSolutions:\n\n1. Eager Loading (ORM):\n```\nposts = Post.includes(:author).all()\n```\nQuery 1: SELECT * FROM posts\nQuery 2: SELECT * FROM users WHERE id IN (1,2,3...)\nTotal: 2 queries\n\n2. JOIN:\n```\nSELECT posts.*, users.name\nFROM posts\nJOIN users ON posts.author_id = users.id\n```\nTotal: 1 query\n\n3. Batch loading:\n```\nposts = Post.all()\nauthor_ids = posts.map(&:author_id).uniq\nauthors = User.where(id: author_ids).index_by(&:id)\n```\nTotal: 2 queries\n\nDetection:\n- Query logging in development\n- APM tools (New Relic, Datadog)\n- Bullet gem (Rails)\n\nNested N+1 can be N*M queries!"
  },
  {
    "id": 54,
    "question": "What is a Distributed Lock?",
    "answer": "A mechanism to ensure only one process across multiple servers can access a shared resource at a time. Implemented using coordination services like Redis, ZooKeeper, or databases. Must handle lock expiration and fencing tokens",
    "example": "Preventing duplicate payment processing:\n\nProblem:\n- User clicks pay twice quickly\n- Request hits server A and server B\n- Both process payment simultaneously\n- Customer charged twice!\n\nSolution - Redis distributed lock:\n```\nLOCK_KEY = \"payment:order:123\"\nLOCK_TTL = 30 seconds\n\n# Acquire lock\nacquired = redis.SET(LOCK_KEY, node_id, NX=True, EX=LOCK_TTL)\n\nif acquired:\n    try:\n        process_payment(order)\n    finally:\n        # Release only if we own the lock\n        if redis.GET(LOCK_KEY) == node_id:\n            redis.DEL(LOCK_KEY)\nelse:\n    return \"Payment already processing\"\n```\n\nRedlock algorithm (more robust):\n- Acquire lock on N/2+1 Redis instances\n- Handles single Redis failure\n\nChallenges:\n- Lock holder crashes: TTL releases eventually\n- Clock skew: use fencing tokens\n- Network partition: lock may appear held on both sides\n\nFencing token:\n- Monotonically increasing number with each lock\n- Resource rejects requests with older tokens\n- Prevents stale lock holder from causing damage"
  },
  {
    "id": 55,
    "question": "What is Database Connection Pooling?",
    "answer": "Maintaining a cache of database connections that can be reused across requests. Creating connections is expensive (TCP handshake, authentication). Pool manages connection lifecycle, limits, and health checks",
    "example": "Without connection pooling:\n- Each request: open connection → query → close\n- 1000 requests/sec = 1000 connect/close cycles\n- Connection setup: ~50ms each\n- Database overwhelmed with connection handling\n\nWith connection pooling:\n- Pool maintains 20 open connections\n- Request borrows connection from pool\n- Query executes\n- Connection returned to pool (not closed)\n- Same 20 connections serve 1000 requests/sec\n\nPool configuration:\n- Min connections: 5 (always ready)\n- Max connections: 50 (prevent overload)\n- Idle timeout: close unused connections after 10min\n- Max lifetime: 30min (prevent stale connections)\n- Connection validation: test before giving to app\n\nExternal pooler (PgBouncer for PostgreSQL):\n- Sits between app and database\n- App servers share one pool\n- 100 app servers, each wants 20 connections\n- Database limit: 100 connections\n- PgBouncer multiplexes, database sees 100\n\nTransaction pooling modes:\n- Session: connection per user session\n- Transaction: connection per transaction\n- Statement: connection per query (most efficient)"
  },
  {
    "id": 56,
    "question": "What is the Ambassador pattern?",
    "answer": "A helper service that handles network-related tasks on behalf of the main application. Acts as an out-of-process proxy managing connections, retries, and monitoring. Similar to sidecar but specifically for outbound communications",
    "example": "Legacy application connecting to cloud services:\n\nProblem:\n- Legacy app written in COBOL\n- Needs to call modern cloud APIs\n- Can't modify code for retries, auth, metrics\n\nAmbassador solution:\n\n┌─────────────────────────────────┐\n│         Application Pod          │\n│  ┌──────────┐   ┌───────────┐  │\n│  │  Legacy  │──→│ Ambassador│──→ Cloud API\n│  │   App    │   │   Proxy   │  │\n│  └──────────┘   └───────────┘  │\n└─────────────────────────────────┘\n\nAmbassador handles:\n- Circuit breaking for cloud service\n- Retry with exponential backoff\n- OAuth token refresh\n- Request/response logging\n- Metrics collection\n- Protocol translation (SOAP → REST)\n\nApp thinks it's calling localhost:\n- App calls: http://localhost:8080/api\n- Ambassador transforms and forwards\n- Cloud-specific complexity hidden\n\nUse cases:\n- Legacy system modernization\n- Cloud service abstraction\n- Adding resilience to unmodifiable code\n- Development/testing (mock ambassador)"
  },
  {
    "id": 57,
    "question": "How would you design a distributed job scheduler?",
    "answer": "Coordinate job execution across multiple workers with reliability. Handle job assignment, failure recovery, scheduling, and monitoring. Use database or coordination service for state. Support cron-like scheduling and one-time jobs",
    "example": "Requirements:\n- Schedule jobs: \"run every day at 3 AM\"\n- Distribute across workers\n- Exactly-once execution\n- Handle worker failures\n\nArchitecture:\n\n1. Job Storage (PostgreSQL):\n- Jobs: {id, schedule, handler, params, status}\n- Job runs: {id, job_id, scheduled_at, started_at, worker_id}\n\n2. Scheduler Service:\n- Evaluates cron expressions\n- Creates job_run records for upcoming jobs\n- Leader election (only one scheduler active)\n\n3. Worker Pool:\n- Workers poll for available job_runs\n- Claim job with atomic update:\n  UPDATE job_runs SET worker_id = ?, started_at = NOW()\n  WHERE id = ? AND worker_id IS NULL\n\n4. Failure handling:\n- Worker heartbeat every 30s\n- Stuck jobs (no heartbeat > 2min): reset for retry\n- Max retries with exponential backoff\n- Dead letter for permanently failed\n\n5. Guarantees:\n- At-least-once: retry on failure\n- Exactly-once: idempotent handlers + deduplication\n\nTools: Airflow, Temporal, Celery Beat, Quartz"
  },
  {
    "id": 58,
    "question": "What is the difference between Stateless and Stateful services?",
    "answer": "Stateless services don't store client state between requests; any instance can handle any request. Stateful services maintain client state locally, requiring affinity or shared storage. Stateless scales easier but stateful may be necessary for performance",
    "example": "Web application design:\n\nStateless (preferred for web):\n- User session stored in Redis, not server memory\n- Any server can handle any request\n- Load balancer uses round-robin\n- Easy horizontal scaling\n- Server crashes don't lose user state\n\nStateful (when necessary):\n- In-memory cache for user data\n- WebSocket connection to specific server\n- Game server with match state\n- Database connections\n\nMaking services stateless:\n\n1. Session externalization:\n   Before: session in server memory\n   After: session in Redis\n\n2. Shared nothing:\n   Before: uploaded files on local disk\n   After: files in S3\n\n3. Request contains all context:\n   Before: rely on server knowing user state\n   After: JWT token contains user info\n\nStateful service patterns:\n- Sticky sessions: load balancer routes user to same server\n- Session replication: sync state across servers\n- Distributed cache: Redis cluster\n\nStateful at scale:\n- Consistent hashing for routing\n- State handoff on server shutdown\n- State recovery from persistent store"
  },
  {
    "id": 59,
    "question": "What is the Outbox Pattern?",
    "answer": "A technique for reliable event publishing alongside database changes. Write event to an outbox table in the same transaction as business data. A separate process reads outbox and publishes to message broker. Ensures atomicity without distributed transactions",
    "example": "Order service needs to publish OrderCreated event:\n\nProblem without outbox:\n1. Save order to database\n2. Publish to Kafka\n3. If Kafka fails after DB commit:\n   - Order exists, but event never sent\n   - Downstream services never notified\n\nOutbox pattern solution:\n\n1. Single transaction:\n   BEGIN;\n   INSERT INTO orders (id, ...) VALUES (...);\n   INSERT INTO outbox (id, event_type, payload)\n     VALUES (uuid, 'OrderCreated', '{...}');\n   COMMIT;\n\n2. Outbox processor (separate process):\n   - Poll outbox table for new records\n   - Publish to Kafka\n   - Mark record as processed (or delete)\n\n3. If Kafka publish fails:\n   - Retry from outbox\n   - Event still in outbox table\n\nChange Data Capture alternative:\n- Use Debezium to stream outbox table changes\n- Database binlog → Kafka directly\n- No polling needed\n\nBenefits:\n- Atomic: event written iff business data written\n- Reliable: at-least-once delivery\n- Decoupled: service doesn't wait for broker"
  },
  {
    "id": 60,
    "question": "What is the difference between gRPC and REST?",
    "answer": "REST uses HTTP/JSON with resource-based URLs and standard methods. gRPC uses HTTP/2 with Protocol Buffers binary format and generated client/server code. gRPC offers better performance and streaming; REST offers simplicity and browser support",
    "example": "Getting user profile:\n\nREST:\n- GET /users/123\n- Response: JSON {\"id\": 123, \"name\": \"John\"}\n- Human readable\n- Browser/curl friendly\n- Any HTTP client works\n\ngRPC:\n- UserService.GetUser(UserRequest{id: 123})\n- Response: Binary protobuf\n- Need generated client\n- Type-safe, fast serialization\n\nProtocol Buffers (.proto file):\n```\nservice UserService {\n  rpc GetUser(UserRequest) returns (User);\n  rpc StreamUpdates(UserRequest) returns (stream Update);\n}\n\nmessage User {\n  int64 id = 1;\n  string name = 2;\n}\n```\n\nWhen to use gRPC:\n- Internal microservices communication\n- Low latency requirements\n- Streaming (bidirectional)\n- Strong typing needed\n- Bandwidth constrained (mobile)\n\nWhen to use REST:\n- Public APIs\n- Browser clients\n- Simple CRUD operations\n- Team familiarity\n- Easy debugging\n\nHybrid approach:\n- gRPC for internal services\n- REST gateway for external clients\n- grpc-gateway generates REST from proto"
  },
  {
    "id": 61,
    "question": "How would you design a file storage service like Dropbox?",
    "answer": "Handle large file uploads with chunking and deduplication. Use object storage for files, database for metadata. Support sync across devices with versioning and conflict resolution. CDN for fast downloads",
    "example": "Core components:\n\n1. Upload flow:\n- Client splits file into 4MB chunks\n- Hash each chunk (content-addressable)\n- Upload only new chunks (deduplication)\n- Metadata service tracks file → chunks mapping\n\n2. Storage architecture:\n- Metadata DB: {file_id, name, chunks[], version}\n- Chunk store: S3 or custom blob storage\n- Chunks replicated across data centers\n\n3. Sync protocol:\n- Client maintains local state + last_sync_version\n- Poll or WebSocket for changes\n- Differential sync: only changed chunks\n\n4. Conflict resolution:\n- Vector clocks or version numbers\n- Same file edited on two devices:\n  - Detect conflict on sync\n  - Keep both as \"file.txt\" and \"file (conflict).txt\"\n  - User decides which to keep\n\n5. Deduplication:\n- Hash chunks: same content = same hash\n- 100 users upload same PDF\n- Stored only once\n- 50-60% storage savings typical\n\n6. Performance:\n- CDN for downloads\n- Regional upload endpoints\n- Resume interrupted uploads (chunk-level)\n- Compression for compressible file types"
  },
  {
    "id": 62,
    "question": "What is the ACID properties of transactions?",
    "answer": "Atomicity: all or nothing execution. Consistency: data always valid per rules. Isolation: concurrent transactions don't interfere. Durability: committed data survives failures. Core guarantees of relational databases",
    "example": "Bank transfer: $100 from A to B\n\nAtomicity:\n- Debit A AND credit B, or neither\n- No partial state where A debited but B not credited\n- Implemented via: undo logs, redo logs\n\nConsistency:\n- Account balance can't go negative (constraint)\n- Total money in system unchanged\n- App + DB constraints enforced\n\nIsolation:\n- Two transfers happening simultaneously\n- Each sees consistent snapshot\n- Levels: Read Uncommitted < Read Committed < \n  Repeatable Read < Serializable\n\nDurability:\n- After COMMIT returns, data persists\n- Survives crash, power failure\n- Write-ahead log ensures this\n\nIsolation levels trade-offs:\n\nRead Committed (default PostgreSQL):\n- See only committed data\n- May see different values in same transaction\n\nRepeatable Read:\n- Same query returns same results\n- Phantom reads possible\n\nSerializable:\n- Full isolation, as if sequential\n- Highest correctness, lowest performance\n\nNoSQL trade-off:\n- Often relaxes ACID for availability/performance\n- BASE: Basically Available, Soft state, Eventually consistent"
  },
  {
    "id": 63,
    "question": "What is the difference between Synchronous and Asynchronous communication?",
    "answer": "Synchronous: caller waits for response before continuing. Asynchronous: caller sends request and continues immediately, handles response later. Async improves availability and decoupling but adds complexity",
    "example": "E-commerce checkout flow:\n\nSynchronous (simple but fragile):\nCheckout → Inventory → Payment → Shipping → Email\n\n- User waits for all services\n- Total latency = sum of all services\n- One service down = checkout fails\n- Tight coupling\n\nAsynchronous (resilient):\n1. Checkout service:\n   - Validate order\n   - Publish OrderCreated event\n   - Return 202 Accepted immediately\n\n2. Background processing:\n   - Inventory service: reserve items\n   - Payment service: charge card\n   - Shipping service: create label\n   - Email service: send confirmation\n\n3. Each service:\n   - Subscribes to relevant events\n   - Processes independently\n   - Publishes result events\n\nBenefits:\n- Fast user response\n- Services can fail independently\n- Retry without user waiting\n- Scale services independently\n\nChallenges:\n- Eventual consistency\n- Complex debugging\n- Need correlation IDs\n- User might not see immediate result\n\nPattern: Async with callback\n- Client provides callback URL\n- Service calls back when complete\n- Or: client polls for status"
  },
  {
    "id": 64,
    "question": "What is Eventual Consistency?",
    "answer": "A consistency model where updates propagate asynchronously and all replicas converge to the same state given enough time without new updates. Trades immediate consistency for availability and partition tolerance",
    "example": "Social media likes count:\n\nStrong consistency:\n- Like button clicked\n- Write to primary database\n- Wait for all replicas to confirm\n- Then show updated count\n- Slow, but always accurate\n\nEventual consistency:\n- Like button clicked\n- Write to primary database\n- Return immediately (count might show old value)\n- Replicas sync in background (milliseconds to seconds)\n- Eventually all see same count\n\nAcceptable scenarios:\n- Like counts (off by a few is okay)\n- Follower counts\n- View counts\n- Recommendation feeds\n- Shopping cart (before checkout)\n\nNot acceptable:\n- Account balance\n- Inventory (can't oversell)\n- Unique username check\n- Authentication tokens\n\nHandling eventual consistency:\n\n1. Read-your-writes:\n- User likes post → read from primary\n- Others can read from replicas\n\n2. Monotonic reads:\n- Once seen value X, never see older value\n- Stick to same replica or use version numbers\n\n3. Client-side reconciliation:\n- Track pending changes locally\n- Merge with server state"
  },
  {
    "id": 65,
    "question": "How would you design a web crawler?",
    "answer": "Systematically browse the web to index content. Handle URL frontier, politeness (rate limiting per domain), duplicate detection, and distributed coordination. Must be robust to malformed HTML and infinite loops",
    "example": "Google-scale web crawler:\n\n1. URL Frontier (queue of URLs to visit):\n- Priority queue: important sites first\n- Per-domain queues for politeness\n- Bloom filter: avoid re-crawling\n\n2. Fetcher:\n- HTTP client with timeout\n- Respect robots.txt (per domain)\n- Handle redirects (limit depth)\n- Politeness: 1 request/domain/second\n\n3. Parser:\n- Extract links from HTML\n- Normalize URLs (remove fragments, etc.)\n- Extract content for indexer\n\n4. Duplicate detection:\n- URL-level: exact URL seen before?\n- Content-level: SimHash for near-duplicates\n- Canonical URLs: www vs non-www\n\n5. Distributed architecture:\n- URL frontier in Redis/Kafka\n- Workers fetch in parallel\n- Partition by domain: one worker per domain\n- Coordinator assigns domains to workers\n\n6. Challenges:\n- Spider traps: infinite URLs (calendars)\n- Dynamic content: need headless browser\n- Rate limits: respect or get blocked\n- Scale: billions of pages\n\nPoliteness:\n- Crawl-delay in robots.txt\n- Distribute load across domains\n- Identify with User-Agent"
  },
  {
    "id": 66,
    "question": "What is the Twelve-Factor App methodology?",
    "answer": "Best practices for building cloud-native applications: codebase in version control, explicit dependencies, config in environment, backing services as attachments, strict build/run separation, stateless processes, port binding, concurrency via processes, fast startup/shutdown, dev/prod parity, logs as streams, admin tasks as one-offs",
    "example": "Key factors explained:\n\n1. Codebase: One repo per app, many deploys\n2. Dependencies: Declare explicitly (package.json)\n\n3. Config: Environment variables, not code\n   Bad: const DB_HOST = 'prod-db.example.com'\n   Good: const DB_HOST = process.env.DB_HOST\n\n4. Backing services: Treat DB, cache as attachments\n   - Swap MySQL for PostgreSQL via config\n   - No code changes needed\n\n5. Build, release, run: Strict separation\n   - Build: compile code\n   - Release: combine with config\n   - Run: execute in environment\n\n6. Processes: Stateless, share-nothing\n   - No sticky sessions\n   - State in external stores\n\n7. Port binding: Self-contained web server\n   - App exports HTTP on port\n   - No external web server required\n\n8. Concurrency: Scale via process model\n   - More workers = more capacity\n   - Not bigger single process\n\n9. Disposability: Fast startup, graceful shutdown\n10. Dev/prod parity: Keep environments similar\n11. Logs: Write to stdout, platform handles\n12. Admin: One-off scripts run in same environment"
  },
  {
    "id": 67,
    "question": "What is an LSM Tree and when is it used?",
    "answer": "Log-Structured Merge Tree optimizes write performance by buffering writes in memory, flushing to sorted files, and periodically merging files. Trades read performance for write throughput. Used in Cassandra, RocksDB, LevelDB",
    "example": "LSM Tree structure:\n\n1. MemTable (in-memory):\n   - Sorted tree (red-black, skip list)\n   - All writes go here first\n   - Fast: no disk I/O\n\n2. When MemTable full:\n   - Flush to disk as SSTable (Sorted String Table)\n   - Immutable, sorted file\n   - New MemTable created\n\n3. SSTables on disk:\n   - Level 0: recent flushes (may overlap)\n   - Level 1-N: merged, non-overlapping\n   - Compaction: merge SSTables, remove deleted\n\nWrite path (fast):\n- Write to MemTable: O(log n)\n- Async flush to disk\n- Sequential writes only\n\nRead path (slower):\n- Check MemTable\n- Check SSTables level by level\n- Bloom filters: skip SSTables that don't have key\n- May need to check multiple files\n\nCompaction strategies:\n- Size-tiered: merge similar-sized SSTables\n- Leveled: keep levels sorted, better read\n\nComparison with B-Tree:\n- B-Tree: balanced read/write, random I/O\n- LSM: write-optimized, sequential I/O\n\nUse cases: Write-heavy workloads, time-series data"
  },
  {
    "id": 68,
    "question": "What is API Versioning and what are the strategies?",
    "answer": "Managing multiple API versions to support backward compatibility while evolving. Strategies: URL path versioning, query parameter, header-based, content negotiation. Balance compatibility with maintenance burden",
    "example": "API evolution scenario:\n- V1 returns: {first_name, last_name}\n- V2 returns: {full_name} (breaking change)\n\nVersioning strategies:\n\n1. URL Path (most common):\n   GET /v1/users/123\n   GET /v2/users/123\n   Pros: explicit, easy routing\n   Cons: not RESTful purist\n\n2. Query Parameter:\n   GET /users/123?version=1\n   GET /users/123?version=2\n   Pros: single URL\n   Cons: caching harder\n\n3. Header (Accept header):\n   Accept: application/vnd.api+json;version=1\n   Pros: clean URLs\n   Cons: harder to test, share links\n\n4. Content Negotiation:\n   Accept: application/vnd.company.v1+json\n   Most RESTful, complex to implement\n\nBest practices:\n- Semantic versioning for major changes\n- Deprecation warnings before removal\n- Support N-1 versions minimum\n- Document migration guides\n- Sunset headers: Sunset: Sat, 31 Dec 2024\n\nBackward compatible changes (no version bump):\n- Add optional fields\n- Add new endpoints\n- Add optional parameters"
  },
  {
    "id": 69,
    "question": "What is the difference between Pub/Sub and Message Queues?",
    "answer": "Message Queue: point-to-point, one consumer processes each message. Pub/Sub: one-to-many, message delivered to all subscribers. Queues for task distribution, Pub/Sub for event broadcasting",
    "example": "Email sending system:\n\nMessage Queue (task distribution):\n- Producer: enqueue email jobs\n- 10 worker consumers\n- Each email processed by ONE worker\n- Load distributed across workers\n- If worker fails, message requeued\n\nUse cases:\n- Background job processing\n- Work distribution\n- Load balancing tasks\n\nPub/Sub (event broadcasting):\n- Publisher: UserCreated event\n- Subscribers:\n  - Email service: send welcome email\n  - Analytics: track signup\n  - CRM: create customer record\n- ALL subscribers receive the event\n\nUse cases:\n- Event-driven architecture\n- Decoupled services\n- Real-time notifications\n\nKafka combines both:\n- Consumer groups: queue semantics within group\n- Multiple groups: pub/sub between groups\n\nExample:\n- Topic: orders\n- Group A (billing): 3 consumers, each processes 1/3\n- Group B (shipping): 3 consumers, each processes 1/3\n- Both groups receive ALL orders\n- Within group, each order processed once"
  },
  {
    "id": 70,
    "question": "How would you design a real-time leaderboard?",
    "answer": "Track and rank users by score with fast updates and queries. Use sorted sets (Redis ZSET) for O(log n) operations. Handle ties, pagination, and near-real-time updates. Consider sharding for massive scale",
    "example": "Mobile game leaderboard:\n\nRequirements:\n- Update score on game completion\n- Get top 100 players\n- Get player's rank\n- 10M players, 1000 updates/sec\n\nRedis Sorted Set solution:\n\n1. Store scores:\n   ZADD leaderboard 1500 \"user:123\"\n   ZADD leaderboard 1800 \"user:456\"\n   O(log n) per update\n\n2. Get top 100:\n   ZREVRANGE leaderboard 0 99 WITHSCORES\n   Returns highest scores first\n\n3. Get player rank:\n   ZREVRANK leaderboard \"user:123\"\n   Returns 0-indexed position\n\n4. Get player score:\n   ZSCORE leaderboard \"user:123\"\n\n5. Get surrounding players:\n   rank = ZREVRANK leaderboard \"user:123\"\n   ZREVRANGE leaderboard (rank-5) (rank+5)\n\nScaling for 100M+ players:\n- Shard by region or game mode\n- Approximate ranks for very large sets\n- Periodic recalculation to secondary store\n- Cache top 1000 (rarely changes)\n\nTie-breaking:\n- Secondary sort by timestamp\n- Composite score: score * 1000000 + (MAX_TIME - timestamp)"
  },
  {
    "id": 71,
    "question": "What is Database Denormalization?",
    "answer": "Intentionally adding redundant data to optimize read performance by reducing joins. Trade storage space and write complexity for faster queries. Common in read-heavy applications and data warehouses",
    "example": "E-commerce order display:\n\nNormalized schema:\n- orders: {id, user_id, created_at}\n- order_items: {id, order_id, product_id, quantity}\n- products: {id, name, price}\n- users: {id, name, email}\n\nQuery for order history:\nSELECT o.*, u.name, p.name, oi.quantity\nFROM orders o\nJOIN users u ON o.user_id = u.id\nJOIN order_items oi ON oi.order_id = o.id\nJOIN products p ON oi.product_id = p.id\n4 table join - slow!\n\nDenormalized:\n- orders: {id, user_id, user_name, created_at}\n- order_items: {id, order_id, product_id, product_name, price_at_order, quantity}\n\nQuery:\nSELECT * FROM orders WHERE user_id = 123\nSELECT * FROM order_items WHERE order_id IN (...)\nNo joins needed!\n\nBenefits:\n- Faster reads\n- Simpler queries\n- Better for reporting/analytics\n\nDrawbacks:\n- Data duplication (storage cost)\n- Update anomalies: user changes name?\n- Must maintain consistency\n\nWhen to denormalize:\n- Read >> write ratio\n- Query performance critical\n- Join data rarely changes"
  },
  {
    "id": 72,
    "question": "What is the Materialized View pattern?",
    "answer": "Pre-compute and store query results to speed up reads. View is refreshed periodically or on data change. Useful for expensive aggregations and cross-service data in microservices. Trade freshness for performance",
    "example": "Analytics dashboard:\n\nProblem:\n- Dashboard shows: orders per day, revenue per region\n- Query scans millions of rows\n- Slow for every dashboard load\n\nMaterialized view:\nCREATE MATERIALIZED VIEW daily_stats AS\nSELECT\n  date_trunc('day', created_at) as day,\n  region,\n  count(*) as order_count,\n  sum(total) as revenue\nFROM orders\nGROUP BY 1, 2;\n\nRefresh strategies:\n\n1. Periodic refresh:\n   REFRESH MATERIALIZED VIEW daily_stats;\n   - Cron job every hour\n   - Simple, data may be stale\n\n2. Incremental refresh:\n   - Only process new/changed rows\n   - More complex, fresher data\n\n3. Event-driven (CQRS):\n   - On OrderCreated: update view\n   - Near real-time\n\nMicroservices use case:\n- Order service owns order data\n- Analytics service needs aggregates\n- Event: OrderCreated → update local view\n- No cross-service queries needed\n\nTrade-offs:\n- Storage for materialized data\n- Refresh computation cost\n- Data freshness vs query speed"
  },
  {
    "id": 73,
    "question": "What is the difference between Pull and Push architectures?",
    "answer": "Push: server sends data to clients when available. Pull: clients request data from server periodically. Push is real-time but requires connection management. Pull is simpler but has latency and may waste resources",
    "example": "News feed updates:\n\nPull (polling):\n- Client: GET /feed every 30 seconds\n- Server: return latest posts\n\nPros:\n- Simple implementation\n- Works through firewalls\n- No persistent connections\n\nCons:\n- Delayed updates (up to poll interval)\n- Wasted requests if no new data\n- Server load from frequent polling\n\nPush (WebSocket/SSE):\n- Client: open persistent connection\n- Server: push new posts immediately\n\nPros:\n- Real-time updates\n- Efficient: only send when data exists\n- Lower latency\n\nCons:\n- Connection management complex\n- Server maintains many connections\n- Reconnection handling needed\n\nFan-out strategies for push:\n\n1. Push on write:\n- User posts → push to all follower connections\n- Fast for followers\n- Expensive for celebrities (millions of followers)\n\n2. Pull on read:\n- User opens feed → query followees' posts\n- Fast write\n- Slow read (many queries)\n\n3. Hybrid:\n- Push for regular users\n- Pull for celebrity posts\n- Best of both worlds"
  },
  {
    "id": 74,
    "question": "What is the Health Check pattern?",
    "answer": "Endpoints that report service health status for load balancers and orchestrators. Distinguish liveness (is it running?) from readiness (can it serve traffic?). Enable graceful handling of failures and deployments",
    "example": "Kubernetes health probes:\n\n1. Liveness probe (/health/live):\n- \"Is the process alive?\"\n- Failure: container restarted\n- Should be cheap, no dependencies\n\n```\nGET /health/live\n200 OK {\"status\": \"alive\"}\n```\n\n2. Readiness probe (/health/ready):\n- \"Can you serve traffic?\"\n- Failure: removed from load balancer\n- Check dependencies: DB, cache, etc.\n\n```\nGET /health/ready\n503 Service Unavailable\n{\"status\": \"not ready\", \"db\": \"timeout\"}\n```\n\n3. Startup probe:\n- Initial check before liveness begins\n- For slow-starting applications\n\nHealth check implementation:\n\n```\napp.get('/health/ready', async (req, res) => {\n  const checks = {\n    database: await checkDb(),\n    redis: await checkRedis(),\n    kafka: await checkKafka()\n  };\n  \n  const healthy = Object.values(checks)\n    .every(c => c.status === 'up');\n    \n  res.status(healthy ? 200 : 503).json(checks);\n});\n```\n\nBest practices:\n- Timeout health checks (don't hang forever)\n- Don't fail readiness for non-critical services\n- Include version info for debugging"
  },
  {
    "id": 75,
    "question": "How would you design a unique ID generator for distributed systems?",
    "answer": "Generate globally unique IDs without coordination. Options: UUID, database sequences, Snowflake (timestamp + machine + sequence). Consider sortability, size, and collision probability based on requirements",
    "example": "Requirements:\n- Globally unique\n- Sortable by time (for database indexing)\n- No single point of failure\n- High throughput\n\nOptions:\n\n1. UUID v4 (random):\n   550e8400-e29b-41d4-a716-446655440000\n   + No coordination needed\n   - Not sortable, 128 bits, bad for indexes\n\n2. Database auto-increment:\n   + Simple, sortable\n   - Single point of failure\n   - Reveals count (security)\n\n3. Snowflake ID (Twitter):\n   64 bits total:\n   - 41 bits: timestamp (69 years)\n   - 10 bits: machine ID (1024 machines)\n   - 12 bits: sequence (4096/ms/machine)\n\n   Result: 1382971839123456001\n   \n   + Sortable by time\n   + No coordination\n   + Compact (64 bits)\n   - Need machine ID assignment\n\n4. ULID (Universally Unique Lexicographically Sortable):\n   01ARZ3NDEKTSV4RRFFQ69G5FAV\n   - 48 bits: timestamp\n   - 80 bits: randomness\n   + Lexicographically sortable\n   + No machine coordination\n\nImplementation (Snowflake):\n- Timestamp from epoch\n- Machine ID from config/ZooKeeper\n- Sequence resets each millisecond\n- Wait if sequence exhausted in same ms"
  },
  {
    "id": 76,
    "question": "What is the Anti-Corruption Layer pattern?",
    "answer": "A translation layer between your domain model and external systems or legacy code. Isolates your clean model from external concepts and changes. Prevents external model corruption from spreading into your codebase",
    "example": "Integrating with legacy billing system:\n\nProblem:\n- Legacy uses: CustomerNumber, InvoiceAmount, DueDate\n- Your domain: CustomerId, Money, PaymentDeadline\n- Legacy changes format randomly\n- Don't want legacy concepts in your code\n\nAnti-Corruption Layer:\n\n```\n// Your clean domain\nclass Invoice {\n  customerId: CustomerId\n  amount: Money\n  deadline: PaymentDeadline\n}\n\n// ACL translates between domains\nclass BillingSystemAdapter {\n  constructor(private legacy: LegacyBillingAPI) {}\n  \n  async createInvoice(invoice: Invoice): Promise<void> {\n    // Translate to legacy format\n    const legacyRequest = {\n      CustomerNumber: invoice.customerId.toLegacyFormat(),\n      InvoiceAmount: invoice.amount.toCents(),\n      DueDate: invoice.deadline.toYYYYMMDD()\n    };\n    await this.legacy.CreateInvoice(legacyRequest);\n  }\n}\n```\n\nBenefits:\n- Domain stays clean\n- Legacy changes isolated to ACL\n- Can mock ACL for testing\n- Gradual migration path\n\nUse cases:\n- Legacy system integration\n- Third-party API wrappers\n- Bounded context boundaries\n- Migration from monolith"
  },
  {
    "id": 77,
    "question": "What is the Token Bucket algorithm?",
    "answer": "A rate limiting algorithm using tokens that refill at a constant rate. Each request consumes a token. Allows bursts up to bucket capacity while maintaining average rate. Flexible and widely used for API rate limiting",
    "example": "API rate limit: 100 requests/minute, allow bursts\n\nToken bucket configuration:\n- Bucket capacity: 100 tokens\n- Refill rate: 100 tokens/minute (~1.67/second)\n\nAlgorithm:\n```\nclass TokenBucket {\n  capacity = 100\n  tokens = 100\n  refillRate = 1.67  // per second\n  lastRefill = now()\n  \n  allowRequest() {\n    this.refill()\n    if (this.tokens >= 1) {\n      this.tokens -= 1\n      return true\n    }\n    return false\n  }\n  \n  refill() {\n    elapsed = now() - this.lastRefill\n    this.tokens = min(\n      this.capacity,\n      this.tokens + elapsed * this.refillRate\n    )\n    this.lastRefill = now()\n  }\n}\n```\n\nBehavior:\n- Idle user: bucket fills to 100\n- Burst: can send 100 requests instantly\n- After burst: limited to 1.67/second\n- Smooth degradation\n\nComparison with Leaky Bucket:\n- Leaky: constant output rate, no bursts\n- Token: allows bursts, more flexible\n\nDistributed implementation:\n- Store tokens in Redis\n- Atomic operations for decrement\n- Consider Redis token bucket libraries"
  },
  {
    "id": 78,
    "question": "What is Change Data Capture (CDC)?",
    "answer": "Tracking and capturing changes in database in real-time to propagate to other systems. Reads database transaction log to detect inserts, updates, deletes. Enables real-time data pipelines without application changes",
    "example": "Syncing orders to analytics warehouse:\n\nTraditional approach:\n- Nightly batch job\n- SELECT * FROM orders WHERE updated > yesterday\n- Miss deletes, high database load\n- Data 24 hours stale\n\nCDC approach:\n1. CDC tool (Debezium) reads PostgreSQL WAL\n2. Captures every INSERT, UPDATE, DELETE\n3. Streams to Kafka as events\n4. Analytics consumes Kafka, updates warehouse\n\nCDC event example:\n```\n{\n  \"op\": \"u\",  // update\n  \"before\": {\"id\": 1, \"status\": \"pending\"},\n  \"after\": {\"id\": 1, \"status\": \"shipped\"},\n  \"source\": {\n    \"table\": \"orders\",\n    \"ts_ms\": 1234567890\n  }\n}\n```\n\nUse cases:\n- Real-time data warehousing\n- Cache invalidation (DB change → invalidate cache)\n- Microservices sync (outbox pattern alternative)\n- Audit logging\n- Search index updates (DB → Elasticsearch)\n\nTools:\n- Debezium (Kafka Connect based)\n- AWS DMS\n- Oracle GoldenGate\n- Postgres logical replication\n\nBenefits: No app changes, captures all changes, low latency"
  },
  {
    "id": 79,
    "question": "What is the difference between Active-Active and Active-Passive replication?",
    "answer": "Active-Active: all replicas serve traffic and accept writes, requiring conflict resolution. Active-Passive: one primary handles writes, secondaries for failover only. Active-Active offers better availability but more complexity",
    "example": "Global e-commerce platform:\n\nActive-Passive (simpler):\n- Primary in US: handles all writes\n- Replica in EU: read-only, for failover\n- On failure: promote EU to primary\n- Failover time: minutes\n\n┌────────────┐    replication    ┌────────────┐\n│  US (RW)   │ ───────────────→ │  EU (RO)   │\n│  Primary   │                   │  Standby   │\n└────────────┘                   └────────────┘\n\nActive-Active (complex):\n- US and EU both accept writes\n- Bidirectional replication\n- Conflicts possible: same row edited in both\n\n┌────────────┐  ←──replication──→  ┌────────────┐\n│  US (RW)   │                     │  EU (RW)   │\n│   Active   │                     │   Active   │\n└────────────┘                     └────────────┘\n\nConflict resolution strategies:\n- Last-write-wins (timestamp)\n- Merge changes (CRDTs)\n- Application-level resolution\n- Avoid conflicts by partitioning (US users → US DC)\n\nActive-Active use cases:\n- Global low-latency writes\n- Zero downtime requirement\n- Geographic data sovereignty"
  },
  {
    "id": 80,
    "question": "What is the Scatter-Gather pattern?",
    "answer": "Broadcast a request to multiple services in parallel, then aggregate their responses. Used for distributed search, price comparison, or querying partitioned data. Must handle partial failures and timeouts gracefully",
    "example": "Flight search across airlines:\n\nProblem:\n- Search: NYC to LAX on Dec 15\n- Need to query: United, Delta, American, etc.\n- Sequential: 10 airlines × 200ms = 2 seconds\n\nScatter-Gather:\n1. Scatter: send request to all airlines in parallel\n2. Wait: collect responses (with timeout)\n3. Gather: merge results, sort by price\n\n```\nasync function searchFlights(query) {\n  const airlines = [united, delta, american, ...];\n  \n  // Scatter\n  const promises = airlines.map(airline => \n    airline.search(query)\n      .catch(err => ({ error: err, airline }))\n  );\n  \n  // Gather with timeout\n  const results = await Promise.allSettled(\n    promises.map(p => withTimeout(p, 500))\n  );\n  \n  // Merge successful responses\n  return results\n    .filter(r => r.status === 'fulfilled')\n    .flatMap(r => r.value.flights)\n    .sort((a, b) => a.price - b.price);\n}\n```\n\nHandling failures:\n- Timeout: return partial results\n- Some failures: show results from available airlines\n- All failures: show error\n\nOptimizations:\n- Return early: show results as they arrive\n- Caching: cache airline responses\n- Circuit breaker: skip known-slow airlines"
  },
  {
    "id": 81,
    "question": "What is a Quorum in distributed systems?",
    "answer": "The minimum number of nodes that must agree for an operation to succeed. For N nodes, typically requires majority (N/2 + 1) for both reads and writes. Ensures consistency by guaranteeing overlap between read and write sets",
    "example": "Distributed database with 5 nodes:\n\nQuorum calculation:\n- Total nodes: N = 5\n- Write quorum: W = 3 (majority)\n- Read quorum: R = 3 (majority)\n- Rule: W + R > N (ensures overlap)\n\nWrite operation:\n1. Client writes to all 5 nodes\n2. Wait for 3 acknowledgments\n3. Return success\n\nRead operation:\n1. Read from 3 nodes\n2. Compare versions/timestamps\n3. Return latest value\n\nWhy it works:\n- Write reached 3 nodes\n- Read queries 3 nodes\n- At least 1 node has latest (overlap)\n\nTuning for different needs:\n\nStrong consistency (R + W > N):\n- W=3, R=3: balanced\n- W=5, R=1: fast reads, slow writes\n- W=1, R=5: fast writes, slow reads\n\nEventual consistency (R + W ≤ N):\n- W=1, R=1: fast but may read stale\n\nFailure handling:\n- 2 nodes down: can still write (3 available)\n- 3 nodes down: writes fail (can't reach quorum)\n\nUsed by: Cassandra, DynamoDB, Riak"
  },
  {
    "id": 82,
    "question": "How would you design a recommendation system?",
    "answer": "Suggest relevant items based on user behavior and item attributes. Approaches: collaborative filtering (similar users/items), content-based (item features), hybrid. Handle cold start for new users/items. Balance relevance, diversity, and freshness",
    "example": "Netflix-like movie recommendations:\n\nData signals:\n- Explicit: ratings, likes\n- Implicit: watch time, searches, clicks\n\nApproaches:\n\n1. Collaborative Filtering:\n- User-based: \"Users like you watched X\"\n- Item-based: \"People who watched A also watched B\"\n- Matrix factorization (SVD)\n\n2. Content-Based:\n- Movie features: genre, actors, director\n- User profile: preferred genres, actors\n- Match user profile to movie features\n\n3. Hybrid:\n- Combine collaborative + content-based\n- Weight based on data availability\n\nArchitecture:\n- Offline pipeline: train models on historical data\n- Batch: pre-compute recommendations daily\n- Real-time: adjust for recent activity\n\nCold start solutions:\n- New user: popular items, ask preferences\n- New item: content-based initially\n- Explore/exploit: show some random items\n\nEvaluation metrics:\n- Precision/Recall at K\n- Click-through rate\n- Watch time\n- Diversity (avoid filter bubble)\n\nScale:\n- Pre-compute top 100 per user\n- Store in Redis for fast serving\n- A/B test algorithm changes"
  },
  {
    "id": 83,
    "question": "What is the Priority Queue pattern?",
    "answer": "Process messages based on priority rather than arrival order. High-priority items jump ahead in the queue. Implement with multiple queues per priority level or heap-based data structures. Essential for SLA-sensitive systems",
    "example": "Customer support ticket system:\n\nRequirements:\n- Premium customers: < 1 hour response\n- Standard customers: < 24 hours\n- Free tier: best effort\n\nImplementation options:\n\n1. Multiple queues:\n- Queue: premium (polled first)\n- Queue: standard (polled second)\n- Queue: free (polled last)\n\nWorker logic:\n```\nwhile (true) {\n  ticket = premium.poll()\n       || standard.poll()\n       || free.poll();\n  if (ticket) process(ticket);\n}\n```\n\n2. Single priority queue:\n- Heap-based: O(log n) insert, O(log n) pop\n- Redis ZSET: score = priority * 1000 + timestamp\n\n3. Weighted fair queuing:\n- Premium: 60% of processing time\n- Standard: 30%\n- Free: 10%\n- Prevents starvation of low priority\n\nPriority inversion problem:\n- Low-priority task holds resource\n- High-priority task blocked\n- Solution: priority inheritance\n\nStarvation prevention:\n- Age-based priority boost\n- After X time, promote to higher priority\n- Guarantees eventual processing"
  },
  {
    "id": 84,
    "question": "What is the Saga Orchestration vs Choreography debate?",
    "answer": "Two ways to coordinate distributed sagas. Orchestration: central coordinator directs each step. Choreography: services react to events, no central controller. Orchestration is easier to understand, choreography is more decoupled",
    "example": "Order fulfillment saga:\n\nOrchestration (central coordinator):\n```\nOrderOrchestrator:\n1. Tell InventoryService: reserve items\n2. If success: Tell PaymentService: charge\n3. If success: Tell ShippingService: ship\n4. If any fails: send compensations\n```\n\nPros:\n- Clear flow, easy to follow\n- Centralized error handling\n- Easy to add steps\n\nCons:\n- Orchestrator is coupling point\n- Single point of failure\n- Orchestrator must know all services\n\nChoreography (event-driven):\n```\nOrderService: publishes OrderCreated\nInventoryService: listens, reserves, publishes InventoryReserved\nPaymentService: listens, charges, publishes PaymentProcessed\nShippingService: listens, ships, publishes OrderShipped\n```\n\nPros:\n- Loosely coupled\n- No single point of failure\n- Services independent\n\nCons:\n- Hard to understand full flow\n- Distributed debugging nightmare\n- Cyclic dependencies possible\n\nWhen to use:\n- Orchestration: complex workflows, few services\n- Choreography: simple flows, many services\n- Hybrid: orchestrate critical paths, choreograph rest"
  },
  {
    "id": 85,
    "question": "What is the difference between Throughput and Latency?",
    "answer": "Throughput: amount of work completed per time unit (requests/second). Latency: time to complete a single request. They're related but different: high throughput doesn't guarantee low latency. Optimize based on requirements",
    "example": "Highway analogy:\n- Throughput: cars passing per hour (capacity)\n- Latency: time for one car to travel (delay)\n\n8-lane highway: high throughput, but traffic = high latency\n2-lane highway: low throughput, but empty = low latency\n\nWeb service metrics:\n\nThroughput:\n- 10,000 requests/second\n- Measured: total requests / time period\n- Scale: add more servers\n\nLatency:\n- P50: 50ms (median)\n- P95: 200ms (95th percentile)\n- P99: 500ms (99th percentile)\n- Measured: individual request times\n\nTrade-offs:\n\n1. Batching increases throughput, hurts latency:\n   - Process 100 items at once\n   - Higher throughput (efficient)\n   - First item waits for batch to fill\n\n2. Caching improves both:\n   - Cache hit: fast response (latency)\n   - Less backend load (throughput)\n\n3. Async processing:\n   - Return immediately (latency)\n   - Process more requests (throughput)\n\nSLA example:\n- Throughput: handle 10K rps\n- Latency: P99 < 200ms\n- Both must be met simultaneously"
  }
]
