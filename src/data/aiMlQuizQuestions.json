[
  {
    "id": 1,
    "question": "What is the difference between supervised and unsupervised learning?",
    "answer": "**Supervised learning** uses labeled data where the algorithm learns to map inputs to known outputs (e.g., classification, regression). The model is trained with input-output pairs.\n\n**Unsupervised learning** works with unlabeled data to find hidden patterns or structures (e.g., clustering, dimensionality reduction). The model discovers relationships without predefined labels.",
    "example": "# Supervised: Predicting house prices (labeled)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)  # y_train contains actual prices\n\n# Unsupervised: Customer segmentation (unlabeled)\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)  # No labels needed"
  },
  {
    "id": 2,
    "question": "What is overfitting and how can you prevent it?",
    "answer": "**Overfitting** occurs when a model learns the training data too well, including noise and outliers, resulting in poor generalization to new data. The model has high variance and low bias.\n\n**Prevention techniques:**\n- Cross-validation\n- Regularization (L1/L2)\n- Early stopping\n- Dropout (for neural networks)\n- Data augmentation\n- Reducing model complexity\n- Ensemble methods",
    "example": "# L2 Regularization (Ridge)\nfrom sklearn.linear_model import Ridge\nmodel = Ridge(alpha=1.0)  # alpha controls regularization strength\n\n# Dropout in neural networks\nimport torch.nn as nn\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(100, 50)\n        self.dropout = nn.Dropout(0.5)  # 50% dropout\n        self.fc2 = nn.Linear(50, 10)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)"
  },
  {
    "id": 3,
    "question": "Explain the bias-variance tradeoff.",
    "answer": "The **bias-variance tradeoff** is a fundamental concept describing the balance between two sources of error:\n\n**Bias**: Error from oversimplified assumptions. High bias leads to underfitting (model misses patterns).\n\n**Variance**: Error from sensitivity to training data fluctuations. High variance leads to overfitting (model captures noise).\n\n**Total Error** = Bias² + Variance + Irreducible Error\n\nThe goal is finding the sweet spot where both are minimized.",
    "example": "# High Bias (Underfitting) - Too simple\nfrom sklearn.linear_model import LinearRegression\nsimple_model = LinearRegression()  # May miss complex patterns\n\n# High Variance (Overfitting) - Too complex\nfrom sklearn.tree import DecisionTreeRegressor\ncomplex_model = DecisionTreeRegressor(max_depth=None)  # Memorizes data\n\n# Balanced - Regularized model\nfrom sklearn.ensemble import RandomForestRegressor\nbalanced_model = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_leaf=5\n)"
  },
  {
    "id": 4,
    "question": "What are precision, recall, and F1-score? When would you prioritize one over another?",
    "answer": "**Precision** = TP / (TP + FP) — Of all positive predictions, how many were correct? Prioritize when false positives are costly (spam detection).\n\n**Recall** = TP / (TP + FN) — Of all actual positives, how many were found? Prioritize when false negatives are costly (cancer detection).\n\n**F1-Score** = 2 × (Precision × Recall) / (Precision + Recall) — Harmonic mean balancing both. Use when you need a single metric and classes are imbalanced.",
    "example": "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n\nprint(f\"Precision: {precision_score(y_true, y_pred):.2f}\")  # 0.80\nprint(f\"Recall: {recall_score(y_true, y_pred):.2f}\")        # 0.67\nprint(f\"F1-Score: {f1_score(y_true, y_pred):.2f}\")          # 0.73\n\n# Full report\nprint(classification_report(y_true, y_pred))"
  },
  {
    "id": 5,
    "question": "What is gradient descent and what are its variants?",
    "answer": "**Gradient descent** is an optimization algorithm that iteratively updates parameters by moving in the direction of steepest descent (negative gradient) to minimize a loss function.\n\n**Variants:**\n- **Batch GD**: Uses entire dataset per update. Stable but slow.\n- **Stochastic GD (SGD)**: Uses one sample per update. Fast but noisy.\n- **Mini-batch GD**: Uses small batches. Balances speed and stability.\n- **Momentum**: Accelerates convergence using velocity.\n- **Adam**: Adaptive learning rates per parameter with momentum.",
    "example": "import torch.optim as optim\n\n# Standard SGD\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# SGD with Momentum\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Adam (most commonly used)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    loss.backward()\n    optimizer.step()"
  },
  {
    "id": 6,
    "question": "What is the difference between bagging and boosting?",
    "answer": "Both are ensemble methods combining multiple models:\n\n**Bagging (Bootstrap Aggregating)**:\n- Trains models in parallel on random subsets (with replacement)\n- Reduces variance\n- Models are independent\n- Example: Random Forest\n\n**Boosting**:\n- Trains models sequentially, each correcting predecessor's errors\n- Reduces bias\n- Models are dependent\n- Examples: AdaBoost, Gradient Boosting, XGBoost",
    "example": "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Bagging - Random Forest\nrf = RandomForestClassifier(\n    n_estimators=100,\n    max_features='sqrt',\n    bootstrap=True  # Bagging enabled\n)\n\n# Boosting - Gradient Boosting\ngb = GradientBoostingClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3\n)\n\n# XGBoost (optimized boosting)\nxgb = XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3\n)"
  },
  {
    "id": 7,
    "question": "Explain the architecture of a Transformer and why self-attention is important.",
    "answer": "**Transformer architecture** (introduced in 'Attention Is All You Need'):\n- **Encoder**: Processes input sequence with self-attention + feed-forward layers\n- **Decoder**: Generates output with masked self-attention + cross-attention\n- **Positional encoding**: Adds sequence order information\n\n**Self-attention importance**:\n- Captures long-range dependencies regardless of distance\n- Enables parallel processing (unlike RNNs)\n- Each position attends to all positions in input\n- Foundation for BERT, GPT, and modern LLMs",
    "example": "import torch\nimport torch.nn as nn\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.multihead_attn = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads\n        )\n    \n    def forward(self, x):\n        # x shape: (seq_len, batch, embed_dim)\n        # Self-attention: query, key, value are all x\n        attn_output, attn_weights = self.multihead_attn(x, x, x)\n        return attn_output, attn_weights\n\n# Using PyTorch's TransformerEncoder\nencoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)"
  },
  {
    "id": 8,
    "question": "What is backpropagation and how does it work?",
    "answer": "**Backpropagation** is the algorithm for computing gradients of the loss function with respect to network weights, enabling gradient descent optimization.\n\n**How it works:**\n1. **Forward pass**: Compute predictions and loss\n2. **Backward pass**: Apply chain rule to compute gradients layer by layer, from output to input\n3. **Update weights**: Adjust parameters using gradients\n\nThe chain rule allows decomposing complex gradient computations into simpler local gradients.",
    "example": "import torch\nimport torch.nn as nn\n\n# Simple network\nmodel = nn.Sequential(\n    nn.Linear(10, 5),\n    nn.ReLU(),\n    nn.Linear(5, 1)\n)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Training step\nx = torch.randn(32, 10)\ny = torch.randn(32, 1)\n\n# Forward pass\npredictions = model(x)\nloss = criterion(predictions, y)\n\n# Backward pass (computes gradients via chain rule)\nloss.backward()\n\n# Check gradients\nprint(model[0].weight.grad.shape)  # Gradients for first layer\n\n# Update weights\noptimizer.step()"
  },
  {
    "id": 9,
    "question": "What are word embeddings and how do Word2Vec and contextual embeddings differ?",
    "answer": "**Word embeddings** are dense vector representations capturing semantic meaning of words.\n\n**Word2Vec** (static embeddings):\n- Fixed vector per word regardless of context\n- Trained via CBOW or Skip-gram\n- 'bank' has same embedding in 'river bank' and 'bank account'\n\n**Contextual embeddings** (BERT, GPT):\n- Dynamic vectors based on surrounding context\n- Same word gets different embeddings in different sentences\n- Better capture polysemy and nuance",
    "example": "# Word2Vec - Static embeddings\nfrom gensim.models import Word2Vec\nsentences = [[\"cat\", \"sat\", \"mat\"], [\"dog\", \"ran\", \"fast\"]]\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\nvector = model.wv['cat']  # Same vector always\n\n# BERT - Contextual embeddings\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Same word, different contexts\ntext1 = \"I deposited money at the bank\"\ntext2 = \"I sat by the river bank\"\n\nfor text in [text1, text2]:\n    inputs = tokenizer(text, return_tensors='pt')\n    outputs = model(**inputs)\n    # 'bank' embedding differs based on context"
  },
  {
    "id": 10,
    "question": "How do you handle imbalanced datasets in classification?",
    "answer": "**Techniques for imbalanced data:**\n\n**Data-level:**\n- Oversampling minority class (SMOTE)\n- Undersampling majority class\n- Synthetic data generation\n\n**Algorithm-level:**\n- Class weights in loss function\n- Cost-sensitive learning\n- Anomaly detection approach\n\n**Evaluation:**\n- Use precision, recall, F1, AUC-ROC instead of accuracy\n- Stratified cross-validation",
    "example": "from imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# SMOTE oversampling\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Class weights\nclass_weights = compute_class_weight(\n    'balanced', \n    classes=np.unique(y), \n    y=y\n)\nweight_dict = dict(zip(np.unique(y), class_weights))\n\n# Apply weights in model\nmodel = RandomForestClassifier(class_weight='balanced')\n\n# Or in PyTorch loss\nimport torch.nn as nn\nweights = torch.tensor([1.0, 10.0])  # Higher weight for minority\ncriterion = nn.CrossEntropyLoss(weight=weights)"
  },
  {
    "id": 11,
    "question": "What is cross-validation and why is k-fold preferred over a simple train/test split?",
    "answer": "**Cross-validation** is a technique to assess model generalization by training and evaluating on different data subsets.\n\n**K-fold cross-validation:**\n1. Split data into k equal folds\n2. Train on k-1 folds, validate on remaining fold\n3. Repeat k times, rotating the validation fold\n4. Average results across all folds\n\n**Advantages over simple split:**\n- Uses all data for both training and validation\n- More reliable performance estimate\n- Reduces variance from lucky/unlucky splits\n- Better for smaller datasets",
    "example": "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\n# Simple k-fold\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(f\"Accuracy: {scores.mean():.3f} (+/- {scores.std()*2:.3f})\")\n\n# Stratified k-fold (maintains class distribution)\nstrat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=strat_kfold, scoring='f1')\n\n# Multiple metrics\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(\n    model, X, y, cv=5,\n    scoring=['accuracy', 'precision', 'recall', 'f1']\n)"
  },
  {
    "id": 12,
    "question": "Explain the difference between generative and discriminative models.",
    "answer": "**Discriminative models** learn the decision boundary P(y|x) directly:\n- Focus on distinguishing between classes\n- Examples: Logistic Regression, SVM, Neural Networks\n- Generally better for classification when you have enough data\n\n**Generative models** learn the joint distribution P(x,y) or P(x):\n- Model how data is generated\n- Can generate new samples\n- Examples: Naive Bayes, GMM, VAE, GANs, GPT\n- Better with limited data, can handle missing features",
    "example": "# Discriminative - Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\ndiscrim_model = LogisticRegression()\ndiscrim_model.fit(X_train, y_train)\nprob = discrim_model.predict_proba(X_test)  # P(y|x) directly\n\n# Generative - Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngen_model = GaussianNB()\ngen_model.fit(X_train, y_train)\n# Internally models P(x|y) and P(y), uses Bayes' theorem\n\n# Generative - VAE for image generation\nclass VAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()  # q(z|x)\n        self.decoder = Decoder()  # p(x|z)\n    \n    def generate(self, num_samples):\n        z = torch.randn(num_samples, latent_dim)\n        return self.decoder(z)  # Generate new samples"
  },
  {
    "id": 13,
    "question": "What is batch normalization and why is it used?",
    "answer": "**Batch normalization** normalizes layer inputs by re-centering and re-scaling across a mini-batch.\n\n**Benefits:**\n- Reduces internal covariate shift\n- Allows higher learning rates\n- Acts as regularization\n- Faster convergence\n- Reduces sensitivity to initialization\n\n**Formula:** Normalize to zero mean, unit variance, then scale (γ) and shift (β) with learnable parameters.",
    "example": "import torch.nn as nn\n\n# In CNNs - BatchNorm2d\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.bn = nn.BatchNorm2d(out_ch)  # After conv\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\n# In fully connected - BatchNorm1d\nmodel = nn.Sequential(\n    nn.Linear(100, 50),\n    nn.BatchNorm1d(50),\n    nn.ReLU(),\n    nn.Linear(50, 10)\n)\n\n# Layer Normalization (for transformers/RNNs)\nln = nn.LayerNorm(normalized_shape=512)  # Normalizes across features"
  },
  {
    "id": 14,
    "question": "How do you design a feature store for ML systems in production?",
    "answer": "**Feature store** is a centralized repository for storing, managing, and serving ML features.\n\n**Key components:**\n- **Offline store**: Batch features for training (data warehouse)\n- **Online store**: Low-latency features for inference (Redis, DynamoDB)\n- **Feature registry**: Metadata, lineage, documentation\n- **Transformation engine**: Feature computation pipelines\n\n**Design considerations:**\n- Point-in-time correctness (avoid data leakage)\n- Feature freshness requirements\n- Consistency between training and serving\n- Feature versioning and monitoring",
    "example": "# Using Feast feature store\nfrom feast import FeatureStore, Entity, Feature, FeatureView\nfrom feast.types import Float32, Int64\n\n# Define entity\nuser = Entity(name=\"user_id\", value_type=Int64)\n\n# Define feature view\nuser_features = FeatureView(\n    name=\"user_features\",\n    entities=[\"user_id\"],\n    features=[\n        Feature(name=\"avg_purchase_amount\", dtype=Float32),\n        Feature(name=\"total_orders\", dtype=Int64),\n    ],\n    ttl=timedelta(days=1),\n    online=True,\n)\n\n# Retrieve for training (point-in-time join)\nstore = FeatureStore(repo_path=\".\")\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    features=[\"user_features:avg_purchase_amount\", \"user_features:total_orders\"]\n).to_df()\n\n# Retrieve for online inference\nonline_features = store.get_online_features(\n    features=[\"user_features:avg_purchase_amount\"],\n    entity_rows=[{\"user_id\": 123}]\n).to_dict()"
  },
  {
    "id": 15,
    "question": "What is the vanishing gradient problem and how do modern architectures address it?",
    "answer": "**Vanishing gradient problem**: In deep networks, gradients become exponentially smaller as they backpropagate through layers, preventing early layers from learning.\n\n**Causes:**\n- Sigmoid/tanh activations saturate (gradient → 0)\n- Many multiplications of small values\n\n**Solutions:**\n- **ReLU activation**: Non-saturating, gradient = 1 for positive values\n- **Residual connections**: Skip connections allow gradient flow\n- **LSTM/GRU**: Gating mechanisms preserve gradients\n- **Batch normalization**: Maintains healthy gradient magnitudes\n- **Proper initialization**: Xavier, He initialization",
    "example": "import torch.nn as nn\n\n# ResNet-style skip connection\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n    \n    def forward(self, x):\n        residual = x  # Skip connection\n        out = nn.functional.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual  # Add skip connection\n        return nn.functional.relu(out)\n\n# He initialization for ReLU networks\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')"
  },
  {
    "id": 16,
    "question": "Explain how you would implement A/B testing for ML models in production.",
    "answer": "**A/B testing for ML models** compares a new model (treatment) against the current model (control) using statistical methods.\n\n**Implementation steps:**\n1. **Traffic splitting**: Randomly assign users to control/treatment\n2. **Metric definition**: Primary (conversion) and guardrail metrics\n3. **Sample size calculation**: Determine required users for statistical power\n4. **Run experiment**: Collect data over sufficient time\n5. **Statistical analysis**: Hypothesis testing, confidence intervals\n6. **Decision**: Ship, iterate, or rollback\n\n**Considerations:**\n- Novelty effects\n- Network effects\n- Multiple comparisons correction",
    "example": "import numpy as np\nfrom scipy import stats\n\n# Simple A/B test analysis\ndef ab_test_analysis(control_conversions, control_total,\n                     treatment_conversions, treatment_total):\n    # Conversion rates\n    p_control = control_conversions / control_total\n    p_treatment = treatment_conversions / treatment_total\n    \n    # Pooled probability\n    p_pooled = (control_conversions + treatment_conversions) / \\\n               (control_total + treatment_total)\n    \n    # Standard error\n    se = np.sqrt(p_pooled * (1-p_pooled) * \n                 (1/control_total + 1/treatment_total))\n    \n    # Z-score and p-value\n    z_score = (p_treatment - p_control) / se\n    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n    \n    # Lift\n    lift = (p_treatment - p_control) / p_control * 100\n    \n    return {\n        'control_rate': p_control,\n        'treatment_rate': p_treatment,\n        'lift': f\"{lift:.2f}%\",\n        'p_value': p_value,\n        'significant': p_value < 0.05\n    }"
  },
  {
    "id": 17,
    "question": "What are the key considerations when deploying LLMs to production?",
    "answer": "**Key LLM deployment considerations:**\n\n**Infrastructure:**\n- GPU memory requirements (model size)\n- Batching strategies for throughput\n- Quantization (INT8, INT4) for efficiency\n- Model sharding across GPUs\n\n**Performance:**\n- Latency requirements and KV-cache optimization\n- Streaming responses for UX\n- Request queuing and rate limiting\n\n**Quality & Safety:**\n- Prompt injection protection\n- Output filtering and guardrails\n- Hallucination mitigation (RAG)\n- Monitoring for drift and quality\n\n**Cost:**\n- Token usage optimization\n- Caching common responses\n- Model selection (smaller models for simpler tasks)",
    "example": "# vLLM for efficient LLM serving\nfrom vllm import LLM, SamplingParams\n\n# Load model with quantization\nllm = LLM(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    quantization=\"awq\",  # 4-bit quantization\n    tensor_parallel_size=2,  # Shard across 2 GPUs\n    gpu_memory_utilization=0.9\n)\n\n# Batch inference for throughput\nsampling_params = SamplingParams(\n    temperature=0.7,\n    max_tokens=256,\n    stop=[\"</s>\"]\n)\n\nprompts = [\"What is ML?\", \"Explain transformers\"]\noutputs = llm.generate(prompts, sampling_params)\n\n# Streaming with FastAPI\nfrom fastapi.responses import StreamingResponse\n\nasync def stream_response(prompt: str):\n    async for chunk in llm.generate_stream(prompt):\n        yield chunk.text"
  },
  {
    "id": 18,
    "question": "Explain Retrieval-Augmented Generation (RAG) and its architecture.",
    "answer": "**RAG** combines retrieval systems with generative models to ground responses in external knowledge.\n\n**Architecture:**\n1. **Indexing**: Chunk documents, create embeddings, store in vector DB\n2. **Retrieval**: Query embedding → find similar chunks\n3. **Augmentation**: Inject retrieved context into prompt\n4. **Generation**: LLM generates response using context\n\n**Benefits:**\n- Reduces hallucinations\n- Enables knowledge updates without retraining\n- Provides source attribution\n- More cost-effective than fine-tuning for domain knowledge",
    "example": "from langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\n\n# 1. Index documents\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nchunks = text_splitter.split_documents(documents)\n\n# 2. Create vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(chunks, embeddings)\n\n# 3. Create RAG chain\nllm = ChatOpenAI(model=\"gpt-4\")\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(\n        search_kwargs={\"k\": 4}  # Top 4 chunks\n    ),\n    return_source_documents=True\n)\n\n# 4. Query\nresult = qa_chain({\"query\": \"What is our refund policy?\"})\nprint(result[\"result\"])\nprint(result[\"source_documents\"])"
  },
  {
    "id": 19,
    "question": "How do you implement model monitoring and detect data/model drift in production?",
    "answer": "**Model monitoring** tracks model health and detects degradation in production.\n\n**Types of drift:**\n- **Data drift**: Input distribution changes (P(X) shifts)\n- **Concept drift**: Relationship between input and output changes (P(Y|X) shifts)\n- **Prediction drift**: Model output distribution changes\n\n**Detection methods:**\n- Statistical tests (KS test, chi-square, PSI)\n- Distribution comparisons over time windows\n- Performance monitoring against labeled samples\n\n**Key metrics:**\n- Input feature distributions\n- Prediction distributions\n- Latency, throughput, error rates\n- Business metrics (conversion, engagement)",
    "example": "import numpy as np\nfrom scipy import stats\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metrics import DataDriftTable, DatasetDriftMetric\n\n# Population Stability Index (PSI)\ndef calculate_psi(expected, actual, bins=10):\n    expected_hist, bin_edges = np.histogram(expected, bins=bins)\n    actual_hist, _ = np.histogram(actual, bins=bin_edges)\n    \n    expected_pct = expected_hist / len(expected) + 0.0001\n    actual_pct = actual_hist / len(actual) + 0.0001\n    \n    psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n    return psi  # PSI > 0.2 indicates significant drift\n\n# Using Evidently for drift detection\nreport = Report(metrics=[\n    DatasetDriftMetric(),\n    DataDriftTable()\n])\n\nreport.run(\n    reference_data=training_data,\n    current_data=production_data,\n    column_mapping=ColumnMapping()\n)\nreport.save_html(\"drift_report.html\")"
  },
  {
    "id": 20,
    "question": "What is fine-tuning vs prompt engineering vs RAG, and when should you use each?",
    "answer": "**Prompt Engineering:**\n- Craft instructions/examples in the prompt\n- Use when: Quick iteration, general tasks, no training data\n- Pros: No training cost, instant changes\n- Cons: Limited by context window, inconsistent\n\n**RAG (Retrieval-Augmented Generation):**\n- Retrieve relevant docs and inject into context\n- Use when: Domain knowledge needed, data changes frequently, need citations\n- Pros: Up-to-date knowledge, attributable\n- Cons: Retrieval quality dependent, added latency\n\n**Fine-tuning:**\n- Train model on task-specific data\n- Use when: Specific style/format needed, consistent behavior required, have quality training data\n- Pros: Consistent outputs, smaller context needed\n- Cons: Training cost, can forget general knowledge",
    "example": "# Decision framework:\n\n# 1. PROMPT ENGINEERING - Start here\n#    - \"Summarize this in 3 bullet points\"\n#    - Few-shot examples in prompt\n#    - Works for ~60% of use cases\n\n# 2. RAG - When external knowledge needed\nif needs_domain_knowledge or data_changes_frequently:\n    use_rag = True\n    # Example: Customer support with product docs\n    # Example: Legal research with case law\n\n# 3. FINE-TUNING - When behavior/format matters\nif (need_consistent_style or \n    specific_output_format or \n    have_quality_labeled_data):\n    use_fine_tuning = True\n    # Example: Code completion in company style\n    # Example: Medical report generation\n\n# 4. COMBINATION - Often best\n# Fine-tuned model + RAG for domain-specific Q&A\n# Example: Fine-tune for tone, RAG for knowledge"
  },
  {
    "id": 21,
    "question": "What is a Convolutional Neural Network (CNN) and how do convolutions work?",
    "answer": "**CNN** is a neural network designed for processing grid-like data (images, audio) using convolutional layers.\n\n**How convolutions work:**\n- A filter/kernel slides across the input\n- At each position, element-wise multiplication and sum produces one output value\n- Learns local patterns (edges, textures) that are translation invariant\n\n**Key components:**\n- **Convolutional layers**: Extract features\n- **Pooling layers**: Reduce spatial dimensions\n- **Fully connected layers**: Classification at the end",
    "example": "import torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            # Conv layer: 3 input channels, 32 filters, 3x3 kernel\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # Reduces spatial size by half\n            \n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x"
  },
  {
    "id": 22,
    "question": "What are RNNs and why do LSTMs/GRUs solve their limitations?",
    "answer": "**RNN (Recurrent Neural Network)** processes sequences by maintaining hidden state across time steps.\n\n**RNN limitations:**\n- Vanishing/exploding gradients over long sequences\n- Difficulty learning long-term dependencies\n\n**LSTM (Long Short-Term Memory):**\n- Cell state acts as memory highway\n- Gates (forget, input, output) control information flow\n- Can learn dependencies over 100s of steps\n\n**GRU (Gated Recurrent Unit):**\n- Simplified LSTM with 2 gates (reset, update)\n- Fewer parameters, often similar performance\n- Faster to train",
    "example": "import torch.nn as nn\n\n# Simple RNN (suffers from vanishing gradients)\nrnn = nn.RNN(input_size=100, hidden_size=256, num_layers=2, batch_first=True)\n\n# LSTM (handles long sequences)\nlstm = nn.LSTM(\n    input_size=100,\n    hidden_size=256,\n    num_layers=2,\n    batch_first=True,\n    dropout=0.2,\n    bidirectional=True  # Process both directions\n)\n\n# GRU (faster, fewer params)\ngru = nn.GRU(input_size=100, hidden_size=256, num_layers=2, batch_first=True)\n\n# Usage\nx = torch.randn(32, 50, 100)  # (batch, seq_len, features)\noutput, (h_n, c_n) = lstm(x)  # LSTM returns hidden and cell state\noutput, h_n = gru(x)          # GRU only returns hidden state"
  },
  {
    "id": 23,
    "question": "What is transfer learning and when should you use it?",
    "answer": "**Transfer learning** uses knowledge from a model trained on one task to improve performance on a different but related task.\n\n**When to use:**\n- Limited labeled data for target task\n- Source and target domains are related\n- Training from scratch is computationally expensive\n\n**Strategies:**\n- **Feature extraction**: Freeze pretrained layers, train new classifier\n- **Fine-tuning**: Unfreeze some/all layers, train with small learning rate\n- **Domain adaptation**: Bridge domain gap between source and target",
    "example": "import torch\nimport torchvision.models as models\nimport torch.nn as nn\n\n# Load pretrained ResNet\nmodel = models.resnet50(pretrained=True)\n\n# Strategy 1: Feature extraction (freeze all layers)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace classifier for new task\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Strategy 2: Fine-tuning (unfreeze later layers)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze layer4 and fc\nfor param in model.layer4.parameters():\n    param.requires_grad = True\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n# Use smaller learning rate for pretrained layers\noptimizer = torch.optim.Adam([\n    {'params': model.layer4.parameters(), 'lr': 1e-5},\n    {'params': model.fc.parameters(), 'lr': 1e-3}\n])"
  },
  {
    "id": 24,
    "question": "What are common activation functions and when would you use each?",
    "answer": "**Activation functions** introduce non-linearity, enabling networks to learn complex patterns.\n\n**Common functions:**\n- **ReLU**: f(x) = max(0, x). Default for hidden layers. Fast, avoids vanishing gradient.\n- **Leaky ReLU**: f(x) = max(0.01x, x). Addresses dying ReLU problem.\n- **Sigmoid**: f(x) = 1/(1+e^-x). Output [0,1]. Use for binary classification output.\n- **Tanh**: f(x) = (e^x - e^-x)/(e^x + e^-x). Output [-1,1]. Zero-centered.\n- **Softmax**: Converts logits to probabilities. Use for multi-class classification output.\n- **GELU**: Used in transformers (BERT, GPT). Smooth approximation of ReLU.",
    "example": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Built-in activations\nrelu = nn.ReLU()\nleaky_relu = nn.LeakyReLU(negative_slope=0.01)\nsigmoid = nn.Sigmoid()\ntanh = nn.Tanh()\ngelu = nn.GELU()\n\n# In a network\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(100, 50)\n        self.fc2 = nn.Linear(50, 10)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))     # Hidden layer: ReLU\n        x = self.fc2(x)              # Output: raw logits\n        return x  # Apply softmax in loss function (CrossEntropyLoss)\n\n# Softmax for inference\nlogits = model(x)\nprobabilities = F.softmax(logits, dim=-1)"
  },
  {
    "id": 25,
    "question": "What are common loss functions and when to use each?",
    "answer": "**Loss functions** measure prediction error and guide optimization.\n\n**Regression:**\n- **MSE (L2)**: Mean squared error. Penalizes large errors heavily.\n- **MAE (L1)**: Mean absolute error. Robust to outliers.\n- **Huber**: Combines MSE and MAE. Robust yet smooth.\n\n**Classification:**\n- **Cross-Entropy**: Multi-class classification. Measures divergence from true distribution.\n- **Binary Cross-Entropy**: Binary classification.\n- **Focal Loss**: Handles class imbalance by down-weighting easy examples.\n\n**Other:**\n- **Contrastive/Triplet Loss**: Embedding learning.\n- **KL Divergence**: Distribution matching (VAEs).",
    "example": "import torch.nn as nn\nimport torch.nn.functional as F\n\n# Regression losses\nmse_loss = nn.MSELoss()\nmae_loss = nn.L1Loss()\nhuber_loss = nn.SmoothL1Loss()\n\n# Classification losses\nce_loss = nn.CrossEntropyLoss()  # Combines LogSoftmax + NLLLoss\nbce_loss = nn.BCEWithLogitsLoss()  # Binary, includes sigmoid\n\n# Usage\nlogits = model(x)  # Shape: (batch, num_classes)\ntargets = torch.tensor([0, 2, 1])  # Class indices\n\nloss = ce_loss(logits, targets)\n\n# Weighted loss for imbalanced classes\nweights = torch.tensor([1.0, 2.0, 5.0])  # Higher weight for rare classes\nweighted_ce = nn.CrossEntropyLoss(weight=weights)\n\n# Focal loss (custom implementation)\ndef focal_loss(logits, targets, gamma=2.0, alpha=0.25):\n    ce = F.cross_entropy(logits, targets, reduction='none')\n    pt = torch.exp(-ce)\n    return (alpha * (1 - pt) ** gamma * ce).mean()"
  },
  {
    "id": 26,
    "question": "What is PCA and when would you use it?",
    "answer": "**PCA (Principal Component Analysis)** is a dimensionality reduction technique that finds orthogonal directions (principal components) of maximum variance.\n\n**Use cases:**\n- Reduce features before training (speed, memory)\n- Visualization (project to 2D/3D)\n- Noise reduction\n- Decorrelate features\n- Compression\n\n**Limitations:**\n- Assumes linear relationships\n- Sensitive to feature scaling\n- Principal components may not be interpretable",
    "example": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Always scale before PCA\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Fit PCA\npca = PCA(n_components=0.95)  # Keep 95% variance\nX_reduced = pca.fit_transform(X_scaled)\n\nprint(f\"Original features: {X.shape[1]}\")\nprint(f\"Reduced features: {X_reduced.shape[1]}\")\nprint(f\"Explained variance: {pca.explained_variance_ratio_}\")\n\n# For visualization (2D)\npca_2d = PCA(n_components=2)\nX_2d = pca_2d.fit_transform(X_scaled)\nplt.scatter(X_2d[:, 0], X_2d[:, 1], c=y)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\n\n# Cumulative variance plot\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')"
  },
  {
    "id": 27,
    "question": "What is regularization and how do L1 and L2 differ?",
    "answer": "**Regularization** adds a penalty to the loss function to prevent overfitting by constraining model complexity.\n\n**L2 Regularization (Ridge):**\n- Penalty: λ × Σ(w²)\n- Shrinks weights toward zero but rarely exactly zero\n- Good when all features are potentially relevant\n- Handles multicollinearity\n\n**L1 Regularization (Lasso):**\n- Penalty: λ × Σ|w|\n- Produces sparse solutions (some weights exactly zero)\n- Performs feature selection\n- Good when you expect few relevant features\n\n**Elastic Net:** Combines L1 and L2.",
    "example": "from sklearn.linear_model import Ridge, Lasso, ElasticNet\nimport torch.nn as nn\n\n# Scikit-learn\nridge = Ridge(alpha=1.0)      # L2\nlasso = Lasso(alpha=0.1)      # L1\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Mix\n\n# PyTorch - L2 via weight_decay\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    weight_decay=1e-4  # L2 regularization\n)\n\n# PyTorch - L1 manually\ndef l1_regularization(model, lambda_l1=1e-4):\n    l1_norm = sum(p.abs().sum() for p in model.parameters())\n    return lambda_l1 * l1_norm\n\n# In training loop\nloss = criterion(outputs, targets)\nloss += l1_regularization(model)\nloss.backward()"
  },
  {
    "id": 28,
    "question": "What is hyperparameter tuning and what are common approaches?",
    "answer": "**Hyperparameter tuning** finds optimal model settings that aren't learned during training (learning rate, layers, regularization).\n\n**Approaches:**\n- **Grid Search**: Exhaustive search over specified values. Simple but expensive.\n- **Random Search**: Sample random combinations. Often more efficient than grid.\n- **Bayesian Optimization**: Uses probabilistic model to guide search. Efficient for expensive evaluations.\n- **Hyperband/ASHA**: Early stopping of bad configurations. Efficient resource allocation.\n\n**Best practices:**\n- Use validation set, not test set\n- Start with wide ranges, then narrow\n- Consider computational budget",
    "example": "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport optuna\n\n# Grid Search\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5]\n}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\ngrid_search.fit(X, y)\nprint(grid_search.best_params_)\n\n# Random Search\nfrom scipy.stats import randint\nparam_dist = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(5, 50)\n}\nrandom_search = RandomizedSearchCV(\n    RandomForestClassifier(), param_dist, n_iter=20, cv=5\n)\n\n# Optuna (Bayesian)\ndef objective(trial):\n    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n    depth = trial.suggest_int('max_depth', 3, 20)\n    # Train and return validation score\n    return val_accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)"
  },
  {
    "id": 29,
    "question": "What is the ROC curve and AUC, and when should you use them?",
    "answer": "**ROC (Receiver Operating Characteristic) curve** plots True Positive Rate vs False Positive Rate at various classification thresholds.\n\n**AUC (Area Under Curve):**\n- Measures overall discriminative ability\n- 0.5 = random guessing, 1.0 = perfect\n- Threshold-independent metric\n\n**When to use:**\n- Binary classification evaluation\n- Comparing models regardless of threshold\n- When you'll tune threshold later based on business needs\n\n**Limitations:**\n- Can be misleading with severe class imbalance\n- Doesn't tell you about calibration",
    "example": "from sklearn.metrics import roc_curve, roc_auc_score, auc\nimport matplotlib.pyplot as plt\n\n# Get probabilities\ny_proba = model.predict_proba(X_test)[:, 1]\n\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\n# Or directly\nroc_auc = roc_auc_score(y_test, y_proba)\n\n# Plot\nplt.figure()\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\n\n# Find optimal threshold (Youden's J)\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(f\"Optimal threshold: {optimal_threshold:.3f}\")"
  },
  {
    "id": 30,
    "question": "What is data augmentation and why is it important?",
    "answer": "**Data augmentation** artificially increases training data by applying transformations that preserve labels.\n\n**Benefits:**\n- Reduces overfitting\n- Improves generalization\n- Especially valuable with limited data\n- Teaches invariances to the model\n\n**Common techniques:**\n- **Images**: Flip, rotate, crop, color jitter, cutout, mixup\n- **Text**: Synonym replacement, back-translation, random insertion/deletion\n- **Audio**: Time stretch, pitch shift, noise injection\n- **Tabular**: SMOTE, noise injection",
    "example": "import torchvision.transforms as T\nfrom albumentations import Compose, HorizontalFlip, RandomBrightnessContrast\n\n# PyTorch transforms\ntrain_transform = T.Compose([\n    T.RandomResizedCrop(224),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(brightness=0.2, contrast=0.2),\n    T.RandomRotation(15),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Albumentations (more options)\ntrain_aug = Compose([\n    HorizontalFlip(p=0.5),\n    RandomBrightnessContrast(p=0.2),\n    # CutOut, GridDistortion, etc.\n])\n\n# Text augmentation\nimport nlpaug.augmenter.word as naw\naug = naw.SynonymAug(aug_src='wordnet')\naugmented = aug.augment('The quick brown fox')\n\n# MixUp (interpolate samples)\ndef mixup(x, y, alpha=0.2):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    return mixed_x, y, y[idx], lam"
  },
  {
    "id": 31,
    "question": "What is a GAN and how does it work?",
    "answer": "**GAN (Generative Adversarial Network)** consists of two networks trained in competition:\n\n**Generator (G):**\n- Takes random noise, generates fake samples\n- Goal: Fool the discriminator\n\n**Discriminator (D):**\n- Classifies samples as real or fake\n- Goal: Correctly identify fakes\n\n**Training:**\n- Minimax game: G tries to maximize D's error, D tries to minimize it\n- When balanced, G produces realistic samples\n\n**Challenges:**\n- Mode collapse, training instability\n- Variants: DCGAN, WGAN, StyleGAN address these",
    "example": "import torch\nimport torch.nn as nn\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim=100):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, 784),\n            nn.Tanh()\n        )\n    \n    def forward(self, z):\n        return self.model(z)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(784, 512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Training step\nz = torch.randn(batch_size, latent_dim)\nfake = generator(z)\nd_real = discriminator(real_images)\nd_fake = discriminator(fake.detach())"
  },
  {
    "id": 32,
    "question": "What is model interpretability and what techniques exist?",
    "answer": "**Model interpretability** explains how and why a model makes predictions.\n\n**Intrinsic methods (interpretable by design):**\n- Linear models: Coefficients show feature importance\n- Decision trees: Clear decision rules\n- Attention weights: Show what model focuses on\n\n**Post-hoc methods (explain any model):**\n- **SHAP**: Game-theoretic feature attribution\n- **LIME**: Local linear approximations\n- **Feature importance**: Permutation-based\n- **Grad-CAM**: Visual explanations for CNNs\n- **Integrated Gradients**: Attribution for neural networks",
    "example": "import shap\nimport lime.lime_tabular\nfrom sklearn.inspection import permutation_importance\n\n# SHAP\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)\n\n# LIME\nexplainer = lime.lime_tabular.LimeTabularExplainer(\n    X_train, feature_names=feature_names, class_names=class_names\n)\nexplanation = explainer.explain_instance(X_test[0], model.predict_proba)\nexplanation.show_in_notebook()\n\n# Permutation importance\nperm_importance = permutation_importance(model, X_test, y_test, n_repeats=10)\nfor i in perm_importance.importances_mean.argsort()[::-1]:\n    print(f\"{feature_names[i]}: {perm_importance.importances_mean[i]:.3f}\")\n\n# Grad-CAM for CNNs\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\ncam = GradCAM(model=model, target_layers=[model.layer4[-1]])\ngrayscale_cam = cam(input_tensor=input_tensor)"
  },
  {
    "id": 33,
    "question": "What is reinforcement learning and what are its key components?",
    "answer": "**Reinforcement Learning (RL)** trains agents to make sequential decisions by maximizing cumulative reward through trial and error.\n\n**Key components:**\n- **Agent**: Learner/decision maker\n- **Environment**: What agent interacts with\n- **State (s)**: Current situation\n- **Action (a)**: What agent can do\n- **Reward (r)**: Feedback signal\n- **Policy (π)**: Strategy mapping states to actions\n- **Value function**: Expected cumulative reward\n\n**Types:**\n- **Model-free**: Q-learning, Policy Gradient, PPO\n- **Model-based**: Learn environment dynamics",
    "example": "import gymnasium as gym\nimport numpy as np\n\n# Q-Learning example\nenv = gym.make('FrozenLake-v1')\nQ = np.zeros([env.observation_space.n, env.action_space.n])\n\nalpha = 0.1  # Learning rate\ngamma = 0.99  # Discount factor\nepsilon = 0.1  # Exploration rate\n\nfor episode in range(1000):\n    state, _ = env.reset()\n    done = False\n    \n    while not done:\n        # Epsilon-greedy action selection\n        if np.random.random() < epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(Q[state])\n        \n        next_state, reward, terminated, truncated, _ = env.step(action)\n        done = terminated or truncated\n        \n        # Q-learning update\n        Q[state, action] += alpha * (\n            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n        )\n        state = next_state"
  },
  {
    "id": 34,
    "question": "What is the difference between online and batch learning?",
    "answer": "**Batch Learning (Offline):**\n- Train on entire dataset at once\n- Model is static after training\n- Requires retraining for new data\n- Good when data fits in memory and doesn't change frequently\n\n**Online Learning:**\n- Learn incrementally from streaming data\n- Model updates continuously\n- Can adapt to changing patterns (concept drift)\n- Memory efficient for large/streaming data\n\n**Mini-batch:** Hybrid approach processing small batches sequentially.",
    "example": "from sklearn.linear_model import SGDClassifier\nfrom river import linear_model, preprocessing\n\n# Batch learning\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)  # Train on all data at once\n\n# Online learning with scikit-learn (partial_fit)\nsgd = SGDClassifier()\nfor X_batch, y_batch in data_stream:\n    sgd.partial_fit(X_batch, y_batch, classes=[0, 1])\n\n# Online learning with River library\nmodel = preprocessing.StandardScaler() | linear_model.LogisticRegression()\n\nfor x, y in stream:\n    # Make prediction before learning\n    y_pred = model.predict_one(x)\n    \n    # Update model with new observation\n    model.learn_one(x, y)"
  },
  {
    "id": 35,
    "question": "What is t-SNE and how does it differ from PCA?",
    "answer": "**t-SNE (t-distributed Stochastic Neighbor Embedding)** is a non-linear dimensionality reduction technique for visualization.\n\n**Differences from PCA:**\n- **PCA**: Linear, preserves global structure, variance-based\n- **t-SNE**: Non-linear, preserves local structure, probability-based\n\n**t-SNE characteristics:**\n- Excellent for visualizing clusters\n- Computationally expensive (O(n²))\n- Results vary with perplexity parameter\n- Not suitable for new data projection\n- Distances between clusters not meaningful\n\n**Alternative:** UMAP is faster and preserves more global structure.",
    "example": "from sklearn.manifold import TSNE\nimport umap\nimport matplotlib.pyplot as plt\n\n# t-SNE\ntsne = TSNE(\n    n_components=2,\n    perplexity=30,      # Balance local/global (5-50)\n    learning_rate=200,\n    n_iter=1000,\n    random_state=42\n)\nX_tsne = tsne.fit_transform(X)\n\n# UMAP (faster alternative)\nreducer = umap.UMAP(\n    n_components=2,\n    n_neighbors=15,\n    min_dist=0.1,\n    metric='euclidean'\n)\nX_umap = reducer.fit_transform(X)\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10')\naxes[0].set_title('t-SNE')\naxes[1].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10')\naxes[1].set_title('UMAP')"
  },
  {
    "id": 36,
    "question": "What is feature engineering and what are common techniques?",
    "answer": "**Feature engineering** transforms raw data into features that better represent the underlying problem, improving model performance.\n\n**Common techniques:**\n- **Numerical**: Scaling, binning, log transform, polynomial features\n- **Categorical**: One-hot encoding, target encoding, embedding\n- **Text**: TF-IDF, word embeddings, n-grams\n- **Time**: Cyclical encoding, lag features, rolling statistics\n- **Domain-specific**: Ratios, interactions, aggregations\n\n**Feature selection:**\n- Filter methods (correlation, mutual information)\n- Wrapper methods (RFE)\n- Embedded methods (L1, tree importance)",
    "example": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Numerical transformations\ndf['log_income'] = np.log1p(df['income'])\ndf['age_binned'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 100])\n\n# Cyclical encoding for time\ndf['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\ndf['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n\n# Polynomial features\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Target encoding (mean encoding)\ntarget_mean = df.groupby('category')['target'].mean()\ndf['category_encoded'] = df['category'].map(target_mean)\n\n# Lag features for time series\ndf['sales_lag_1'] = df['sales'].shift(1)\ndf['sales_rolling_7'] = df['sales'].rolling(7).mean()"
  },
  {
    "id": 37,
    "question": "What is the difference between parametric and non-parametric models?",
    "answer": "**Parametric models** have a fixed number of parameters regardless of data size:\n- Examples: Linear regression, logistic regression, neural networks\n- Pros: Faster inference, lower memory, interpretable\n- Cons: Strong assumptions about data distribution, may underfit\n\n**Non-parametric models** have parameters that grow with data:\n- Examples: KNN, decision trees, kernel SVM, Gaussian processes\n- Pros: Flexible, fewer assumptions, can capture complex patterns\n- Cons: Slower inference, memory intensive, can overfit\n\n**Note:** \"Non-parametric\" doesn't mean no parameters—it means the number isn't fixed beforehand.",
    "example": "from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\n# Parametric: Fixed parameters (weights)\nlinear = LinearRegression()  # n_features + 1 parameters\nlinear.fit(X_train, y_train)\nprint(f\"Parameters: {linear.coef_.shape}\")\n\n# Non-parametric: Stores training data\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, y_train)\n# Must store all training points for prediction\n\n# Semi-parametric: Support Vector Regression\n# Number of support vectors depends on data\nsvr = SVR(kernel='rbf')\nsvr.fit(X_train, y_train)\nprint(f\"Support vectors: {svr.support_vectors_.shape}\")"
  },
  {
    "id": 38,
    "question": "What is model quantization and why is it important for deployment?",
    "answer": "**Model quantization** reduces model size and speeds up inference by using lower precision numbers (e.g., INT8 instead of FP32).\n\n**Benefits:**\n- 2-4x smaller model size\n- Faster inference (especially on CPUs/edge devices)\n- Lower memory bandwidth\n- Reduced power consumption\n\n**Types:**\n- **Post-training quantization**: Quantize after training (easy, some accuracy loss)\n- **Quantization-aware training**: Simulate quantization during training (better accuracy)\n- **Dynamic quantization**: Quantize weights, compute activations dynamically",
    "example": "import torch\nfrom torch.quantization import quantize_dynamic, get_default_qconfig\n\n# Dynamic quantization (easiest)\nmodel_quantized = quantize_dynamic(\n    model,\n    {torch.nn.Linear, torch.nn.LSTM},\n    dtype=torch.qint8\n)\n\n# Static quantization (better performance)\nmodel.qconfig = get_default_qconfig('fbgemm')\nmodel_prepared = torch.quantization.prepare(model)\n# Run calibration data through model\nfor data in calibration_loader:\n    model_prepared(data)\nmodel_quantized = torch.quantization.convert(model_prepared)\n\n# Check size reduction\ndef model_size(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    size = os.path.getsize(\"temp.p\") / 1e6\n    os.remove(\"temp.p\")\n    return size\n\nprint(f\"Original: {model_size(model):.2f} MB\")\nprint(f\"Quantized: {model_size(model_quantized):.2f} MB\")"
  },
  {
    "id": 39,
    "question": "What is knowledge distillation?",
    "answer": "**Knowledge distillation** trains a smaller \"student\" model to mimic a larger \"teacher\" model, transferring knowledge while reducing model size.\n\n**How it works:**\n1. Train large teacher model\n2. Generate soft labels (probability distributions) from teacher\n3. Train student on combination of:\n   - Hard labels (ground truth)\n   - Soft labels (teacher predictions)\n\n**Benefits:**\n- Smaller, faster models for deployment\n- Student can exceed training on hard labels alone\n- Soft labels provide richer information (class similarities)\n\n**Temperature parameter** softens probability distribution, revealing more structure.",
    "example": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef distillation_loss(student_logits, teacher_logits, labels, \n                      temperature=4.0, alpha=0.7):\n    \"\"\"\n    Combines soft (distillation) and hard (classification) losses\n    \"\"\"\n    # Soft loss: KL divergence between soft targets\n    soft_targets = F.softmax(teacher_logits / temperature, dim=1)\n    soft_student = F.log_softmax(student_logits / temperature, dim=1)\n    soft_loss = F.kl_div(soft_student, soft_targets, reduction='batchmean')\n    soft_loss = soft_loss * (temperature ** 2)  # Scale by T^2\n    \n    # Hard loss: Standard cross-entropy\n    hard_loss = F.cross_entropy(student_logits, labels)\n    \n    # Combined loss\n    return alpha * soft_loss + (1 - alpha) * hard_loss\n\n# Training loop\nfor x, y in dataloader:\n    with torch.no_grad():\n        teacher_logits = teacher_model(x)\n    \n    student_logits = student_model(x)\n    loss = distillation_loss(student_logits, teacher_logits, y)\n    loss.backward()\n    optimizer.step()"
  },
  {
    "id": 40,
    "question": "What is the difference between content-based and collaborative filtering in recommendation systems?",
    "answer": "**Content-based filtering:**\n- Recommends items similar to what user liked before\n- Uses item features (genre, description, attributes)\n- No cold start for new items (if features known)\n- Can explain recommendations\n- Limited discovery (filter bubble)\n\n**Collaborative filtering:**\n- Recommends based on similar users' preferences\n- Uses user-item interaction matrix\n- Can discover unexpected items\n- Suffers from cold start (new users/items)\n- Types: User-based, Item-based, Matrix factorization\n\n**Hybrid approaches** combine both for better results.",
    "example": "import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse.linalg import svds\n\n# Content-based: Item similarity using features\nitem_features = np.array([...])  # Items x Features\nitem_similarity = cosine_similarity(item_features)\n\ndef content_recommend(item_id, top_n=5):\n    similar_scores = item_similarity[item_id]\n    top_items = similar_scores.argsort()[::-1][1:top_n+1]\n    return top_items\n\n# Collaborative: Matrix Factorization (SVD)\nuser_item_matrix = np.array([...])  # Users x Items\nU, sigma, Vt = svds(user_item_matrix, k=50)\n\n# Reconstruct predictions\nsigma_diag = np.diag(sigma)\npredicted_ratings = np.dot(np.dot(U, sigma_diag), Vt)\n\ndef cf_recommend(user_id, top_n=5):\n    user_predictions = predicted_ratings[user_id]\n    # Filter already rated items\n    already_rated = user_item_matrix[user_id].nonzero()[0]\n    user_predictions[already_rated] = -np.inf\n    return user_predictions.argsort()[::-1][:top_n]"
  },
  {
    "id": 41,
    "question": "What is attention mechanism and how does multi-head attention work?",
    "answer": "**Attention** allows models to focus on relevant parts of input when producing output. Core idea: weighted sum of values based on query-key similarity.\n\n**Scaled Dot-Product Attention:**\nAttention(Q,K,V) = softmax(QK^T / √d_k) × V\n\n**Multi-head attention:**\n- Run attention multiple times in parallel with different learned projections\n- Each head can attend to different aspects\n- Concatenate outputs and project\n\n**Benefits:**\n- Captures different types of relationships\n- More expressive than single attention\n- Foundation of transformer architecture",
    "example": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n        \n        # Linear projections and reshape for multi-head\n        q = self.W_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        k = self.W_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        v = self.W_v(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        \n        # Concatenate and project\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        return self.W_o(context)"
  },
  {
    "id": 42,
    "question": "What is RLHF and how is it used to train LLMs?",
    "answer": "**RLHF (Reinforcement Learning from Human Feedback)** aligns LLMs with human preferences through a multi-stage training process.\n\n**Stages:**\n1. **Supervised Fine-tuning (SFT)**: Fine-tune base model on high-quality demonstrations\n2. **Reward Model Training**: Train a model to predict human preferences from comparison data\n3. **RL Optimization**: Use PPO to optimize policy against reward model while staying close to SFT model (KL penalty)\n\n**Why RLHF:**\n- Hard to specify good behavior in loss function\n- Humans can compare outputs easier than rate absolutely\n- Aligns model with helpfulness, harmlessness, honesty",
    "example": "# Conceptual RLHF pipeline (simplified)\nimport torch\nfrom trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n\n# Stage 1: Start with SFT model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"sft_model\")\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"sft_model\")\n\n# Stage 2: Load trained reward model\nreward_model = load_reward_model(\"reward_model\")\n\n# Stage 3: PPO training\nconfig = PPOConfig(\n    learning_rate=1e-5,\n    batch_size=16,\n    ppo_epochs=4,\n    kl_penalty=\"kl\",  # Stay close to reference\n)\n\nppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)\n\nfor batch in dataloader:\n    # Generate responses\n    response_tensors = ppo_trainer.generate(batch[\"input_ids\"])\n    \n    # Get rewards from reward model\n    rewards = reward_model(response_tensors)\n    \n    # PPO update\n    stats = ppo_trainer.step(batch[\"input_ids\"], response_tensors, rewards)"
  },
  {
    "id": 43,
    "question": "What are diffusion models and how do they generate images?",
    "answer": "**Diffusion models** generate data by learning to reverse a gradual noising process.\n\n**Forward process (fixed):**\n- Gradually add Gaussian noise to data over T steps\n- Eventually becomes pure noise\n\n**Reverse process (learned):**\n- Neural network learns to denoise step by step\n- Start from noise, iteratively remove noise\n- Generates clean samples\n\n**Key concepts:**\n- **Score matching**: Learn gradient of data distribution\n- **Classifier-free guidance**: Condition generation without separate classifier\n\n**Examples:** DALL-E 2, Stable Diffusion, Midjourney",
    "example": "import torch\nimport torch.nn as nn\n\n# Simplified diffusion training step\ndef diffusion_loss(model, x_0, noise_scheduler):\n    # Sample random timestep\n    t = torch.randint(0, noise_scheduler.num_steps, (x_0.shape[0],))\n    \n    # Add noise to get x_t\n    noise = torch.randn_like(x_0)\n    x_t = noise_scheduler.add_noise(x_0, noise, t)\n    \n    # Predict noise\n    noise_pred = model(x_t, t)\n    \n    # MSE loss between predicted and actual noise\n    return nn.functional.mse_loss(noise_pred, noise)\n\n# Sampling (generation)\n@torch.no_grad()\ndef sample(model, noise_scheduler, shape):\n    # Start from pure noise\n    x = torch.randn(shape)\n    \n    # Iteratively denoise\n    for t in reversed(range(noise_scheduler.num_steps)):\n        noise_pred = model(x, t)\n        x = noise_scheduler.step(noise_pred, t, x)\n    \n    return x\n\n# Using diffusers library\nfrom diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\nimage = pipe(\"a photo of an astronaut riding a horse\").images[0]"
  },
  {
    "id": 44,
    "question": "What is the curse of dimensionality?",
    "answer": "**Curse of dimensionality** refers to problems that arise when working with high-dimensional data.\n\n**Effects:**\n- **Sparsity**: Data points become increasingly sparse; volume grows exponentially\n- **Distance concentration**: All points become equidistant, making similarity metrics less meaningful\n- **Overfitting**: Need exponentially more data to maintain density\n- **Computational cost**: Storage and processing scale with dimensions\n\n**Solutions:**\n- Dimensionality reduction (PCA, t-SNE)\n- Feature selection\n- Regularization\n- Domain knowledge to select relevant features",
    "example": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Demonstrate distance concentration\ndef avg_distance_ratio(n_samples=1000, dimensions=[2, 10, 100, 1000]):\n    results = []\n    for d in dimensions:\n        # Random points in d-dimensional unit hypercube\n        points = np.random.rand(n_samples, d)\n        \n        # Calculate pairwise distances\n        from sklearn.metrics import pairwise_distances\n        distances = pairwise_distances(points).flatten()\n        distances = distances[distances > 0]  # Remove self-distances\n        \n        # Ratio of max to min distance\n        ratio = distances.max() / distances.min()\n        results.append((d, ratio, distances.std() / distances.mean()))\n        \n    return results\n\n# As dimensions increase:\n# - Max/min distance ratio approaches 1\n# - Coefficient of variation decreases\n# This means all points are \"equally far\" from each other\n\n# Volume of hypersphere vs hypercube\ndef volume_ratio(d):\n    # Volume of unit hypersphere / Volume of unit hypercube\n    return (np.pi ** (d/2)) / (2**d * np.math.gamma(d/2 + 1))\n\n# Volume ratio approaches 0 as d increases\n# Most volume is in the \"corners\" of high-dimensional space"
  },
  {
    "id": 45,
    "question": "What is MLOps and what are its key components?",
    "answer": "**MLOps** applies DevOps principles to ML systems, enabling reliable and efficient deployment and maintenance of ML models.\n\n**Key components:**\n- **Version Control**: Code, data, models, experiments (DVC, MLflow)\n- **CI/CD**: Automated testing, training, deployment pipelines\n- **Feature Store**: Centralized feature management\n- **Model Registry**: Track model versions and metadata\n- **Monitoring**: Data drift, model performance, system health\n- **Infrastructure**: Scalable training and serving (Kubernetes, cloud services)\n\n**Maturity levels:**\n- Level 0: Manual, ad-hoc\n- Level 1: ML pipeline automation\n- Level 2: CI/CD for ML, continuous training",
    "example": "# MLflow experiment tracking\nimport mlflow\n\nmlflow.set_experiment(\"fraud_detection\")\n\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_params({\n        \"learning_rate\": 0.01,\n        \"n_estimators\": 100\n    })\n    \n    # Train model\n    model.fit(X_train, y_train)\n    \n    # Log metrics\n    mlflow.log_metrics({\n        \"accuracy\": accuracy,\n        \"f1_score\": f1\n    })\n    \n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n    \n    # Register model\n    mlflow.register_model(\n        f\"runs:/{mlflow.active_run().info.run_id}/model\",\n        \"fraud_detector\"\n    )\n\n# DVC for data versioning\n# dvc init\n# dvc add data/training_data.csv\n# git add data/training_data.csv.dvc .gitignore\n# git commit -m \"Add training data\"\n# dvc push  # Push to remote storage"
  },
  {
    "id": 46,
    "question": "What is few-shot and zero-shot learning?",
    "answer": "**Zero-shot learning:**\n- Classify examples from classes never seen during training\n- Uses auxiliary information (descriptions, attributes, embeddings)\n- Example: Classify \"zebra\" using description \"horse with stripes\"\n\n**Few-shot learning:**\n- Learn from very few examples (1-5) per class\n- Techniques: Meta-learning, metric learning, transfer learning\n- Example: Recognize new person from 3 photos\n\n**In LLMs:**\n- Zero-shot: Task description only\n- Few-shot: Task description + examples in prompt (in-context learning)\n- No parameter updates required",
    "example": "# Zero-shot classification with transformers\nfrom transformers import pipeline\n\nclassifier = pipeline(\"zero-shot-classification\")\n\nresult = classifier(\n    \"I love this new smartphone, the camera is amazing!\",\n    candidate_labels=[\"technology\", \"sports\", \"politics\", \"entertainment\"]\n)\n# Output: technology (highest score)\n\n# Few-shot learning with LLM\nprompt = \"\"\"\nClassify the sentiment as positive or negative.\n\nText: \"This movie was fantastic!\"\nSentiment: positive\n\nText: \"Terrible service, never coming back.\"\nSentiment: negative\n\nText: \"The food was delicious and staff were friendly.\"\nSentiment:\"\"\"\n\n# Model completes: \"positive\"\n\n# Meta-learning (Prototypical Networks concept)\nimport torch.nn.functional as F\n\ndef prototypical_loss(support_embeddings, support_labels, query_embeddings, query_labels):\n    # Compute class prototypes (mean of support embeddings per class)\n    classes = torch.unique(support_labels)\n    prototypes = torch.stack([\n        support_embeddings[support_labels == c].mean(0) for c in classes\n    ])\n    \n    # Classify queries by distance to prototypes\n    distances = torch.cdist(query_embeddings, prototypes)\n    log_probs = F.log_softmax(-distances, dim=1)\n    return F.nll_loss(log_probs, query_labels)"
  },
  {
    "id": 47,
    "question": "What is positional encoding in transformers and why is it needed?",
    "answer": "**Positional encoding** adds sequence order information to transformer inputs.\n\n**Why needed:**\n- Self-attention is permutation invariant (treats input as a set)\n- Without position info, \"dog bites man\" = \"man bites dog\"\n- Must explicitly inject position information\n\n**Types:**\n- **Sinusoidal (fixed)**: sin/cos functions of different frequencies. Generalizes to longer sequences.\n- **Learned**: Trainable embedding per position. Often works better but fixed max length.\n- **Relative**: Encode relative positions (e.g., RoPE in LLaMA). Better for varying lengths.\n- **ALiBi**: Attention bias based on distance. Extrapolates well.",
    "example": "import torch\nimport torch.nn as nn\nimport math\n\n# Sinusoidal positional encoding\nclass SinusoidalPE(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# Learned positional encoding\nclass LearnedPE(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        self.pe = nn.Embedding(max_len, d_model)\n    \n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device)\n        return x + self.pe(positions)"
  },
  {
    "id": 48,
    "question": "How do you handle missing data in machine learning?",
    "answer": "**Strategies for missing data:**\n\n**Deletion:**\n- **Listwise**: Remove rows with any missing values (loses data)\n- **Pairwise**: Use available data for each calculation\n\n**Imputation:**\n- **Simple**: Mean, median, mode, constant\n- **Model-based**: KNN, regression, iterative imputation\n- **Multiple imputation**: Create multiple datasets, combine results\n\n**Indicators:**\n- Add binary column indicating missingness\n- Can capture informative missingness patterns\n\n**Algorithm choice:**\n- Some algorithms handle missing data natively (XGBoost, LightGBM)",
    "example": "import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n\n# Check missing data\nprint(df.isnull().sum())\nprint(df.isnull().mean())  # Percentage missing\n\n# Simple imputation\nimputer = SimpleImputer(strategy='median')  # or 'mean', 'most_frequent'\nX_imputed = imputer.fit_transform(X)\n\n# KNN imputation (uses similar samples)\nknn_imputer = KNNImputer(n_neighbors=5)\nX_imputed = knn_imputer.fit_transform(X)\n\n# Iterative imputation (MICE-like)\niterative_imputer = IterativeImputer(max_iter=10, random_state=42)\nX_imputed = iterative_imputer.fit_transform(X)\n\n# Add missingness indicator\ndf['feature_missing'] = df['feature'].isnull().astype(int)\ndf['feature'] = df['feature'].fillna(df['feature'].median())\n\n# XGBoost handles missing natively\nimport xgboost as xgb\nmodel = xgb.XGBClassifier()\nmodel.fit(X_with_nan, y)  # Works directly with NaN"
  },
  {
    "id": 49,
    "question": "What is the difference between encoder-only, decoder-only, and encoder-decoder transformers?",
    "answer": "**Encoder-only (BERT-style):**\n- Bidirectional attention (sees full input)\n- Good for understanding tasks\n- Use cases: Classification, NER, embeddings\n- Examples: BERT, RoBERTa, DeBERTa\n\n**Decoder-only (GPT-style):**\n- Causal/autoregressive attention (only sees past)\n- Good for generation tasks\n- Use cases: Text generation, code completion, chatbots\n- Examples: GPT, LLaMA, Claude\n\n**Encoder-Decoder (T5-style):**\n- Encoder processes input, decoder generates output\n- Good for sequence-to-sequence tasks\n- Use cases: Translation, summarization, Q&A\n- Examples: T5, BART, mT5",
    "example": "from transformers import (\n    BertModel, BertTokenizer,\n    GPT2LMHeadModel, GPT2Tokenizer,\n    T5ForConditionalGeneration, T5Tokenizer\n)\n\n# Encoder-only: BERT for classification\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\ninputs = bert_tokenizer(\"Hello world\", return_tensors=\"pt\")\noutputs = bert_model(**inputs)\nembedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n\n# Decoder-only: GPT for generation\ngpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ngpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\ninputs = gpt_tokenizer(\"Once upon a time\", return_tensors=\"pt\")\noutputs = gpt_model.generate(**inputs, max_length=50)\nprint(gpt_tokenizer.decode(outputs[0]))\n\n# Encoder-Decoder: T5 for translation\nt5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\nt5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\ninputs = t5_tokenizer(\"translate English to French: Hello\", return_tensors=\"pt\")\noutputs = t5_model.generate(**inputs)\nprint(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))"
  },
  {
    "id": 50,
    "question": "What is model calibration and why does it matter?",
    "answer": "**Model calibration** measures how well predicted probabilities match actual frequencies. A calibrated model's 80% confidence predictions should be correct 80% of the time.\n\n**Why it matters:**\n- Critical for decision-making under uncertainty\n- Required for proper risk assessment (medical, financial)\n- Enables meaningful probability thresholds\n- Important for ensembling and cascading models\n\n**Calibration methods:**\n- **Platt scaling**: Fit sigmoid to logits\n- **Temperature scaling**: Single parameter scaling (popular for NNs)\n- **Isotonic regression**: Non-parametric, monotonic mapping\n\n**Measuring calibration:**\n- Reliability diagrams\n- Expected Calibration Error (ECE)",
    "example": "import numpy as np\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nimport matplotlib.pyplot as plt\n\n# Check calibration with reliability diagram\ny_prob = model.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(\n    y_test, y_prob, n_bins=10\n)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Model')\nplt.plot([0, 1], [0, 1], '--', label='Perfectly calibrated')\nplt.xlabel('Mean predicted probability')\nplt.ylabel('Fraction of positives')\nplt.legend()\n\n# Expected Calibration Error\ndef expected_calibration_error(y_true, y_prob, n_bins=10):\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    ece = 0\n    for i in range(n_bins):\n        in_bin = (y_prob >= bin_boundaries[i]) & (y_prob < bin_boundaries[i+1])\n        if in_bin.sum() > 0:\n            accuracy = y_true[in_bin].mean()\n            confidence = y_prob[in_bin].mean()\n            ece += in_bin.sum() * abs(accuracy - confidence)\n    return ece / len(y_true)\n\n# Calibrate model\ncalibrated = CalibratedClassifierCV(model, method='isotonic', cv=5)\ncalibrated.fit(X_train, y_train)"
  }
]
