[
  {
    "id": 1,
    "question": "What is the difference between UX and UI design?",
    "answer": "**UX (User Experience) Design** focuses on the overall feel and functionality of a product:\n- Research and understanding user needs\n- Information architecture and user flows\n- Wireframing and prototyping\n- Usability testing\n- End-to-end journey optimization\n\n**UI (User Interface) Design** focuses on the visual and interactive elements:\n- Visual design (colors, typography, icons)\n- Component design and styling\n- Micro-interactions and animations\n- Responsive layouts\n- Design system implementation\n\n**Relationship**: UX determines WHAT users need and HOW they'll accomplish tasks. UI determines how it LOOKS and FEELS visually.",
    "example": "UX Design Process:\n1. User Research → \"Users struggle to find checkout button\"\n2. Analysis → \"Button placement and flow need improvement\"\n3. Wireframe → Sketch new layout with prominent checkout\n4. Prototype → Interactive mockup for testing\n5. Test → Validate with users\n\nUI Design Process:\n1. Take approved wireframe\n2. Apply visual style → Brand colors, typography\n3. Design button → Size, color, hover states\n4. Add micro-interactions → Button feedback animation\n5. Create specs → Spacing, exact colors, assets\n\nMetaphor: UX is the blueprint of a house (layout, room purposes)\nUI is the interior design (paint colors, furniture style)"
  },
  {
    "id": 2,
    "question": "What is a user persona and how do you create one?",
    "answer": "A **user persona** is a semi-fictional representation of your ideal user based on research and data.\n\n**Components of a persona:**\n- Demographics (name, age, occupation, location)\n- Goals and motivations\n- Pain points and frustrations\n- Behaviors and habits\n- Technical proficiency\n- Quote that captures their mindset\n\n**Creation process:**\n1. Conduct user research (interviews, surveys, analytics)\n2. Identify patterns and segments\n3. Create 3-5 distinct personas\n4. Validate with stakeholders and real users\n5. Keep personas living documents\n\n**Common mistakes:**\n- Basing personas on assumptions, not data\n- Creating too many personas\n- Making them too generic or too specific",
    "example": "Example Persona:\n\n**Name**: Sarah Chen\n**Age**: 34 | **Role**: Marketing Manager | **Location**: Austin, TX\n\n**Goals**:\n- Streamline campaign reporting\n- Prove ROI to leadership\n- Save time on repetitive tasks\n\n**Pain Points**:\n- Switching between 5+ tools daily\n- Manual data compilation takes hours\n- Can't get real-time performance data\n\n**Behaviors**:\n- Checks metrics first thing every morning\n- Prefers visual dashboards over spreadsheets\n- Often works remotely\n\n**Tech Comfort**: High - early adopter of new tools\n\n**Quote**: \"I spend more time gathering data than acting on it.\"\n\n**Scenario**: Sarah needs to present weekly campaign results.\nCurrent: 2 hours pulling data from multiple sources.\nDesired: One dashboard with automated reporting."
  },
  {
    "id": 3,
    "question": "What is the double diamond design process?",
    "answer": "The **Double Diamond** is a design process model developed by the British Design Council, consisting of four phases in two diamonds.\n\n**First Diamond - Problem Space:**\n\n1. **Discover** (Diverge): Research and explore the problem space\n   - User interviews, observation, data analysis\n   - Understand context and needs\n\n2. **Define** (Converge): Synthesize findings into clear problem statement\n   - Identify patterns and insights\n   - Create \"How Might We\" statements\n\n**Second Diamond - Solution Space:**\n\n3. **Develop** (Diverge): Generate multiple solutions\n   - Ideation, brainstorming, sketching\n   - Prototyping and experimentation\n\n4. **Deliver** (Converge): Refine and implement solution\n   - Testing and iteration\n   - Final design and handoff\n\n**Key principle**: Diverge before converging - explore broadly, then focus.",
    "example": "Project: Improve employee onboarding experience\n\n**DISCOVER (Weeks 1-2)**\n- Interview 15 recent hires\n- Shadow HR during onboarding sessions\n- Analyze onboarding completion metrics\n- Review competitor onboarding flows\n\n**DEFINE (Week 3)**\n- Affinity mapping of research findings\n- Key insight: \"New hires feel overwhelmed by information dump on day 1\"\n- Problem statement: \"How might we help new hires absorb information at their own pace?\"\n\n**DEVELOP (Weeks 4-6)**\n- Brainstorm 30+ solution ideas\n- Sketch concepts: drip email series, interactive checklist, buddy system app\n- Create 3 prototypes\n- Test with 5 recent hires each\n\n**DELIVER (Weeks 7-8)**\n- Refine winning concept (progressive onboarding checklist)\n- High-fidelity designs\n- Developer handoff with specs\n- Launch plan and success metrics"
  },
  {
    "id": 4,
    "question": "What is a Minimum Viable Product (MVP)?",
    "answer": "An **MVP** is the simplest version of a product that delivers core value to users while allowing you to learn and validate assumptions with minimal resources.\n\n**Purpose:**\n- Test product-market fit quickly\n- Gather real user feedback early\n- Reduce wasted development effort\n- Learn what users actually need vs. assume\n\n**Characteristics of a good MVP:**\n- Solves one core problem well\n- Usable and functional (not broken)\n- Provides value to early adopters\n- Includes metrics/feedback mechanisms\n\n**Common mistakes:**\n- Making it too minimal (no value)\n- Making it too featured (not \"minimum\")\n- Not defining success metrics\n- Treating it as the final product\n\n**MVP is NOT**: A prototype, a beta, or a low-quality product.",
    "example": "Example: Building a food delivery app\n\n**Full Vision**: App with restaurants, drivers, real-time tracking, payments, ratings, loyalty program, scheduled orders\n\n**MVP Approach**:\n\nStep 1 - Identify core assumption to test:\n\"People in this area will pay for restaurant delivery\"\n\nStep 2 - Simplest way to test:\n- Landing page with menu from 3 local restaurants\n- Phone/text ordering\n- Founder personally delivers orders\n- Payment via Venmo/cash\n\nStep 3 - Success metrics:\n- 50 orders in first month\n- 30% repeat customers\n- Average order value > $25\n\nStep 4 - Learn and iterate:\n- If validated → Build actual app\n- If not → Pivot (different area, cuisine type, etc.)\n\nThis MVP tests demand with ~$500 (landing page + ads)\ninstead of $50,000+ for full app development."
  },
  {
    "id": 5,
    "question": "What is the difference between qualitative and quantitative research?",
    "answer": "**Quantitative Research** answers \"What\" and \"How much\":\n- Numerical data and statistics\n- Large sample sizes\n- Structured methods\n- Measurable and comparable\n- Examples: surveys, A/B tests, analytics\n\n**Qualitative Research** answers \"Why\" and \"How\":\n- Descriptive insights and context\n- Smaller sample sizes (5-15 typically sufficient)\n- Open-ended exploration\n- Rich, nuanced understanding\n- Examples: interviews, usability tests, ethnography\n\n**When to use each:**\n- Qualitative: Early exploration, understanding motivations, generating hypotheses\n- Quantitative: Validating hypotheses, measuring impact, tracking trends\n\n**Best practice**: Combine both - qualitative to discover, quantitative to validate.",
    "example": "Scenario: Users are abandoning checkout\n\n**Quantitative data tells us:**\n- 67% cart abandonment rate\n- 45% drop off at shipping info step\n- Mobile abandonment 20% higher than desktop\n- Average cart value at abandonment: $85\n\n**Qualitative research reveals WHY:**\n\nUser interview quotes:\n- \"I couldn't find how much shipping would cost\"\n- \"I had to create an account, I just wanted to buy one thing\"\n- \"The form was impossible on my phone\"\n- \"I wasn't sure if my payment info was secure\"\n\n**Combined insight:**\nQuantitative: 45% drop at shipping step\nQualitative: Users want shipping costs upfront\n\n**Action**: Show estimated shipping cost on product page\n\n**Validation**: A/B test shows 15% reduction in abandonment\n\nWithout qualitative: You know WHAT but not WHY\nWithout quantitative: You know WHY but not HOW BIG the problem is"
  },
  {
    "id": 6,
    "question": "What is a design system and why is it important?",
    "answer": "A **design system** is a collection of reusable components, guidelines, and standards that ensure consistency and efficiency across products.\n\n**Components of a design system:**\n- **Foundations**: Colors, typography, spacing, grid, icons\n- **Components**: Buttons, forms, cards, navigation, modals\n- **Patterns**: Common UI patterns and layouts\n- **Guidelines**: Usage rules, accessibility, voice and tone\n- **Documentation**: How and when to use each element\n\n**Benefits:**\n- Consistency across products and teams\n- Faster design and development\n- Easier maintenance and updates\n- Better accessibility compliance\n- Smoother designer-developer handoff\n- Scalability as teams grow\n\n**Examples**: Material Design (Google), Human Interface Guidelines (Apple), Polaris (Shopify), Carbon (IBM)",
    "example": "Design System Structure:\n\n**1. Design Tokens (Foundations)**\n$color-primary: #0066CC\n$color-error: #DC3545\n$spacing-sm: 8px\n$spacing-md: 16px\n$font-size-body: 16px\n$border-radius: 4px\n\n**2. Component: Button**\nVariants: Primary, Secondary, Tertiary, Danger\nStates: Default, Hover, Active, Disabled, Loading\nSizes: Small (32px), Medium (40px), Large (48px)\n\nUsage guidelines:\n- Use Primary for main CTA (one per screen)\n- Use Danger only for destructive actions\n- Always include accessible label\n- Minimum touch target: 44x44px\n\n**3. Pattern: Form Validation**\n- Show inline errors below field\n- Use error color for border and text\n- Include icon + text for accessibility\n- Don't disable submit, show errors on attempt\n\n**4. Documentation**\nEach component includes:\n- Visual examples\n- Code snippets\n- Do's and Don'ts\n- Accessibility requirements"
  },
  {
    "id": 7,
    "question": "How do you prioritize features using the RICE framework?",
    "answer": "**RICE** is a prioritization framework that scores features based on four factors:\n\n**R - Reach**: How many users will this affect in a given time period?\n- Measured in users/quarter or events/month\n\n**I - Impact**: How much will this move the needle for users who experience it?\n- Score: 3 (massive), 2 (high), 1 (medium), 0.5 (low), 0.25 (minimal)\n\n**C - Confidence**: How confident are you in your estimates?\n- Percentage: 100% (high), 80% (medium), 50% (low)\n\n**E - Effort**: How many person-months will this take?\n- Includes design, development, QA, launch\n\n**Formula**: RICE Score = (Reach × Impact × Confidence) / Effort\n\n**Higher score = Higher priority**",
    "example": "Comparing two feature ideas:\n\n**Feature A: One-click reorder**\nReach: 5,000 users/quarter (returning customers)\nImpact: 2 (high - significantly faster checkout)\nConfidence: 80% (have customer feedback)\nEffort: 2 person-months\n\nRICE = (5000 × 2 × 0.8) / 2 = 4,000\n\n**Feature B: Social sharing**\nReach: 20,000 users/quarter (all users see it)\nImpact: 0.5 (low - nice to have)\nConfidence: 50% (assumption, no validation)\nEffort: 1 person-month\n\nRICE = (20000 × 0.5 × 0.5) / 1 = 5,000\n\n**Feature C: Accessibility improvements**\nReach: 500 users/quarter (users with disabilities)\nImpact: 3 (massive - enables usage)\nConfidence: 100% (legal requirement + right thing)\nEffort: 3 person-months\n\nRICE = (500 × 3 × 1.0) / 3 = 500\n\nPure RICE ranking: B > A > C\n\nBut consider: C might be prioritized anyway due to\nlegal requirements and company values (RICE is input, not final decision)"
  },
  {
    "id": 8,
    "question": "What are the key principles of accessibility (a11y) in design?",
    "answer": "**Web Content Accessibility Guidelines (WCAG)** define four principles (POUR):\n\n**Perceivable**: Information must be presentable in ways users can perceive\n- Text alternatives for images\n- Captions for videos\n- Sufficient color contrast (4.5:1 for text)\n- Don't rely on color alone\n\n**Operable**: Interface must be navigable and usable\n- Keyboard accessible\n- Enough time to read/interact\n- No seizure-triggering content\n- Clear navigation and focus indicators\n\n**Understandable**: Content and interface must be understandable\n- Readable text\n- Predictable behavior\n- Input assistance and error prevention\n\n**Robust**: Content must work with current and future technologies\n- Valid HTML\n- Compatible with assistive technologies\n- Semantic markup",
    "example": "Accessibility Checklist for Designers:\n\n**Color & Contrast**\n✓ Text contrast ratio ≥ 4.5:1 (normal text)\n✓ Large text contrast ratio ≥ 3:1\n✓ Don't convey info by color alone\n✓ Focus states visible (not just color change)\n\n**Typography**\n✓ Base font size ≥ 16px\n✓ Line height ≥ 1.5\n✓ Resizable up to 200% without breaking\n\n**Interactive Elements**\n✓ Touch targets ≥ 44x44px\n✓ Clear hover and focus states\n✓ Logical tab order\n✓ Skip links for navigation\n\n**Forms**\n✓ Labels for all inputs (not placeholder only)\n✓ Error messages are specific and helpful\n✓ Required fields clearly marked\n✓ Group related fields\n\n**Images & Media**\n✓ Alt text for informative images\n✓ Decorative images marked (alt=\"\")\n✓ Captions/transcripts for video/audio\n\n**Testing**\n- Use screen reader (VoiceOver, NVDA)\n- Navigate with keyboard only\n- Test with color blindness simulators\n- Use automated tools (axe, WAVE)"
  },
  {
    "id": 9,
    "question": "What is a product roadmap and how do you create one?",
    "answer": "A **product roadmap** is a strategic document that communicates the vision, direction, and planned evolution of a product over time.\n\n**Types of roadmaps:**\n- **Timeline-based**: Features mapped to dates/quarters\n- **Theme-based**: Grouped by strategic themes or goals\n- **Now-Next-Later**: Prioritized without specific dates\n- **Outcome-based**: Focused on metrics, not features\n\n**Key components:**\n- Vision and strategic goals\n- Themes or initiatives\n- Features or epics\n- Timeline or priority\n- Dependencies and risks\n\n**Best practices:**\n- Align with company strategy\n- Include stakeholder input\n- Stay flexible (roadmap ≠ commitment)\n- Communicate appropriate detail per audience\n- Review and update regularly",
    "example": "Now-Next-Later Roadmap Example:\n\n**Product Vision**: Make expense reporting effortless\n\n**NOW (Current Quarter)**\nTheme: Core Reliability\n- Receipt auto-scanning accuracy improvements\n- Fix sync issues with accounting software\n- Mobile app performance optimization\nGoal: Reduce support tickets by 30%\n\n**NEXT (Next Quarter)**\nTheme: Smart Automation\n- Auto-categorization of expenses\n- Policy violation warnings\n- Recurring expense detection\nGoal: Reduce manual entry by 50%\n\n**LATER (Future)**\nTheme: Insights & Intelligence\n- Spending analytics dashboard\n- Budget forecasting\n- Anomaly detection\nGoal: Enable proactive financial decisions\n\n**Audience-specific versions:**\n- Executives: Vision + themes + outcomes\n- Engineering: Detailed features + dependencies\n- Sales: Customer-facing features + timelines\n- Customers: High-level themes + value props"
  },
  {
    "id": 10,
    "question": "What is usability testing and how do you conduct it?",
    "answer": "**Usability testing** is a research method where you observe real users attempting to complete tasks with your product to identify issues and improvement opportunities.\n\n**Types:**\n- **Moderated**: Facilitator guides session, can probe deeper\n- **Unmoderated**: User completes tasks independently (remote tools)\n- **Formative**: During design to inform decisions\n- **Summative**: After launch to measure success\n\n**Planning a test:**\n1. Define objectives and research questions\n2. Identify tasks to test (realistic scenarios)\n3. Recruit representative users (5-8 typically sufficient)\n4. Prepare prototype/product and script\n5. Conduct sessions (think-aloud protocol)\n6. Analyze findings and prioritize issues\n\n**Key principle**: 5 users find ~85% of usability problems (Nielsen).",
    "example": "Usability Test Plan:\n\n**Objective**: Evaluate new checkout flow before launch\n\n**Research Questions**:\n- Can users complete purchase without assistance?\n- Where do users hesitate or show confusion?\n- Is shipping cost presentation clear?\n\n**Participants**: 6 users matching target persona\n- Mix of mobile and desktop\n- Some new, some returning customers\n\n**Tasks**:\n1. \"You want to buy a blue medium t-shirt. Please complete the purchase.\"\n2. \"You changed your mind and want to use a different shipping address.\"\n3. \"You have a promo code SAVE20. Apply it to your order.\"\n\n**Script**:\n- Intro: Explain think-aloud, we're testing the design not you\n- Tasks: Read scenario, observe, don't help\n- Probing: \"What were you expecting?\" \"What would you do next?\"\n- Wrap-up: Overall impressions, suggestions\n\n**Analysis**:\n- Severity rating: Critical / Major / Minor\n- Task success rate: Complete / Partial / Fail\n- Time on task\n- User quotes and observations"
  },
  {
    "id": 11,
    "question": "What are the key product metrics and how do you choose them?",
    "answer": "**Product metrics** measure how well your product delivers value. Choose metrics aligned with business goals and user outcomes.\n\n**Common metric categories:**\n\n**Acquisition**: How users find you\n- Traffic, signups, cost per acquisition (CPA)\n\n**Activation**: First value experience\n- Onboarding completion, time to value, aha moment\n\n**Engagement**: Ongoing usage\n- DAU/MAU, session length, feature adoption\n\n**Retention**: Users coming back\n- Day 1/7/30 retention, churn rate\n\n**Revenue**: Business value\n- ARPU, LTV, conversion rate\n\n**Framework - North Star Metric:**\n- Single metric that best captures core value\n- Leading indicator of long-term success\n- Examples: Airbnb = Nights booked, Spotify = Time listening",
    "example": "Metric Selection for a Fitness App:\n\n**Business Goal**: Increase paid subscriptions\n\n**North Star Metric**: Weekly Active Exercisers\n(Users who complete ≥3 workouts per week)\n\nRationale: Correlates with subscription conversion and retention\n\n**Supporting Metrics (Input Metrics):**\n\nAcquisition:\n- App downloads\n- Free trial starts\n\nActivation:\n- First workout completed (within 24 hours)\n- Profile setup completion\n\nEngagement:\n- Workouts per user per week\n- Workout completion rate\n- Feature usage (plans, tracking, social)\n\nRetention:\n- Week 1, 4, 12 retention\n- Subscription renewal rate\n\nRevenue:\n- Free to paid conversion rate\n- Monthly recurring revenue (MRR)\n- Average revenue per user (ARPU)\n\n**Counter Metrics** (ensure quality):\n- App crashes/errors\n- Customer support tickets\n- Reported injuries"
  },
  {
    "id": 12,
    "question": "What is A/B testing and when should you use it?",
    "answer": "**A/B testing** (split testing) compares two versions of something to determine which performs better based on statistical analysis.\n\n**How it works:**\n1. Create hypothesis: \"Changing X will improve Y\"\n2. Split traffic randomly between control (A) and variant (B)\n3. Run until statistically significant\n4. Analyze results and implement winner\n\n**When to use A/B testing:**\n- Optimizing conversion rates\n- Validating design decisions with data\n- Reducing risk of major changes\n- Settling internal debates objectively\n\n**When NOT to use:**\n- Low traffic (won't reach significance)\n- Testing completely new products (use qualitative)\n- Obvious usability issues (just fix them)\n- When speed matters more than optimization\n\n**Statistical considerations:**\n- Sample size (use calculators)\n- Statistical significance (typically 95%)\n- Test duration (avoid peeking)",
    "example": "A/B Test Example:\n\n**Hypothesis**: Changing CTA from \"Sign Up\" to \"Start Free Trial\" will increase conversion because it emphasizes value and low commitment.\n\n**Setup**:\n- Control (A): \"Sign Up\" button\n- Variant (B): \"Start Free Trial\" button\n- Metric: Click-through rate to registration\n- Traffic split: 50/50\n- Sample size needed: 10,000 per variant\n- Expected duration: 2 weeks\n\n**Results**:\nControl: 3.2% CTR (320/10,000)\nVariant: 4.1% CTR (410/10,000)\nLift: +28%\nStatistical significance: 98%\n\n**Decision**: Implement variant\n\n**Common Mistakes to Avoid**:\n- Stopping test early when results look good\n- Testing too many variables at once\n- Not considering segment differences\n- Ignoring secondary metrics (did bounce rate increase?)\n- Testing during atypical periods (holidays)"
  },
  {
    "id": 13,
    "question": "What is information architecture and how do you approach it?",
    "answer": "**Information Architecture (IA)** is the structural design of information environments - how content is organized, labeled, and connected.\n\n**Key components:**\n- **Organization**: How content is grouped and categorized\n- **Labeling**: What things are called\n- **Navigation**: How users move through content\n- **Search**: How users find specific content\n\n**IA methods:**\n- **Card sorting**: Users group content (discover mental models)\n- **Tree testing**: Users find items in proposed structure\n- **Content audit**: Inventory existing content\n- **Sitemap**: Visual hierarchy of pages/sections\n\n**Principles:**\n- Match user mental models\n- Use clear, consistent labeling\n- Keep hierarchy shallow (3 clicks rule is myth, but depth matters)\n- Support multiple navigation paths",
    "example": "IA Process for E-commerce Redesign:\n\n**1. Content Audit**\n- Inventory: 500 products, 50 categories, 20 content pages\n- Issues: Overlapping categories, inconsistent naming\n\n**2. User Research**\nCard sort with 15 users:\n- Users grouped \"Laptops\" and \"Desktops\" under \"Computers\"\n- \"Accessories\" was too vague - split by device type\n- Users expected \"Deals\" in main nav\n\n**3. Proposed Structure**\nComputers\n├── Laptops\n├── Desktops\n└── Monitors\nPhones & Tablets\n├── Smartphones\n├── Tablets\n└── Phone Accessories\nHome & Audio\n├── Speakers\n├── Headphones\n└── Smart Home\nDeals & Offers\n\n**4. Tree Testing**\nTask: \"Find wireless earbuds\"\nResult: 85% success rate via Home & Audio > Headphones\n(previously 45% when buried under \"Accessories\")\n\n**5. Implementation**\n- Updated navigation\n- Breadcrumbs for orientation\n- Filters for cross-category browsing"
  },
  {
    "id": 14,
    "question": "What is the Jobs-to-be-Done (JTBD) framework?",
    "answer": "**Jobs-to-be-Done** is a framework for understanding customer needs based on the \"job\" they're trying to accomplish, not the product they're using.\n\n**Core concept**: People \"hire\" products to do a job for them. Understanding the job leads to better product decisions.\n\n**Job statement format:**\n\"When [situation], I want to [motivation], so I can [expected outcome].\"\n\n**Job types:**\n- **Functional**: Practical task (get from A to B)\n- **Emotional**: How they want to feel (feel confident)\n- **Social**: How they want to be perceived (look successful)\n\n**Benefits:**\n- Focus on outcomes, not features\n- Identify unexpected competitors\n- Find innovation opportunities\n- Create more compelling positioning\n\n**Key insight**: Your competition isn't just similar products - it's anything that accomplishes the same job.",
    "example": "JTBD Analysis: Morning Coffee\n\n**Surface-level view**: Customers want coffee\nCompetitors: Starbucks, Dunkin, home brewing\n\n**JTBD view**: What job is coffee hired to do?\n\nJob 1: \"Help me wake up and start my day\"\n- Competitors: Energy drinks, tea, exercise, cold shower\n- Opportunity: Faster caffeine delivery, routine simplification\n\nJob 2: \"Give me a productive break\"\n- Competitors: Snack, social media, walk outside\n- Opportunity: Combine break with mental reset\n\nJob 3: \"Make me feel like I'm treating myself\"\n- Competitors: Pastry, new app, small purchase\n- Opportunity: Premium experience, personalization\n\nJob 4: \"Give me a reason to meet a friend\"\n- Competitors: Lunch, drinks, phone call\n- Opportunity: Social atmosphere, meeting-friendly space\n\n**Product implications**:\nA coffee shop isn't competing just with other coffee shops.\nFor job #4, the competition is any social venue.\nThis changes how you design the space, pricing, and experience."
  },
  {
    "id": 15,
    "question": "How do you write effective user stories?",
    "answer": "**User stories** are short descriptions of features from the user's perspective, used to capture requirements in agile development.\n\n**Standard format:**\n\"As a [user type], I want [goal], so that [benefit/reason].\"\n\n**INVEST criteria for good stories:**\n- **Independent**: Can be developed separately\n- **Negotiable**: Details can be discussed\n- **Valuable**: Delivers user/business value\n- **Estimable**: Team can estimate effort\n- **Small**: Fits in a sprint\n- **Testable**: Clear acceptance criteria\n\n**Acceptance criteria** define \"done\":\n- Given [context], When [action], Then [outcome]\n- Specific, measurable conditions\n\n**Anti-patterns to avoid:**\n- Technical implementation details\n- Too large (epics disguised as stories)\n- Missing the \"so that\" (no clear value)",
    "example": "User Story Example:\n\n**Epic**: Order Management\n\n**User Story**:\nAs a returning customer,\nI want to reorder my previous purchases with one click,\nso that I can save time on products I buy regularly.\n\n**Acceptance Criteria**:\n1. Given I have previous orders,\n   When I view my order history,\n   Then I see a \"Reorder\" button on each past order\n\n2. Given I click \"Reorder\" on a past order,\n   When the items are available,\n   Then all items are added to my cart with same quantities\n\n3. Given I click \"Reorder\" on a past order,\n   When an item is unavailable,\n   Then I see a message indicating which items couldn't be added\n\n4. Given I'm on mobile,\n   When I use the reorder feature,\n   Then the experience works the same as desktop\n\n**Story Points**: 5\n**Priority**: High (based on customer research showing this is top request)\n\n**NOT a good user story**:\n\"Create API endpoint for reorder functionality\"\n(This is a technical task, not user-focused)"
  },
  {
    "id": 16,
    "question": "What is design thinking and its five stages?",
    "answer": "**Design thinking** is a human-centered problem-solving approach that emphasizes empathy, experimentation, and iteration.\n\n**Five stages (Stanford d.school model):**\n\n1. **Empathize**: Understand users deeply\n   - Observe, engage, immerse\n   - Set aside assumptions\n\n2. **Define**: Synthesize into clear problem statement\n   - Point of view (POV) statement\n   - \"How Might We\" questions\n\n3. **Ideate**: Generate many possible solutions\n   - Brainstorming, quantity over quality\n   - Build on others' ideas\n\n4. **Prototype**: Create quick, cheap representations\n   - Fail fast, learn fast\n   - Just enough fidelity to test\n\n5. **Test**: Get feedback from users\n   - Observe, don't defend\n   - Iterate based on learnings\n\n**Key principles:**\n- Non-linear (iterate between stages)\n- Bias toward action\n- Embrace ambiguity\n- Learn from failure",
    "example": "Design Thinking Workshop: Hospital Waiting Room\n\n**EMPATHIZE (2 hours)**\n- Observed patients in waiting room\n- Interviewed 8 patients and 3 staff\n- Shadowed a patient through their visit\n\nKey observations:\n- Uncertainty about wait time causes anxiety\n- People afraid to leave for bathroom/food\n- No good place to charge devices\n\n**DEFINE (1 hour)**\nPOV: \"Anxious patients need to feel informed and in control while waiting because uncertainty makes stressful situations worse.\"\n\nHMW: \"How might we make wait time feel shorter and less stressful?\"\n\n**IDEATE (1 hour)**\n- Real-time queue display\n- Text notification system\n- Comfortable zones (work, rest, kids)\n- Waiting room concierge\n- Virtual queue (wait elsewhere)\n\n**PROTOTYPE (2 hours)**\n- Paper mockup of queue display\n- Storyboard of text notification flow\n- Cardboard layout of zoned waiting area\n\n**TEST (2 hours)**\n- Showed prototypes to 5 patients\n- Text notification was most valued\n- Queue display caused new anxiety (seeing others skip ahead)\n- Zones less important than info clarity"
  },
  {
    "id": 17,
    "question": "What is the difference between a wireframe, mockup, and prototype?",
    "answer": "These represent different fidelity levels in the design process:\n\n**Wireframe (Low fidelity)**\n- Basic layout and structure\n- No colors, images, or styling\n- Focus on information hierarchy and flow\n- Quick to create and modify\n- Tools: Paper, Balsamiq, basic Figma\n\n**Mockup (High fidelity)**\n- Visual design applied\n- Actual colors, typography, images\n- Looks like final product\n- Static - no interactions\n- Tools: Figma, Sketch, Adobe XD\n\n**Prototype (Interactive)**\n- Clickable/tappable\n- Simulates real interactions\n- Can range from low to high fidelity\n- Used for testing and stakeholder demos\n- Tools: Figma, Principle, Framer, code\n\n**When to use each:**\n- Wireframe: Early exploration, layout decisions\n- Mockup: Visual direction approval, developer handoff\n- Prototype: Usability testing, complex interaction design",
    "example": "Design Process Flow:\n\n**Phase 1: Wireframes**\n[Header: Logo | Nav | Search]\n[Hero: □ Image | Text Text Text ]\n[Products: □  □  □  □ ]\n[Footer: Links | Links | Links]\n\nPurpose: \"Does this layout make sense?\"\nTime: 30 minutes\nFeedback: Layout approved, add filters\n\n**Phase 2: Mockup**\n- Brand colors applied (#0066CC primary)\n- Actual product images\n- Typography (Inter, 16px body)\n- Proper spacing (8px grid)\n- Icons and visual elements\n\nPurpose: \"Does this look right?\"\nTime: 4-8 hours\nFeedback: Visual direction approved\n\n**Phase 3: Prototype**\n- Click \"Add to Cart\" → Cart slides out\n- Hover on product → Quick view appears  \n- Filter selection → Products animate/filter\n- Form validation → Error states shown\n\nPurpose: \"Does this feel right to use?\"\nTime: 2-4 hours (using mockup)\nFeedback: Cart interaction confusing → iterate\n\n**Fidelity vs. Speed tradeoff:**\nLow fidelity = Faster iteration, less attachment\nHigh fidelity = Better user feedback, more investment"
  },
  {
    "id": 18,
    "question": "How do you handle stakeholder disagreements on design decisions?",
    "answer": "**Strategies for navigating design disagreements:**\n\n**1. Understand the concern**\n- Listen fully before responding\n- Ask \"What problem are you trying to solve?\"\n- Often surface-level disagreements hide deeper concerns\n\n**2. Ground in evidence**\n- User research and data\n- Industry best practices\n- Competitive analysis\n- Established design principles\n\n**3. Separate opinion from expertise**\n- Design decisions: Designer leads\n- Business constraints: Stakeholder input valuable\n- User needs: Data decides\n\n**4. Offer alternatives**\n- \"I hear your concern. Here are three options...\"\n- Show tradeoffs clearly\n- Let them choose within good options\n\n**5. Propose testing**\n- \"Let's test both approaches with users\"\n- Takes emotion out, adds objectivity\n\n**6. Document and escalate when needed**\n- Note concerns and decisions\n- Escalate only when necessary",
    "example": "Scenario: VP wants hero image changed to stock photo\n\n**Bad response**: \"That's a bad idea, stock photos look generic.\"\n\n**Good approach**:\n\n1. **Understand**: \"Can you tell me more about what you're hoping to achieve?\"\nVP: \"Current image doesn't show our product clearly enough.\"\n\n2. **Acknowledge**: \"That's a valid concern. Product clarity is important.\"\n\n3. **Share data**: \"In our testing, users found the current lifestyle imagery more engaging. They spent 40% more time on page vs. product-only shots.\"\n\n4. **Offer options**:\n\"Here are three approaches:\nA) Keep lifestyle image, add product overlay\nB) Split test lifestyle vs. product-focused image\nC) Hybrid - lifestyle hero, product detail below fold\"\n\n5. **Recommend**: \"I recommend option B - we get data to make a confident decision. If the test shows product focus wins, we'll change it.\"\n\n6. **Document**: Email summary of discussion and agreed approach.\n\n**If overruled despite evidence**:\n- Document your recommendation and reasoning\n- Implement their decision professionally\n- Set up metrics to evaluate\n- Revisit after data is available"
  },
  {
    "id": 19,
    "question": "What is product-market fit and how do you measure it?",
    "answer": "**Product-market fit** is when your product satisfies strong market demand - you've built something people want.\n\n**Signs of product-market fit:**\n- Users actively seeking your product\n- High retention and engagement\n- Organic growth and referrals\n- Users would be disappointed if product disappeared\n- Scaling challenges (good problem to have)\n\n**Measurement approaches:**\n\n**Sean Ellis Test** (survey):\n\"How would you feel if you could no longer use this product?\"\n- Very disappointed: ≥40% = strong PMF\n- Somewhat disappointed\n- Not disappointed\n\n**Retention curves**: Flatten rather than decline to zero\n\n**NPS (Net Promoter Score)**: >50 indicates strong PMF\n\n**Organic growth**: Significant portion from referrals\n\n**Engagement depth**: Core features heavily used",
    "example": "PMF Assessment Dashboard:\n\n**Sean Ellis Survey Results (n=500)**\n- Very disappointed if gone: 52% ✓ (target: ≥40%)\n- Somewhat disappointed: 31%\n- Not disappointed: 17%\nVerdict: Strong PMF signal\n\n**Retention Cohort Analysis**\nMonth 1 retention: 45%\nMonth 3 retention: 32%\nMonth 6 retention: 28%\nMonth 12 retention: 25%\nVerdict: Curve flattening ✓\n\n**Growth Metrics**\n- Organic signups: 35% of total\n- Referral rate: 22% of users invited someone\n- CAC payback: 8 months\nVerdict: Healthy organic growth ✓\n\n**Engagement**\n- DAU/MAU ratio: 45% (high engagement)\n- Core feature usage: 78% use main feature weekly\nVerdict: Strong engagement ✓\n\n**No PMF Warning Signs**:\n- Need to convince users of value\n- High acquisition, low retention\n- Users require heavy support\n- No organic/referral growth\n- Customers say \"nice to have\" not \"must have\""
  },
  {
    "id": 20,
    "question": "What is the role of design critiques and how do you give effective feedback?",
    "answer": "**Design critique** is a structured feedback session to improve design work through collaborative evaluation.\n\n**Purpose:**\n- Improve design quality\n- Ensure alignment with goals\n- Share knowledge across team\n- Catch issues early\n- Develop designers' skills\n\n**Critique structure:**\n1. Designer presents context and goals\n2. Reviewers ask clarifying questions\n3. Focused feedback on specific aspects\n4. Discussion of alternatives\n5. Clear next steps\n\n**Giving effective feedback:**\n- Be specific, not vague\n- Focus on design, not designer\n- Reference objectives and users\n- Offer alternatives, not just criticism\n- Prioritize feedback by impact\n\n**Receiving feedback:**\n- Listen without defending\n- Ask clarifying questions\n- Take notes\n- Decide what to act on (your design, your call)",
    "example": "Critique Feedback Examples:\n\n**BAD feedback**:\n❌ \"I don't like this\"\n❌ \"Can you make it pop more?\"\n❌ \"This is confusing\"\n❌ \"Use a different color\"\n\n**GOOD feedback**:\n\n✓ \"The primary CTA competes with the secondary action for attention. Based on our goal of driving signups, I'd suggest increasing contrast on the signup button.\"\n\n✓ \"I notice the form is 8 fields on one screen. Our research showed users abandon after 5 fields. Could we explore progressive disclosure or multi-step?\"\n\n✓ \"The icon choice for 'settings' (star) doesn't match the convention users expect (gear). This might cause findability issues.\"\n\n✓ \"I like how you've prioritized the search bar. One thought: the placeholder text 'Search' could be more helpful. What about 'Search products, brands, or categories'?\"\n\n**Framework for feedback**:\n\"I notice [observation] which might [impact on user/goal]. \nHave you considered [alternative]?\"\n\n**Critique Ground Rules**:\n- Assume positive intent\n- Focus on work, not person\n- Ask questions before suggestions\n- Be specific and actionable\n- Acknowledge what's working"
  }
]
